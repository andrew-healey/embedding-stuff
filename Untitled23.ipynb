{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A88OyhAHomH1",
        "outputId": "d8a55acf-014f-4fcf-f51d-ab20bfd23041"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.3.2 is available.\n",
            "You should consider upgrading via the '/Applications/Xcode.app/Contents/Developer/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "env: VOYAGE_API_KEY=pa-gMw1sHgi8agvl2QYoGka_j3cYKo5xLdY-bfSvw8AOk4\n"
          ]
        }
      ],
      "source": [
        "%pip install -q --upgrade einops voyageai openai datasets huggingface_hub\n",
        "%env VOYAGE_API_KEY=pa-gMw1sHgi8agvl2QYoGka_j3cYKo5xLdY-bfSvw8AOk4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sha(s):\n",
        "    import hashlib\n",
        "    return hashlib.sha256(s.encode()).hexdigest()\n",
        "def cache_embedder(embedder):\n",
        "    import os\n",
        "    import pickle\n",
        "    import hashlib\n",
        "    import torch\n",
        "    embedder_name = embedder.__class__.__name__\n",
        "    embedder_hash = sha(str(embedder))\n",
        "    print(f'caching {embedder_name} embedder')\n",
        "\n",
        "    def embedder_cached(texts,*args,**kwargs):\n",
        "        cache_key = {'texts':texts,'args':args,'kwargs':kwargs}\n",
        "        cache_path = f'/tmp/{embedder_hash}_{sha(str(cache_key))}.pkl'\n",
        "        if os.path.exists(cache_path):\n",
        "            with open(cache_path, 'rb') as f:\n",
        "                return pickle.load(f)\n",
        "        else:\n",
        "            embeddings = embedder(texts,*args,**kwargs)\n",
        "            with open(cache_path, 'wb') as f:\n",
        "                pickle.dump(embeddings, f)\n",
        "            return embeddings\n",
        "\n",
        "    return embedder_cached\n",
        "import torch\n",
        "def batched_embedder(batch_size=32):\n",
        "    def decorator(embedder):\n",
        "        def embedder_batched(texts,is_query=None):\n",
        "            return torch.cat([embedder(texts[i:i+batch_size],is_query[i:i+batch_size] if is_query else None) for i in range(0, len(texts), batch_size)])\n",
        "        return embedder_batched\n",
        "    return decorator\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101,
          "referenced_widgets": [
            "e0447abe66e0411cba639563bb5d43ef",
            "fadeff35a86d48c29ed5b559b2de75c8",
            "c8f3ac2f1e2a464dbdfa781b40da5a8c",
            "5ed9d8d5490e4670b3ef732af2feb5ac",
            "ddaa0780a71a4231944819950117ff97",
            "b23ff8d3071b4f48aeaf73c38973314b",
            "846a9be319a148f78466917538b2a186",
            "0cbffaa3365546c083ff2bc555a62951",
            "279b2498a5904ec287625ea4c2531cf2",
            "12647b6611554c2b9a3422c1840fb0ee",
            "69ac2a0ce4eb43b182ceced64838148d"
          ]
        },
        "id": "NFFv4gFJoKMm",
        "outputId": "ff084603-d80d-4fce-ea7b-6e10b9ee0de4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<All keys matched successfully>\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "caching function embedder\n",
            "['search_query: What is TSNE?', 'search_query: Who is Laurens van der Maaten?']\n",
            "tensor([[ 1.0951e-02,  5.7415e-02, -1.1036e-02,  ...,  3.5135e-05,\n",
            "         -2.8092e-02, -2.1600e-02],\n",
            "        [-1.3367e-02,  2.7091e-02, -2.3367e-02,  ...,  2.8799e-02,\n",
            "         -1.0675e-02,  2.8821e-02]])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "def mean_pooling(model_output, attention_mask):\n",
        "    token_embeddings = model_output[0]\n",
        "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "\n",
        "sentences = ['What is TSNE?', 'Who is Laurens van der Maaten?']\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = AutoModel.from_pretrained('nomic-ai/nomic-embed-text-v1', trust_remote_code=True)\n",
        "model.eval()\n",
        "\n",
        "@cache_embedder\n",
        "@batched_embedder()\n",
        "@torch.no_grad()\n",
        "def get_embeddings_nomic(raw_sentences,is_query=[]):\n",
        "    assert len(is_query)==len(raw_sentences)\n",
        "    query_is_query = {\n",
        "        True: 'search_query',\n",
        "        False: 'search_document'\n",
        "    }\n",
        "    sentences = [f\"{query_is_query[is_query[i]]}: {s}\" for i,s in enumerate(raw_sentences)]\n",
        "    print(sentences)\n",
        "    encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
        "\n",
        "    with torch.no_grad():\n",
        "        model_output = model(**encoded_input)\n",
        "\n",
        "    embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
        "    embeddings = F.normalize(embeddings, p=2, dim=1)\n",
        "    return embeddings\n",
        "\n",
        "embeddings = get_embeddings_nomic(sentences,is_query=[True,True])\n",
        "print(embeddings)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "caching function embedder\n",
            "tensor([[ 0.0185,  0.0181,  0.0522,  ...,  0.0027,  0.0395, -0.0464],\n",
            "        [ 0.0051, -0.0164,  0.0126,  ...,  0.0129,  0.0360, -0.0258]])\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import voyageai\n",
        "import torch\n",
        "\n",
        "vo = voyageai.Client(os.environ.get(\"VOYAGE_API_KEY\"))\n",
        "\n",
        "# Get the embedding of the documents\n",
        "@cache_embedder\n",
        "@batched_embedder(16)\n",
        "def get_embeddings_voyage(sentences,is_query=None):\n",
        "    return torch.tensor(vo.embed(sentences, model=\"voyage-code-2\").embeddings)\n",
        "    # batch_size = 128\n",
        "    # documents_embeddings = []\n",
        "    # for i in range(0, len(sentences), batch_size):\n",
        "    #     batch_sentences = sentences[i:i+batch_size]\n",
        "    #     batch_embeddings = vo.embed(batch_sentences, model=\"voyage-code-2\").embeddings\n",
        "    #     documents_embeddings.extend(batch_embeddings)\n",
        "    # return torch.tensor(documents_embeddings)\n",
        "\n",
        "print(get_embeddings_voyage(sentences))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "caching function embedder\n",
            "tensor([[-0.0212, -0.0139,  0.0041,  ..., -0.0161,  0.0056, -0.0048],\n",
            "        [ 0.0041, -0.0187, -0.0004,  ...,  0.0089, -0.0050, -0.0127]])\n"
          ]
        }
      ],
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "\n",
        "@cache_embedder\n",
        "def get_embeddings_openai(sentences,is_query=None):\n",
        "    resp = client.embeddings.create(\n",
        "        model=\"text-embedding-3-small\",\n",
        "        input=sentences,\n",
        "        encoding_format=\"float\"\n",
        "    )\n",
        "\n",
        "    return torch.tensor([r.embedding for r in resp.data])\n",
        "\n",
        "print(get_embeddings_openai(sentences))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# HumanEval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPH1N0JioqDj",
        "outputId": "327ba66f-3a73-4c91-e505-4457718adb2f"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "if not os.path.exists(\"HumanEval.jsonl\"):\n",
        "    !curl -L https://github.com/openai/human-eval/raw/master/data/HumanEval.jsonl.gz | gunzip > HumanEval.jsonl\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y9JvaVmho8Mr",
        "outputId": "0225fc91-5d23-4f7b-a59e-ceb913258785"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "with open(\"HumanEval.jsonl\",\"r\") as f:\n",
        "  raw_objs = [json.loads(line) for line in f.readlines()]\n",
        "\n",
        "\n",
        "def enrich_obj(obj):\n",
        "  docstring = obj[\"prompt\"].split('\"\"\" ')[0].split(\">>>\")[0]\n",
        "  return {**obj, \"docstring\": docstring}\n",
        "\n",
        "all_objs = [enrich_obj(obj) for obj in raw_objs]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CodeSearchNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "250"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "import re\n",
        "\n",
        "csn = load_dataset(\"code_search_net\", \"python\")\n",
        "\n",
        "\n",
        "all_objs = csn[\"train\"].select(range(250)).map(lambda x: {\n",
        "    \"docstring\": x[\"func_documentation_string\"],\n",
        "    \"canonical_solution\": re.sub(\n",
        "        r'def.*?:\\n|\"\"\"(.*?)\"\"\"',\n",
        "        '', x[\"func_code_string\"], flags=re.DOTALL\n",
        "    )#.strip()\n",
        "}).select_columns([\"docstring\", \"canonical_solution\"])\n",
        "len(all_objs)\n",
        "\n",
        "# all_objs[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------docstring:\n",
            "Appends a suffix to this element's ID, and optionally to all child IDs as well. There is sually no need to call this directly, invoked implicitly by :meth:`copy`\n",
            "--------------------canonical_solution:\n",
            "        \n",
            "        if self.id: self.id += idsuffix\n",
            "        if recursive:\n",
            "            for e in self:\n",
            "                try:\n",
            "                    e.addidsuffix(idsuffix, recursive)\n",
            "                except Exception:\n",
            "                    pass\n"
          ]
        }
      ],
      "source": [
        "\n",
        "for k,v in all_objs[0].items():\n",
        "  print(\"-\"*20+f\"{k}:\\n{v}\")\n",
        "\n",
        "\n",
        "# for k,v in csn[\"train\"][0].items():\n",
        "#   print(\"-\"*20+f\"{k}:\\n{v}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Naive testing\n",
        "\n",
        "Compare how often the top result matches the question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "qXGQ7h7-p90i"
      },
      "outputs": [],
      "source": [
        "\n",
        "# add embeddings to all objects\n",
        "def with_embeddings(get_embeddings,objs):\n",
        "  all_questions = [obj[\"docstring\"] for obj in objs]\n",
        "  all_solns = [obj[\"canonical_solution\"] for obj in objs]\n",
        "  is_query = [True]*len(all_questions) + [False]*len(all_solns)\n",
        "  all_embeddings = get_embeddings(all_questions + all_solns,is_query=is_query)\n",
        "  ret = []\n",
        "  for i, obj in enumerate(objs):\n",
        "    ret.append({\n",
        "      **obj,\n",
        "      \"question_embedding\": all_embeddings[i],\n",
        "      \"soln_embedding\": all_embeddings[i+len(objs)]\n",
        "    })\n",
        "  return ret\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_closest_soln_obj(question_obj,objs):\n",
        "  question_embedding = question_obj[\"question_embedding\"]\n",
        "  soln_embeddings = [obj[\"soln_embedding\"] for obj in objs]\n",
        "  cosine_similarities = F.cosine_similarity(question_embedding.unsqueeze(0), torch.stack(soln_embeddings), dim=1)\n",
        "  closest_soln_idx = torch.argmax(cosine_similarities)\n",
        "  return objs[closest_soln_idx]\n",
        "\n",
        "def embedding_accuracy(objs):\n",
        "  annot_objs = []\n",
        "  for obj in objs:\n",
        "      soln_obj = get_closest_soln_obj(obj,objs)\n",
        "      annot_objs.append({\n",
        "         **obj,\n",
        "          \"closest_soln_obj\": soln_obj,\n",
        "          \"correct\": soln_obj[\"canonical_solution\"] == obj[\"canonical_solution\"]\n",
        "      })\n",
        "\n",
        "  # get accuracy\n",
        "  correct = [obj[\"correct\"] for obj in annot_objs]\n",
        "  accuracy = sum(correct)/len(correct)\n",
        "  # print(f\"Accuracy: {accuracy}\")\n",
        "  return accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[\"search_query: Appends a suffix to this element's ID, and optionally to all child IDs as well. There is sually no need to call this directly, invoked implicitly by :meth:`copy`\", 'search_query: Correct all parent relations for elements within the scop. There is sually no need to call this directly, invoked implicitly by :meth:`copy`', 'search_query: Set a different document. Usually no need to call this directly, invoked implicitly by :meth:`copy`', \"search_query: Does this element have text (of the specified class)\\n\\n        By default, and unlike :meth:`text`, this checks strictly, i.e. the element itself must have the text and it is not inherited from its children.\\n\\n        Parameters:\\n            cls (str): The class of the text content to obtain, defaults to ``current``.\\n            strict (bool):  Set this if you are strictly interested in the text explicitly associated with the element, without recursing into children. Defaults to ``True``.\\n            correctionhandling: Specifies what text to check for when corrections are encountered. The default is ``CorrectionHandling.CURRENT``, which will retrieve the corrected/current text. You can set this to ``CorrectionHandling.ORIGINAL`` if you want the text prior to correction, and ``CorrectionHandling.EITHER`` if you don't care.\\n\\n        Returns:\\n            bool\", \"search_query: Does this element have phonetic content (of the specified class)\\n\\n        By default, and unlike :meth:`phon`, this checks strictly, i.e. the element itself must have the phonetic content and it is not inherited from its children.\\n\\n        Parameters:\\n            cls (str): The class of the phonetic content to obtain, defaults to ``current``.\\n            strict (bool):  Set this if you are strictly interested in the phonetic content explicitly associated with the element, without recursing into children. Defaults to ``True``.\\n            correctionhandling: Specifies what phonetic content to check for when corrections are encountered. The default is ``CorrectionHandling.CURRENT``, which will retrieve the corrected/current phonetic content. You can set this to ``CorrectionHandling.ORIGINAL`` if you want the phonetic content prior to correction, and ``CorrectionHandling.EITHER`` if you don't care.\\n\\n        Returns:\\n            bool\", 'search_query: Set the text for this element.\\n\\n        Arguments:\\n            text (str): The text\\n            cls (str): The class of the text, defaults to ``current`` (leave this unless you know what you are doing). There may be only one text content element of each class associated with the element.', 'search_query: Associate a document with this element.\\n\\n        Arguments:\\n            doc (:class:`Document`): A document\\n\\n        Each element must be associated with a FoLiA document.', \"search_query: Tests whether a new element of this class can be added to the parent.\\n\\n        This method is mostly for internal use.\\n        This will use the ``OCCURRENCES`` property, but may be overidden by subclasses for more customised behaviour.\\n\\n        Parameters:\\n            parent (:class:`AbstractElement`): The element that is being added to\\n            set (str or None): The set\\n            raiseexceptions (bool): Raise an exception if the element can't be added?\\n\\n        Returns:\\n            bool\\n\\n        Raises:\\n            ValueError\", 'search_query: This method will be called after an element is added to another and does some checks.\\n\\n        It can do extra checks and if necessary raise exceptions to prevent addition. By default makes sure the right document is associated.\\n\\n        This method is mostly for internal use.', 'search_query: Perform deep validation of this element.\\n\\n        Raises:\\n            :class:`DeepValidationError`', 'search_query: Internal method to find replaceable elements. Auxiliary function used by :meth:`AbstractElement.replace`. Can be overriden for more fine-grained control.', 'search_query: Recompute textual value based on the text content of the children. Only supported on elements that are a ``TEXTCONTAINER``', 'search_query: Appends a child element like ``append()``, but replaces any existing child element of the same type and set. If no such child element exists, this will act the same as append()\\n\\n        Keyword arguments:\\n            alternative (bool): If set to True, the *replaced* element will be made into an alternative. Simply use :meth:`AbstractElement.append` if you want the added element\\n            to be an alternative.\\n\\n        See :meth:`AbstractElement.append` for more information and all parameters.', 'search_query: Generator yielding all ancestors of this element, effectively back-tracing its path to the root element. A tuple of multiple classes may be specified.\\n\\n        Arguments:\\n            *Class: The class or classes (:class:`AbstractElement` or subclasses). Not instances!\\n\\n        Yields:\\n            elements (instances derived from :class:`AbstractElement`)', 'search_query: Find the most immediate ancestor of the specified type, multiple classes may be specified.\\n\\n        Arguments:\\n            *Classes: The possible classes (:class:`AbstractElement` or subclasses) to select from. Not instances!\\n\\n        Example::\\n\\n            paragraph = word.ancestor(folia.Paragraph)', 'search_query: Serialises the FoLiA element and all its contents to XML.\\n\\n        Arguments are mostly for internal use.\\n\\n        Returns:\\n            an lxml.etree.Element\\n\\n        See also:\\n            :meth:`AbstractElement.xmlstring` - for direct string output', 'search_query: Serialises the FoLiA element and all its contents to a Python dictionary suitable for serialisation to JSON.\\n\\n        Example::\\n\\n            import json\\n            json.dumps(word.json())\\n\\n        Returns:\\n            dict', 'search_query: Serialises this FoLiA element and all its contents to XML.\\n\\n        Returns:\\n            str: a string with XML representation for this element and all its children', \"search_query: Select child elements of the specified class.\\n\\n        A further restriction can be made based on set.\\n\\n        Arguments:\\n            Class (class): The class to select; any python class (not instance) subclassed off :class:`AbstractElement`\\n            Set (str): The set to match against, only elements pertaining to this set will be returned. If set to None (default), all elements regardless of set will be returned.\\n            recursive (bool): Select recursively? Descending into child elements? Defaults to ``True``.\\n            ignore: A list of Classes to ignore, if set to ``True`` instead of a list, all non-authoritative elements will be skipped (this is the default behaviour and corresponds to the following elements: :class:`Alternative`, :class:`AlternativeLayer`, :class:`Suggestion`, and :class:`folia.Original`. These elements and those contained within are never *authorative*. You may also include the boolean True as a member of a list, if you want to skip additional tags along the predefined non-authoritative ones.\\n            * ``node``: Reserved for internal usage, used in recursion.\\n\\n        Yields:\\n            Elements (instances derived from :class:`AbstractElement`)\\n\\n        Example::\\n\\n            for sense in text.select(folia.Sense, 'cornetto', True, [folia.Original, folia.Suggestion, folia.Alternative] ):\\n                ..\", 'search_query: Like :meth:`AbstractElement.select`, but instead of returning the elements, it merely counts them.\\n\\n        Returns:\\n            int', 'search_query: Returns a depth-first flat list of *all* items below this element (not limited to AbstractElement)', 'search_query: Get the metadata that applies to this element, automatically inherited from parent elements', 'search_query: Get the index at which an element occurs, recursive by default!\\n\\n        Returns:\\n            int', 'search_query: Returns a boolean indicating whether this element precedes the other element', 'search_query: Generic depth first search algorithm using a callback function, continues as long as the callback function returns None', \"search_query: Returns the next element, if it is of the specified type and if it does not cross the boundary of the defined scope. Returns None if no next element is found. Non-authoritative elements are never returned.\\n\\n        Arguments:\\n            * ``Class``: The class to select; any python class subclassed off `'AbstractElement``, may also be a tuple of multiple classes. Set to ``True`` to constrain to the same class as that of the current instance, set to ``None`` to not constrain at all\\n            * ``scope``: A list of classes which are never crossed looking for a next element. Set to ``True`` to constrain to a default list of structure elements (Sentence,Paragraph,Division,Event, ListItem,Caption), set to ``None`` to not constrain at all.\", \"search_query: Returns the previous element, if it is of the specified type and if it does not cross the boundary of the defined scope. Returns None if no next element is found. Non-authoritative elements are never returned.\\n\\n        Arguments:\\n            * ``Class``: The class to select; any python class subclassed off `'AbstractElement``. Set to ``True`` to constrain to the same class as that of the current instance, set to ``None`` to not constrain at all\\n            * ``scope``: A list of classes which are never crossed looking for a next element. Set to ``True`` to constrain to a default list of structure elements (Sentence,Paragraph,Division,Event, ListItem,Caption), set to ``None`` to not constrain at all.\", 'search_query: Returns the left context for an element, as a list. This method crosses sentence/paragraph boundaries by default, which can be restricted by setting scope', 'search_query: Returns the right context for an element, as a list. This method crosses sentence/paragraph boundaries by default, which can be restricted by setting scope', 'search_query: Returns this word in context, {size} words to the left, the current word, and {size} words to the right', 'search_query: Returns a RelaxNG definition for this element (as an XML element (lxml.etree) rather than a string)', 'search_query: Internal class method used for turning an XML element into an instance of the Class.\\n\\n        Args:\\n            * ``node`` - XML Element\\n            * ``doc`` - Document\\n\\n        Returns:\\n            An instance of the current Class.']\n",
            "['search_query: Removes the child element', 'search_query: Is this element part of a correction? If it is, it returns the Correction element (evaluating to True), otherwise it returns None', 'search_query: Apply a correction (TODO: documentation to be written still)', \"search_query: Obtain child elements (annotations) of the specified class.\\n\\n        A further restriction can be made based on set.\\n\\n        Arguments:\\n            Class (class): The class to select; any python class (not instance) subclassed off :class:`AbstractElement`\\n            Set (str): The set to match against, only elements pertaining to this set will be returned. If set to None (default), all elements regardless of set will be returned.\\n\\n        Yields:\\n            Elements (instances derived from :class:`AbstractElement`)\\n\\n        Example::\\n\\n            for sense in text.annotations(folia.Sense, 'http://some/path/cornetto'):\\n                ..\\n\\n        See also:\\n            :meth:`AbstractElement.select`\\n\\n        Raises:\\n            :meth:`AllowTokenAnnotation.annotations`\\n            :class:`NoSuchAnnotation` if no such annotation exists\", 'search_query: Returns an integer indicating whether such as annotation exists, and if so, how many.\\n\\n        See :meth:`AllowTokenAnnotation.annotations`` for a description of the parameters.', \"search_query: Obtain a single annotation element.\\n\\n        A further restriction can be made based on set.\\n\\n        Arguments:\\n            Class (class): The class to select; any python class (not instance) subclassed off :class:`AbstractElement`\\n            Set (str): The set to match against, only elements pertaining to this set will be returned. If set to None (default), all elements regardless of set will be returned.\\n\\n        Returns:\\n            An element (instance derived from :class:`AbstractElement`)\\n\\n        Example::\\n\\n            sense = word.annotation(folia.Sense, 'http://some/path/cornetto').cls\\n\\n        See also:\\n            :meth:`AllowTokenAnnotation.annotations`\\n            :meth:`AbstractElement.select`\\n\\n        Raises:\\n            :class:`NoSuchAnnotation` if no such annotation exists\", 'search_query: See ``AbstractElement.append()``', \"search_query: Returns a generator of Word elements found (recursively) under this element.\\n\\n        Arguments:\\n            * ``index``: If set to an integer, will retrieve and return the n'th element (starting at 0) instead of returning the list of all\", \"search_query: Returns a generator of Paragraph elements found (recursively) under this element.\\n\\n        Arguments:\\n            index (int or None): If set to an integer, will retrieve and return the n'th element (starting at 0) instead of returning the generator of all\", \"search_query: Returns a generator of Sentence elements found (recursively) under this element\\n\\n        Arguments:\\n            index (int or None): If set to an integer, will retrieve and return the n'th element (starting at 0) instead of returning a generator of all\", 'search_query: Returns a list of annotation layers found *directly* under this element, does not include alternative layers', 'search_query: Does the specified annotation layer exist?', 'search_query: See :meth:`AbstractElement.xml`', 'search_query: See :meth:`AbstractElement.json`', 'search_query: Obtain the text (unicode instance)', \"search_query: Returns and validates the Text Content's reference. Raises UnresolvableTextContent when invalid\", 'search_query: See :meth:`AbstractElement.xml`', \"search_query: Return and validate the Phonetic Content's reference. Raises UnresolvableTextContent when invalid\", \"search_query: Find the default reference for text offsets:\\n          The parent of the current textcontent's parent (counting only Structure Elements and Subtoken Annotation Elements)\\n\\n          Note: This returns not a TextContent element, but its parent. Whether the textcontent actually exists is checked later/elsewhere\", 'search_query: (Method for internal usage, see AbstractElement)', 'search_query: (Method for internal usage, see AbstractElement)', 'search_query: Generator yielding all morphemes (in a particular set if specified). For retrieving one specific morpheme by index, use morpheme() instead', 'search_query: Generator yielding all phonemes (in a particular set if specified). For retrieving one specific morpheme by index, use morpheme() instead', \"search_query: Returns a specific morpheme, the n'th morpheme (given the particular set if specified).\", \"search_query: Returns a specific phoneme, the n'th morpheme (given the particular set if specified).\", 'search_query: Yields span annotation elements of the specified type that include this word.\\n\\n        Arguments:\\n            type: The annotation type, can be passed as using any of the :class:`AnnotationType` member, or by passing the relevant :class:`AbstractSpanAnnotation` or :class:`AbstractAnnotationLayer` class.\\n            set (str or None): Constrain by set\\n\\n        Example::\\n\\n            for chunk in word.findspans(folia.Chunk):\\n                print(\" Chunk class=\", chunk.cls, \" words=\")\\n                for word2 in chunk.wrefs(): #print all words in the chunk (of which the word is a part)\\n                    print(word2, end=\"\")\\n                print()\\n\\n        Yields:\\n            Matching span annotation instances (derived from :class:`AbstractSpanAnnotation`)', 'search_query: Perform deep validation of this element.\\n\\n        Raises:\\n            :class:`DeepValidationError`', 'search_query: See :meth:`AbstractElement.xml`', 'search_query: See :meth:`AbstractElement.append`', 'search_query: Sets the span of the span element anew, erases all data inside.\\n\\n        Arguments:\\n            *args: Instances of :class:`Word`, :class:`Morpheme` or :class:`Phoneme`', 'search_query: Returns an integer indicating whether such as annotation exists, and if so, how many. See ``annotations()`` for a description of the parameters.', 'search_query: Will return a **single** annotation (even if there are multiple). Raises a ``NoSuchAnnotation`` exception if none was found']\n",
            "['search_query: Internal helper function', \"search_query: Returns a list of word references, these can be Words but also Morphemes or Phonemes.\\n\\n        Arguments:\\n            index (int or None): If set to an integer, will retrieve and return the n'th element (starting at 0) instead of returning the list of all\", 'search_query: Makes sure this element (and all subelements), are properly added to the index', 'search_query: Generator creating a deep copy of the children of this element. If idsuffix is a string, if set to True, a random idsuffix will be generated including a random 32-bit hash', 'search_query: See :meth:`AbstractElement.xml`', 'search_query: See :meth:`AbstractElement.append`', 'search_query: Generator over alternatives, either all or only of a specific annotation type, and possibly restrained also by set.\\n\\n        Arguments:\\n            * ``Class`` - The Class you want to retrieve (e.g. PosAnnotation). Or set to None to select all alternatives regardless of what type they are.\\n            * ``set``   - The set you want to retrieve (defaults to None, which selects irregardless of set)\\n\\n        Returns:\\n            Generator over Alternative elements', 'search_query: Returns the span element which spans over the specified words or morphemes.\\n\\n        See also:\\n            :meth:`Word.findspans`', 'search_query: Returns a RelaxNG definition for this element (as an XML element (lxml.etree) rather than a string)', 'search_query: Does the correction define new corrected annotations?', 'search_query: Does the correction record the old annotations prior to correction?', 'search_query: Does the correction record the current authoritative annotation (needed only in a structural context when suggestions are proposed)', 'search_query: Does the correction propose suggestions for correction?', 'search_query: See :meth:`AbstractElement.textcontent`', 'search_query: See :meth:`AbstractElement.phoncontent`', 'search_query: See :meth:`AbstractElement.hastext`', 'search_query: See :meth:`AbstractElement.text`', 'search_query: See :meth:`AbstractElement.phon`', 'search_query: See :meth:`AbstractElement.gettextdelimiter`', 'search_query: Get the new corrected annotation.\\n\\n        This returns only one annotation if multiple exist, use `index` to select another in the sequence.\\n\\n        Returns:\\n            an annotation element (:class:`AbstractElement`)\\n\\n        Raises:\\n            :class:`NoSuchAnnotation`', 'search_query: Get the old annotation prior to correction.\\n\\n        This returns only one annotation if multiple exist, use `index` to select another in the sequence.\\n\\n        Returns:\\n            an annotation element (:class:`AbstractElement`)\\n\\n        Raises:\\n            :class:`NoSuchAnnotation`', 'search_query: Get the current authoritative annotation (used with suggestions in a structural context)\\n\\n        This returns only one annotation if multiple exist, use `index` to select another in the sequence.\\n\\n        Returns:\\n            an annotation element (:class:`AbstractElement`)\\n\\n        Raises:\\n            :class:`NoSuchAnnotation`', 'search_query: Get suggestions for correction.\\n\\n        Yields:\\n            :class:`Suggestion` element that encapsulate the suggested annotations (if index is ``None``, default)\\n\\n        Returns:\\n            a :class:`Suggestion` element that encapsulate the suggested annotations (if index is set)\\n\\n        Raises:\\n            :class:`IndexError`', 'search_query: See :meth:`AbstractElement.select`', 'search_query: Serialises the FoLiA element to XML, by returning an XML Element (in lxml.etree) for this element and all its children. For string output, consider the xmlstring() method instead.', 'search_query: Will return a **single** annotation (even if there are multiple). Raises a ``NoSuchAnnotation`` exception if none was found', 'search_query: Find span annotation of the specified type that include this word', 'search_query: Generic correction method for words. You most likely want to use the helper functions\\n           :meth:`Sentence.splitword` , :meth:`Sentence.mergewords`, :meth:`deleteword`, :meth:`insertword` instead', 'search_query: TODO: Write documentation', 'search_query: TODO: Write documentation', 'search_query: TODO: Write documentation', 'search_query: Inserts a word **as a correction** before an existing word.\\n\\n        Reverse of :meth:`Sentence.insertword`.']\n",
            "['search_query: Resolve a variable sized pattern to all patterns of a certain fixed size', 'search_query: Load a FoLiA XML file.\\n\\n        Argument:\\n            filename (str): The file to load', 'search_query: Returns a depth-first flat list of all items in the document', \"search_query: Run Xpath expression and parse the resulting elements. Don't forget to use the FoLiA namesapace in your expressions, using folia: or the short form f:\", 'search_query: Return the alias for a set (if applicable, returns the unaltered set otherwise iff fallback is enabled)', 'search_query: Return the set for an alias (if applicable, raises an exception otherwise)', 'search_query: Save the document to file.\\n\\n        Arguments:\\n            * filename (str): The filename to save to. If not set (``None``, default), saves to the same file as loaded from.', \"search_query: Add a text (or speech) to the document:\\n\\n        Example 1::\\n\\n            doc.append(folia.Text)\\n\\n        Example 2::\\n            doc.append( folia.Text(doc, id='example.text') )\\n\\n        Example 3::\\n\\n            doc.append(folia.Speech)\", 'search_query: Internal method to generate XML nodes for all declarations', 'search_query: Return all declarations in a form ready to be serialised to JSON.\\n\\n        Returns:\\n            list of dict', 'search_query: Serialise the document to XML.\\n\\n        Returns:\\n            lxml.etree.Element\\n\\n        See also:\\n            :meth:`Document.xmlstring`', 'search_query: Serialise the document to a ``dict`` ready for serialisation to JSON.\\n\\n        Example::\\n\\n            import json\\n            jsondoc = json.dumps(doc.json())', 'search_query: Internal method to serialize metadata to XML', 'search_query: Internal method to parse XML declarations', 'search_query: OBSOLETE', 'search_query: Declare a new annotation type to be used in the document.\\n\\n        Keyword arguments can be used to set defaults for any annotation of this type and set.\\n\\n        Arguments:\\n            annotationtype: The type of annotation, this is conveyed by passing the corresponding annototion class (such as :class:`PosAnnotation` for example), or a member of :class:`AnnotationType`, such as ``AnnotationType.POS``.\\n            set (str): the set, should formally be a URL pointing to the set definition\\n\\n        Keyword Arguments:\\n            annotator (str): Sets a default annotator\\n            annotatortype: Should be either ``AnnotatorType.MANUAL`` or ``AnnotatorType.AUTO``, indicating whether the annotation was performed manually or by an automated process.\\n            datetime (datetime.datetime): Sets the default datetime\\n            alias (str): Defines alias that may be used in set attribute of elements instead of the full set name\\n\\n        Example::\\n\\n            doc.declare(folia.PosAnnotation, \\'http://some/path/brown-tag-set\\', annotator=\"mytagger\", annotatortype=folia.AnnotatorType.AUTO)', \"search_query: Checks if the annotation type is present (i.e. declared) in the document.\\n\\n        Arguments:\\n            annotationtype: The type of annotation, this is conveyed by passing the corresponding annototion class (such as :class:`PosAnnotation` for example), or a member of :class:`AnnotationType`, such as ``AnnotationType.POS``.\\n            set (str): the set, should formally be a URL pointing to the set definition (aliases are also supported)\\n\\n        Example::\\n\\n            if doc.declared(folia.PosAnnotation, 'http://some/path/brown-tag-set'):\\n                ..\\n\\n        Returns:\\n            bool\", 'search_query: Obtain the default set for the specified annotation type.\\n\\n        Arguments:\\n            annotationtype: The type of annotation, this is conveyed by passing the corresponding annototion class (such as :class:`PosAnnotation` for example), or a member of :class:`AnnotationType`, such as ``AnnotationType.POS``.\\n\\n        Returns:\\n            the set (str)\\n\\n        Raises:\\n            :class:`NoDefaultError` if the annotation type does not exist or if there is ambiguity (multiple sets for the same type)', 'search_query: Obtain the default annotator for the specified annotation type and set.\\n\\n        Arguments:\\n            annotationtype: The type of annotation, this is conveyed by passing the corresponding annototion class (such as :class:`PosAnnotation` for example), or a member of :class:`AnnotationType`, such as ``AnnotationType.POS``.\\n            set (str): the set, should formally be a URL pointing to the set definition\\n\\n        Returns:\\n            the set (str)\\n\\n        Raises:\\n            :class:`NoDefaultError` if the annotation type does not exist or if there is ambiguity (multiple sets for the same type)', \"search_query: Get or set the document's title from/in the metadata\\n\\n           No arguments: Get the document's title from metadata\\n           Argument: Set the document's title in metadata\", \"search_query: Get or set the document's date from/in the metadata.\\n\\n           No arguments: Get the document's date from metadata\\n           Argument: Set the document's date in metadata\", \"search_query: No arguments: Get the document's publisher from metadata\\n           Argument: Set the document's publisher in metadata\", \"search_query: No arguments: Get the document's license from metadata\\n           Argument: Set the document's license in metadata\", \"search_query: No arguments: Get the document's language (ISO-639-3) from metadata\\n           Argument: Set the document's language (ISO-639-3) in metadata\", 'search_query: Internal method to parse metadata', 'search_query: Internal method.\\n\\n        This is the main XML parser, will invoke class-specific XML parsers.', \"search_query: Perform any pending validations\\n\\n        Parameters:\\n            warnonly (bool): Warn only (True) or raise exceptions (False). If set to None then this value will be determined based on the document's FoLiA version (Warn only before FoLiA v1.5)\\n\\n        Returns:\\n            bool\", 'search_query: See :meth:`AbstractElement.select`', 'search_query: See :meth:`AbstractElement.count`', \"search_query: Return a generator of all paragraphs found in the document.\\n\\n        If an index is specified, return the n'th paragraph only (starting at 0)\", \"search_query: Return a generator of all sentence found in the document. Except for sentences in quotes.\\n\\n        If an index is specified, return the n'th sentence only (starting at 0)\", 'search_query: Returns the text of the entire document (returns a unicode instance)\\n\\n        See also:\\n            :meth:`AbstractElement.text`']\n",
            "['search_query: Iterate over all states in no particular order', 'search_query: Generic log method. Will prepend timestamp.\\n\\n    Keyword arguments:\\n      system   - Name of the system/module\\n      indent   - Integer denoting the desired level of indentation\\n      streams  - List of streams to output to\\n      stream   - Stream to output to (singleton version of streams)', 'search_query: Returns the single best result (if multiple have the same score, the first match is returned)', 'search_query: Return the top n best resulta (or possibly less if not enough is found)', 'search_query: Return the last n results (or possibly less if not found). Note that the last results are not necessarily the best ones! Depending on the search type.', 'search_query: Returns a list of synset IDs based on a lemma', 'search_query: call cdb_syn with synset identifier -> returns the synset xml;', 'search_query: Returns a list of (word, lu_id) tuples given a synset ID', 'search_query: Returns (lu_id, synonyms=[(word, lu_id)] ) tuple given a synset ID and a lemma', 'search_query: Returns a list of all predicted senses', 'search_query: Receives input_data in the form of a str or unicode object, passes this to the server, with proper consideration for the encodings, and returns the Frog output as a list of tuples: (word,pos,lemma,morphology), each of these is a proper unicode object unless return_unicode is set to False, in which case raw strings will be returned. Return_unicode is no longer optional, it is fixed to True, parameter is still there only for backwards-compatibility.', 'search_query: For each inputword, provides the index of the outputword', 'search_query: Calculate the overlap between two sequences. Yields (overlap, placement) tuples (multiple because there may be multiple overlaps!). The former is the part of the sequence that overlaps, and the latter is -1 if the overlap is on the left side, 0 if it is a subset, 1 if it overlaps on the right side, 2 if its an identical match', 'search_query: Tokenizes a string and returns a list of tokens\\n\\n    :param text: The text to tokenise\\n    :type text: string\\n    :param regexps: Regular expressions to use as tokeniser rules in tokenisation (default=_pynlpl.textprocessors.TOKENIZERRULES_)\\n    :type regexps:  Tuple/list of regular expressions to use in tokenisation\\n    :rtype: Returns a list of tokens\\n\\n    Examples:\\n\\n    >>> for token in tokenize(\"This is a test.\"):\\n    ...    print(token)\\n    This\\n    is\\n    a\\n    test\\n    .', 'search_query: Split sentences (based on tokenised data), returns sentences as a list of lists of tokens, each sentence is a list of tokens', 'search_query: Strip characters with diacritics and return a flat ascii representation', 'search_query: Perform a swap operation on a sequence of tokens, exhaustively swapping all tokens up to the maximum specified distance. This is a subset of all permutations.', 'search_query: Find a keyword in a particular sequence of tokens, and return the local context. Contextsize is the number of words to the left and right. The keyword may have multiple word, in which case it should to passed as a tuple or list', 'search_query: Retrieve the next element in line, this will remove it from the queue', 'search_query: Adds an item to the priority queue (in the right place), returns True if successfull, False if the item was blocked (because of a bad score)', 'search_query: Retrieve the next element in line, this will remove it from the queue', 'search_query: Return the score for item x (cheap lookup), Item 0 is always the best item', 'search_query: prune all but the first (=best) n items', 'search_query: prune down to n items at random, disregarding their score', \"search_query: Deletes all items below/above a certain score from the queue, depending on whether minimize is True or False. Note: It is recommended (more efficient) to use blockworse=True / blockequal=True instead! Preventing the addition of 'worse' items.\", 'search_query: Add an item to the Tree', 'search_query: Size is number of nodes under the trie, including the current node', 'search_query: Depth-first search, walking through trie, returning all encounterd nodes (by default only leaves)', 'search_query: Iterate over all sentences (sentence_id, sentence) in the document, sentence is a list of 4-tuples (word,id,pos,lemma)', 'search_query: Extracts paragraphs, returns list of plain-text(!) paragraphs', 'search_query: checks if the document is valid', 'search_query: Executes an xpath expression using the correct namespaces']\n",
            "['search_query: align the reference sentence with the tagged data', 'search_query: Returns information regarding the set', 'search_query: Returns information regarding the set', 'search_query: Higher-order generator function that yields class information in the right order, combines calls to :meth:`SetDefinition.classes` and :meth:`SetDefinition.classorder`', 'search_query: Returns a dictionary of classes for the specified (sub)set (if None, default, the main set is selected)', 'search_query: Return a list of class IDs in order for presentational purposes: order is determined first and foremost by explicit ordering, else alphabetically by label or as a last resort by class ID', 'search_query: Build the lexer.', 'search_query: <left> tree </left>', 'search_query: Environment value via `user:password|user2:password2`', 'search_query: Environment value via `/login;/register`', 'search_query: Environment value via `/login;/register`', 'search_query: Check if the user is authenticated for the given request.\\n\\n        The include_paths and exclude_paths are first checked. If\\n        authentication is required then the Authorization HTTP header is\\n        checked against the credentials.', 'search_query: Send a login response back to the client.', 'search_query: Check if the request path is in the `_include_paths` list.\\n\\n        If no specific include paths are given then we assume that\\n        authentication is required for all paths.', 'search_query: Check if the request path is in the `_exclude_paths` list', 'search_query: Bootstrap prompt_toolkit kwargs or use user defined values.\\n\\n    :param prompt_kwargs: The user specified prompt kwargs.', 'search_query: Start an interactive shell. All subcommands are available in it.\\n\\n    :param old_ctx: The current Click context.\\n    :param prompt_kwargs: Parameters passed to\\n        :py:func:`prompt_toolkit.shortcuts.prompt`.\\n\\n    If stdin is not a TTY, no prompt will be printed, but only commands read\\n    from stdin.', 'search_query: Register :func:`repl()` as sub-command *name* of *group*.', 'search_query: Run repl-internal commands.\\n\\n    Repl-internal commands are all commands starting with \":\".', 'search_query: Parameters\\n        ----------\\n        X: shape = [n_samples, n_features]', 'search_query: Given a function to map from an ID to an underlying object, and a function\\n    to map from an underlying object to the concrete GraphQLObjectType it\\n    corresponds to, constructs a `Node` interface that objects can implement,\\n    and a field config for a `node` root field.\\n\\n    If the type_resolver is omitted, object resolution on the interface will be\\n    handled with the `isTypeOf` method on object types, as with any GraphQL\\n    interface without a provided `resolveType` method.', 'search_query: Takes the \"global ID\" created by toGlobalID, and retuns the type name and ID\\n    used to create it.', 'search_query: Creates the configuration for an id field on a node, using `to_global_id` to\\n    construct the ID from the provided typename. The type-specific ID is fetcher\\n    by calling id_fetcher on the object, or if not provided, by accessing the `id`\\n    property on the object.', 'search_query: A simple function that accepts an array and connection arguments, and returns\\n    a connection object for use in GraphQL. It uses array offsets as pagination,\\n    so pagination will only work if the array is static.', 'search_query: A version of `connectionFromArray` that takes a promised array, and returns a\\n    promised connection.', 'search_query: Given a slice (subset) of an array, returns a connection object for use in\\n    GraphQL.\\n    This function is similar to `connectionFromArray`, but is intended for use\\n    cases where you know the cardinality of the connection, consider it too large\\n    to materialize the entire array, and instead wish pass in a slice of the\\n    total result large enough to cover the range specified in `args`.', 'search_query: Return the cursor associated with an object in an array.', 'search_query: Given an optional cursor and a default offset, returns the offset\\n    to use; if the cursor contains a valid offset, that will be used,\\n    otherwise it will be the default.', \"search_query: Draws an interactive 3D visualization of the inputted graph.\\n\\n    Args:\\n        data: Either an adjacency list of tuples (ie. [(1,2),...]) or object\\n        size: (Optional) Dimensions of visualization, in pixels\\n        node_size: (Optional) Defaults to 2.0\\n        edge_size: (Optional) Defaults to 0.25\\n        default_node_color: (Optional) If loading data without specified\\n            'color' properties, this will be used. Default is 0x5bc0de\\n        default_edge_color: (Optional) If loading data without specified\\n            'color' properties, this will be used. Default is 0xaaaaaa\\n        z: (Optional) Starting z position of the camera. Default is 100.\\n        shader: (Optional) Specifies shading algorithm to use. Can be 'toon',\\n            'basic', 'phong', or 'lambert'. Default is 'basic'.\\n        optimize: (Optional) Runs a force-directed layout algorithm on the\\n            graph. Default True.\\n        directed: (Optional) Includes arrows on edges to indicate direction.\\n            Default True.\\n        display_html: If True (default), embed the html in a IPython display.\\n            If False, return the html as a string.\\n        show_save: If True, displays a save icon for rendering graph as an\\n            image.\\n\\n    Inputting an adjacency list into `data` results in a 'default' graph type.\\n    For more customization, use the more expressive object format.\", 'search_query: Runs a force-directed algorithm on a graph, returning a data structure.\\n\\n    Args:\\n        data: An adjacency list of tuples (ie. [(1,2),...])\\n        iterations: (Optional) Number of FDL iterations to run in coordinate\\n            generation\\n        force_strength: (Optional) Strength of Coulomb and Hooke forces\\n            (edit this to scale the distance between nodes)\\n        dampening: (Optional) Multiplier to reduce force applied to nodes\\n        max_velocity: (Optional) Maximum distance a node can move in one step\\n        max_distance: (Optional) The maximum inter-node distance considered\\n        is_3d: (Optional) Generates three-dimensional coordinates\\n\\n    Outputs a json-serializable Python object. To visualize, pass the output to\\n    `jgraph.draw(...)`.', 'search_query: Outputs json without whitespace.', 'search_query: Outputs json with formatting edits + object handling.']\n",
            "['search_query: Fired for every object.', 'search_query: Displays each entry on its own line.', 'search_query: Runs a force-directed-layout algorithm on the input graph.\\n\\n    iterations - Number of FDL iterations to run in coordinate generation\\n    force_strength - Strength of Coulomb and Hooke forces\\n                     (edit this to scale the distance between nodes)\\n    dampening - Multiplier to reduce force applied to nodes\\n    max_velocity - Maximum distance a node can move in one step\\n    max_distance - The maximum distance considered for interactions', 'search_query: Calculates Coulomb forces and updates node data.', 'search_query: Wipe the entire context.\\n\\n    Args:\\n        Context is a dictionary or dictionary-like.\\n        Does not require any specific keys in context.', 'search_query: Parse input context string and returns context as dictionary.', \"search_query: pypyr step that checks if a file or directory path exists.\\n\\n    Args:\\n        context: pypyr.context.Context. Mandatory.\\n                 The following context key must exist\\n                - pathsToCheck. str/path-like or list of str/paths.\\n                                Path to file on disk to check.\\n\\n    All inputs support formatting expressions. Supports globs.\\n\\n    This step creates pathCheckOut in context, containing the results of the\\n    path check operation.\\n\\n    pathCheckOut:\\n        'inpath':\\n            exists: true # bool. True if path exists.\\n            count: 0 # int. Number of files found for in path.\\n            found: ['path1', 'path2'] # list of strings. Paths of files found.\\n\\n    [count] is 0 if no files found. If you specified a single input\\n    path to check and it exists, it's going to be 1. If you specified multiple\\n    in paths or a glob expression that found more than 1 result, well, take a\\n    guess.\\n\\n    [found] is a list of all the paths found for the [inpath]. If you passed\\n    in a glob or globs, will contain the globs found for [inpath].\\n\\n    This means you can do an existence evaluation like this in a formatting\\n    expression: '{pathCheckOut[inpathhere][exists]}'\\n\\n    Returns:\\n        None. updates context arg.\\n\\n    Raises:\\n        pypyr.errors.KeyNotInContextError: pathExists missing in context.\\n        pypyr.errors.KeyInContextHasNoValueError: pathCheck exists but is None.\", \"search_query: Write payload out to json file.\\n\\n    Args:\\n        context: pypyr.context.Context. Mandatory.\\n                 The following context keys expected:\\n                - fileWriteJson\\n                    - path. mandatory. path-like. Write output file to\\n                      here. Will create directories in path for you.\\n                    - payload. optional. Write this key to output file. If not\\n                      specified, output entire context.\\n\\n    Returns:\\n        None.\\n\\n    Raises:\\n        pypyr.errors.KeyNotInContextError: fileWriteJson or\\n            fileWriteJson['path'] missing in context.\\n        pypyr.errors.KeyInContextHasNoValueError: fileWriteJson or\\n            fileWriteJson['path'] exists but is None.\", \"search_query: Run another pipeline from this step.\\n\\n    The parent pipeline is the current, executing pipeline. The invoked, or\\n    child pipeline is the pipeline you are calling from this step.\\n\\n    Args:\\n        context: dictionary-like pypyr.context.Context. context is mandatory.\\n                 Uses the following context keys in context:\\n            - pype\\n                - name. mandatory. str. Name of pipeline to execute. This\\n                  {name}.yaml must exist in the working directory/pipelines\\n                  dir.\\n                - pipeArg. string. optional. String to pass to the\\n                  context_parser - the equivalent to context arg on the\\n                  pypyr cli. Only used if skipParse==False.\\n                - raiseError. bool. optional. Defaults to True. If False, log,\\n                  but swallow any errors that happen during the invoked\\n                  pipeline execution. Swallowing means that the current/parent\\n                  pipeline will carry on with the next step even if an error\\n                  occurs in the invoked pipeline.\\n                - skipParse. bool. optional. Defaults to True. skip the\\n                  context_parser on the invoked pipeline.\\n                - useParentContext. optional. bool. Defaults to True. Pass the\\n                  current (i.e parent) pipeline context to the invoked (child)\\n                  pipeline.\\n                - loader: str. optional. Absolute name of pipeline loader\\n                  module. If not specified will use\\n                  pypyr.pypeloaders.fileloader.\\n\\n    Returns:\\n        None\\n\\n    Raises:\\n        pypyr.errors.KeyNotInContextError: if ['pype'] or ['pype']['name']\\n                                           is missing.\\n        pypyr.errors.KeyInContextHasNoValueError: ['pype']['name'] exists but\\n                                                  is empty.\", \"search_query: Parse arguments for pype from context and assign default values.\\n\\n    Args:\\n        context: pypyr.context.Context. context is mandatory.\\n\\n    Returns:\\n        tuple (pipeline_name, #str\\n               use_parent_context, #bool\\n               pipe_arg, #str\\n               skip_parse, #bool\\n               raise_error #bool\\n               )\\n\\n    Raises:\\n       pypyr.errors.KeyNotInContextError: if ['pype']['name'] is missing.\\n       pypyr.errors.KeyInContextHasNoValueError: if ['pype']['name'] exists but\\n                                                 is None.\", 'search_query: Look for the pipeline in the various places it could be.\\n\\n    First checks the cwd. Then checks pypyr/pipelines dir.\\n\\n    Args:\\n        pipeline_name: string. Name of pipeline to find\\n        working_directory: string. Path in which to look for pipeline_name.yaml\\n\\n    Returns:\\n        Absolute path to the pipeline_name.yaml file\\n\\n    Raises:\\n        PipelineNotFoundError: if pipeline_name.yaml not found in working_dir\\n                               or in {pypyr install dir}/pipelines.', 'search_query: Open and parse the pipeline definition yaml.\\n\\n    Parses pipeline yaml and returns dictionary representing the pipeline.\\n\\n    pipeline_name.yaml should be in the working_dir/pipelines/ directory.\\n\\n    Args:\\n        pipeline_name: string. Name of pipeline. This will be the file-name of\\n                       the pipeline - i.e {pipeline_name}.yaml\\n        working_dir: path. Start looking in\\n                           ./working_dir/pipelines/pipeline_name.yaml\\n\\n    Returns:\\n        dict describing the pipeline, parsed from the pipeline yaml.\\n\\n    Raises:\\n        FileNotFoundError: pipeline_name.yaml not found in the various pipeline\\n                           dirs.', 'search_query: How to serialize this class back to yaml.', 'search_query: Run python eval on the input string.', \"search_query: Run step once for each item in foreach_items.\\n\\n        On each iteration, the invoked step can use context['i'] to get the\\n        current iterator value.\\n\\n        Args:\\n            context: (pypyr.context.Context) The pypyr context. This arg will\\n                     mutate.\", \"search_query: Invoke 'run_step' in the dynamically loaded step module.\\n\\n        Don't invoke this from outside the Step class. Use\\n        pypyr.dsl.Step.run_step instead.\\n        invoke_step just does the bare module step invocation, it does not\\n        evaluate any of the decorator logic surrounding the step. So unless\\n        you really know what you're doing, use run_step if you intend on\\n        executing the step the same way pypyr does.\\n\\n        Args:\\n            context: (pypyr.context.Context) The pypyr context. This arg will\\n                     mutate.\", 'search_query: Evaluate the step decorators to decide whether to run step or not.\\n\\n        Use pypyr.dsl.Step.run_step if you intend on executing the step the\\n        same way pypyr does.\\n\\n        Args:\\n            context: (pypyr.context.Context) The pypyr context. This arg will\\n                     mutate.', 'search_query: Run the foreach sequence or the conditional evaluation.\\n\\n        Args:\\n            context: (pypyr.context.Context) The pypyr context. This arg will\\n                     mutate.', 'search_query: Run a single pipeline step.\\n\\n        Args:\\n            context: (pypyr.context.Context) The pypyr context. This arg will\\n                     mutate.', \"search_query: Append step's 'in' parameters to context, if they exist.\\n\\n        Append the[in] dictionary to the context. This will overwrite\\n        existing values if the same keys are already in there. I.e if\\n        in_parameters has {'eggs': 'boiled'} and key 'eggs' already\\n        exists in context, context['eggs'] hereafter will be 'boiled'.\\n\\n        Args:\\n            context: (pypyr.context.Context) The pypyr context. This arg will\\n                     mutate - after method execution will contain the new\\n                     updated context.\", \"search_query: Run a single retry iteration.\\n\\n        This method abides by the signature invoked by poll.while_until_true,\\n        which is to say (counter, *args, **kwargs). In a normal execution\\n        chain, this method's args passed by self.retry_loop where context\\n        and step_method set. while_until_true injects counter as a 1st arg.\\n\\n        Args:\\n            counter. int. loop counter, which number of iteration is this.\\n            context: (pypyr.context.Context) The pypyr context. This arg will\\n                     mutate - after method execution will contain the new\\n                     updated context.\\n            step_method: (method/function) This is the method/function that\\n                         will execute on every loop iteration. Signature is:\\n                         function(context)\\n\\n         Returns:\\n            bool. True if step execution completed without error.\\n                  False if error occured during step execution.\", 'search_query: Run step inside a retry loop.\\n\\n        Args:\\n            context: (pypyr.context.Context) The pypyr context. This arg will\\n                     mutate - after method execution will contain the new\\n                     updated context.\\n            step_method: (method/function) This is the method/function that\\n                         will execute on every loop iteration. Signature is:\\n                         function(context)', \"search_query: Run a single loop iteration.\\n\\n        This method abides by the signature invoked by poll.while_until_true,\\n        which is to say (counter, *args, **kwargs). In a normal execution\\n        chain, this method's args passed by self.while_loop where context\\n        and step_method set. while_until_true injects counter as a 1st arg.\\n\\n        Args:\\n            counter. int. loop counter, which number of iteration is this.\\n            context: (pypyr.context.Context) The pypyr context. This arg will\\n                     mutate - after method execution will contain the new\\n                     updated context.\\n            step_method: (method/function) This is the method/function that\\n                         will execute on every loop iteration. Signature is:\\n                         function(context)\\n\\n         Returns:\\n            bool. True if self.stop evaluates to True after step execution,\\n                  False otherwise.\", 'search_query: Run step inside a while loop.\\n\\n        Args:\\n            context: (pypyr.context.Context) The pypyr context. This arg will\\n                     mutate - after method execution will contain the new\\n                     updated context.\\n            step_method: (method/function) This is the method/function that\\n                         will execute on every loop iteration. Signature is:\\n                         function(context)', \"search_query: Load a yaml file into the pypyr context.\\n\\n    Yaml parsed from the file will be merged into the pypyr context. This will\\n    overwrite existing values if the same keys are already in there.\\n    I.e if file yaml has {'eggs' : 'boiled'} and context {'eggs': 'fried'}\\n    already exists, returned context['eggs'] will be 'boiled'.\\n\\n    Args:\\n        context: pypyr.context.Context. Mandatory.\\n                 The following context key must exist\\n                - fetchYaml\\n                    - path. path-like. Path to file on disk.\\n                    - key. string. If exists, write yaml to this context key.\\n                      Else yaml writes to context root.\\n\\n    All inputs support formatting expressions.\\n\\n    Also supports a passing path as string to fetchYaml, but in this case you\\n    won't be able to specify a key.\\n\\n    Returns:\\n        None. updates context arg.\\n\\n    Raises:\\n        FileNotFoundError: take a guess\\n        pypyr.errors.KeyNotInContextError: fetchYamlPath missing in context.\\n        pypyr.errors.KeyInContextHasNoValueError: fetchYamlPath exists but is\\n                                                  None.\", \"search_query: Parse input yaml file and substitute {tokens} from context.\\n\\n    Loads yaml into memory to do parsing, so be aware of big files.\\n\\n    Args:\\n        context: pypyr.context.Context. Mandatory.\\n        - fileFormatYaml\\n            - in. mandatory.\\n              str, path-like, or an iterable (list/tuple) of\\n              strings/paths. Each str/path can be a glob, relative or\\n              absolute path.\\n            - out. optional. path-like.\\n              Can refer to a file or a directory.\\n              will create directory structure if it doesn't exist. If\\n              in-path refers to >1 file (e.g it's a glob or list), out\\n              path can only be a directory - it doesn't make sense to\\n              write >1 file to the same single file (this is not an\\n              appender.) To ensure out_path is read as a directory and\\n              not a file, be sure to have the path separator (/) at the\\n              end.\\n              If out_path is not specified or None, will in-place edit\\n              and overwrite the in-files.\\n\\n    Returns:\\n        None.\\n\\n    Raises:\\n        FileNotFoundError: take a guess\\n        pypyr.errors.KeyNotInContextError: fileFormatYaml or\\n            fileFormatYaml['in'] missing in context.\\n        pypyr.errors.KeyInContextHasNoValueError: fileFormatYaml or\\n            fileFormatYaml['in'] exists but is None.\", \"search_query: Decorator that executes a function until it returns True.\\n\\n    Executes wrapped function at every number of seconds specified by interval,\\n    until wrapped function either returns True or max_attempts are exhausted,\\n    whichever comes 1st. The wrapped function can have any given signature.\\n\\n    Use me if you always want to time out at max_attempts and you don't care\\n    about the while loop position counter value. If you do care, use\\n    while_until_true instead.\\n\\n    Args:\\n        interval: In seconds. How long to wait between executing the wrapped\\n                  function.\\n        max_attempts: int. Execute wrapped function up to this limit.\\n\\n    Returns:\\n        Bool. True if wrapped function returned True. False if reached\\n              max_attempts without the wrapped function ever returning True.\", 'search_query: Decorator that executes a function until it returns True.\\n\\n    Executes wrapped function at every number of seconds specified by interval,\\n    until wrapped function either returns True or max_attempts are exhausted,\\n    whichever comes 1st.\\n\\n    The difference between while_until_true and wait_until_true is that the\\n    latter will always loop to a max_attempts, whereas while_until_true will\\n    keep going indefinitely.\\n\\n    The other notable difference to wait_until_true is that the wrapped\\n    function signature must be:\\n    func(counter, *args, **kwargs)\\n\\n    This is because this decorator injects the while loop counter into the\\n    invoked function.\\n\\n    Args:\\n        interval: In seconds. How long to wait between executing the wrapped\\n                  function.\\n        max_attempts: int. Execute wrapped function up to this limit. None\\n                      means infinite (or until wrapped function returns True).\\n                      Passing anything <0 also means infinite.\\n\\n    Returns:\\n        Bool. True if wrapped function returned True. False if reached\\n              max_attempts without the wrapped function ever returning True.', \"search_query: Parse input file and substitutes {tokens} from context.\\n\\n    Args:\\n        context: pypyr.context.Context. Mandatory.\\n                 The following context keys expected:\\n                - fileFormat\\n                    - in. mandatory.\\n                      str, path-like, or an iterable (list/tuple) of\\n                      strings/paths. Each str/path can be a glob, relative or\\n                      absolute path.\\n                    - out. optional. path-like.\\n                      Can refer to a file or a directory.\\n                      will create directory structure if it doesn't exist. If\\n                      in-path refers to >1 file (e.g it's a glob or list), out\\n                      path can only be a directory - it doesn't make sense to\\n                      write >1 file to the same single file (this is not an\\n                      appender.) To ensure out_path is read as a directory and\\n                      not a file, be sure to have the path separator (/) at the\\n                      end.\\n                      If out_path is not specified or None, will in-place edit\\n                      and overwrite the in-files.\\n\\n    Returns:\\n        None.\\n\\n    Raises:\\n        FileNotFoundError: take a guess\\n        pypyr.errors.KeyNotInContextError: fileFormat missing in context.\\n        pypyr.errors.KeyInContextHasNoValueError: in or out exists but is None.\", 'search_query: Create new style in params from deprecated.', 'search_query: pypyr step saves current utc datetime to context.\\n\\n    Args:\\n        context: pypyr.context.Context. Mandatory.\\n                 The following context key is optional:\\n                - nowUtcIn. str. Datetime formatting expression. For full list\\n                  of possible expressions, check here:\\n                  https://docs.python.org/3.7/library/datetime.html#strftime-and-strptime-behavior\\n\\n    All inputs support pypyr formatting expressions.\\n\\n    This step creates now in context, containing a string representation of the\\n    timestamp. If input formatting not specified, defaults to ISO8601.\\n\\n    Default is:\\n    YYYY-MM-DDTHH:MM:SS.ffffff+00:00, or, if microsecond is 0,\\n    YYYY-MM-DDTHH:MM:SS\\n\\n    Returns:\\n        None. updates context arg.', \"search_query: Get, set, unset $ENVs.\\n\\n    Context is a dictionary or dictionary-like. context is mandatory.\\n\\n    Input context is:\\n        env:\\n            get: {dict}\\n            set: {dict}\\n            unset: [list]\\n\\n    At least one of env's sub-keys (get, set or unset) must exist.\\n\\n    This step will run whatever combination of Get, Set and Unset you specify.\\n    Regardless of combination, execution order is Get, Set, Unset.\"]\n",
            "[\"search_query: Get $ENVs into the pypyr context.\\n\\n    Context is a dictionary or dictionary-like. context is mandatory.\\n\\n    context['env']['get'] must exist. It's a dictionary.\\n    Values are the names of the $ENVs to write to the pypyr context.\\n    Keys are the pypyr context item to which to write the $ENV values.\\n\\n    For example, say input context is:\\n        key1: value1\\n        key2: value2\\n        pypyrCurrentDir: value3\\n        env:\\n            get:\\n                pypyrUser: USER\\n                pypyrCurrentDir: PWD\\n\\n    This will result in context:\\n        key1: value1\\n        key2: value2\\n        key3: value3\\n        pypyrUser: <<value of $USER here>>\\n        pypyrCurrentDir: <<value of $PWD here, not value3>>\", \"search_query: Set $ENVs to specified string. from the pypyr context.\\n\\n    Args:\\n        context: is dictionary-like. context is mandatory.\\n                 context['env']['set'] must exist. It's a dictionary.\\n                 Values are strings to write to $ENV.\\n                 Keys are the names of the $ENV values to which to write.\\n\\n    For example, say input context is:\\n        key1: value1\\n        key2: value2\\n        key3: value3\\n        env:\\n            set:\\n                MYVAR1: {key1}\\n                MYVAR2: before_{key3}_after\\n                MYVAR3: arbtexthere\\n\\n    This will result in the following $ENVs:\\n    $MYVAR1 = value1\\n    $MYVAR2 = before_value3_after\\n    $MYVAR3 = arbtexthere\\n\\n    Note that the $ENVs are not persisted system-wide, they only exist for\\n    pypyr sub-processes, and as such for the following steps during this pypyr\\n    pipeline execution. If you set an $ENV here, don't expect to see it in your\\n    system environment variables after the pipeline finishes running.\", \"search_query: Unset $ENVs.\\n\\n    Context is a dictionary or dictionary-like. context is mandatory.\\n\\n    context['env']['unset'] must exist. It's a list.\\n    List items are the names of the $ENV values to unset.\\n\\n    For example, say input context is:\\n        key1: value1\\n        key2: value2\\n        key3: value3\\n        env:\\n            unset:\\n                MYVAR1\\n                MYVAR2\\n\\n    This will result in the following $ENVs being unset:\\n    $MYVAR1\\n    $MYVAR2\", 'search_query: Handle deprecated context input.', \"search_query: Assert that something is True or equal to something else.\\n\\n    Args:\\n        context: dictionary-like pypyr.context.Context. context is mandatory.\\n        Uses the following context keys in context:\\n            - assert\\n                - this. mandatory. Any type. If assert['equals'] not specified,\\n                  evals as boolean.\\n                - equals. optional. Any type.\\n\\n    If assert['this'] evaluates to False raises error.\\n    If assert['equals'] is specified, raises error if\\n    assert.this != assert.equals.\\n\\n    assert['this'] & assert['equals'] both support string substitutions.\\n\\n    Returns:\\n        None\\n\\n    Raises:\\n        ContextError: if assert evaluates to False.\", 'search_query: Handle deprecated context input.', 'search_query: Archive and/or extract tars with or without compression.\\n\\n    Args:\\n        context: dictionary-like. Mandatory.\\n\\n        Expects the following context:\\n        tar:\\n            extract:\\n                - in: /path/my.tar\\n                  out: /out/path\\n            archive:\\n                - in: /dir/to/archive\\n                  out: /out/destination.tar\\n            format: \\'\\'\\n\\n        tar[\\'format\\'] - if not specified, defaults to lzma/xz\\n                       Available options:\\n                        - \\'\\' - no compression\\n                        - gz (gzip)\\n                        - bz2 (bzip2)\\n                        - xz (lzma)\\n\\n    This step will run whatever combination of Extract and Archive you specify.\\n    Regardless of combination, execution order is Extract, Archive.\\n\\n    Source and destination paths support {key} string interpolation.\\n\\n    Never extract archives from untrusted sources without prior inspection.\\n    It is possible that files are created outside of path, e.g. members that\\n    have absolute filenames starting with \"/\" or filenames with two dots \"..\".', \"search_query: Get file mode for reading from tar['format'].\\n\\n    This should return r:*, r:gz, r:bz2 or r:xz. If user specified something\\n    wacky in tar.Format, that's their business.\\n\\n    In theory r:* will auto-deduce the correct format.\", \"search_query: Get file mode for writing from tar['format'].\\n\\n    This should return w:, w:gz, w:bz2 or w:xz. If user specified something\\n    wacky in tar.Format, that's their business.\", \"search_query: Archive specified path to a tar archive.\\n\\n    Args:\\n        context: dictionary-like. context is mandatory.\\n            context['tar']['archive'] must exist. It's a dictionary.\\n            keys are the paths to archive.\\n            values are the destination output paths.\\n\\n    Example:\\n        tar:\\n            archive:\\n                - in: path/to/dir\\n                  out: path/to/destination.tar.xs\\n                - in: another/my.file\\n                  out: ./my.tar.xs\\n\\n        This will archive directory path/to/dir to path/to/destination.tar.xs,\\n        and also archive file another/my.file to ./my.tar.xs\", \"search_query: Extract all members of tar archive to specified path.\\n\\n    Args:\\n        context: dictionary-like. context is mandatory.\\n            context['tar']['extract'] must exist. It's a dictionary.\\n            keys are the path to the tar to extract.\\n            values are the destination paths.\\n\\n    Example:\\n        tar:\\n            extract:\\n                - in: path/to/my.tar.xs\\n                  out: /path/extract/here\\n                - in: another/tar.xs\\n                  out: .\\n\\n        This will extract path/to/my.tar.xs to /path/extract/here, and also\\n        extract another/tar.xs to $PWD.\", 'search_query: Handle deprecated context input.', 'search_query: Run shell command without shell interpolation.\\n\\n    Context is a dictionary or dictionary-like.\\n\\n    Context must contain the following keys:\\n    cmd: <<cmd string>> (command + args to execute.)\\n\\n    OR, as a dict\\n    cmd:\\n        run: str. mandatory. <<cmd string>> command + args to execute.\\n        save: bool. defaults False. save output to cmdOut.\\n\\n    Will execute command string in the shell as a sub-process.\\n    The shell defaults to /bin/sh.\\n    The context[\\'cmd\\'] string must be formatted exactly as it would be when\\n    typed at the shell prompt. This includes, for example, quoting or backslash\\n    escaping filenames with spaces in them.\\n    There is an exception to this: Escape curly braces: if you want a literal\\n    curly brace, double it like {{ or }}.\\n\\n    If save is True, will save the output to context as follows:\\n        cmdOut:\\n            returncode: 0\\n            stdout: \\'stdout str here. None if empty.\\'\\n            stderr: \\'stderr str here. None if empty.\\'\\n\\n    cmdOut.returncode is the exit status of the called process. Typically 0\\n    means OK. A negative value -N indicates that the child was terminated by\\n    signal N (POSIX only).\\n\\n    context[\\'cmd\\'] will interpolate anything in curly braces for values\\n    found in context. So if your context looks like this:\\n        key1: value1\\n        key2: value2\\n        cmd: mything --arg1 {key1}\\n\\n    The cmd passed to the shell will be \"mything --arg value1\"', \"search_query: Get $ENVs, allowing a default if not found.\\n\\n    Set context properties from environment variables, and specify a default\\n    if the environment variable is not found.\\n\\n    This differs from pypyr.steps.env get, which raises an error if attempting\\n    to read an $ENV that doesn't exist.\\n\\n    Args:\\n        context. mandatory. Context is a pypyr Context.\\n\\n    Input context is:\\n        envGet:\\n            - env: 'envvarnamehere'\\n              key: 'savetocontexthere'\\n              default: 'save this to key if env doesnt exist'\\n\\n    'env' is the bare environment variable name, do not put the $ in front of\\n    it.\\n\\n    Will process as many env/key/default pairs as exist in the list under\\n    envGet.\\n\\n    Returns:\\n        None.\\n\\n    Raises:\\n        ContextError: envGet is not a list of dicts.\\n        KeyNotInContextError: envGet env or key doesn't exist.\", 'search_query: Parse env, key, default out of input dict.\\n\\n    Args:\\n        get_item: dict. contains keys env/key/default\\n\\n    Returns:\\n        (env, key, has_default, default) tuple, where\\n            env: str. env var name.\\n            key: str. save env value to this context key.\\n            has_default: bool. True if default specified.\\n            default: the value of default, if specified.\\n\\n    Raises:\\n        ContextError: envGet is not a list of dicts.\\n        KeyNotInContextError: If env or key not found in get_config.', \"search_query: Executes dynamic python code.\\n\\n    Context is a dictionary or dictionary-like.\\n    Context must contain key 'pycode'\\n    Will exec context['pycode'] as dynamically interpreted python statements.\\n\\n    context is mandatory. When you execute the pipeline, it should look\\n    something like this:\\n        pipeline-runner [name here] 'pycode=print(1+1)'.\", 'search_query: Parse input context string and returns context as dictionary.', 'search_query: Return ArgumentParser for pypyr cli.', 'search_query: Entry point for pypyr cli.\\n\\n    The setup_py entry_point wraps this in sys.exit already so this effectively\\n    becomes sys.exit(main()).\\n    The __main__ entry point similarly wraps sys.exit().', 'search_query: Run a command.\\n\\n        Runs a program or executable. If is_shell is True, executes the command\\n        through the shell.\\n\\n        Args:\\n            is_shell: bool. defaults False. Set to true to execute cmd through\\n                      the default shell.', \"search_query: Remove specified keys from context.\\n\\n    Args:\\n        Context is a dictionary or dictionary-like.\\n        context['contextClear'] must exist. It's a dictionary.\\n        Will iterate context['contextClear'] and remove those keys from\\n        context.\\n\\n    For example, say input context is:\\n        key1: value1\\n        key2: value2\\n        key3: value3\\n        key4: value4\\n        contextClear:\\n            - key2\\n            - key4\\n            - contextClear\\n\\n    This will result in return context:\\n        key1: value1\\n        key3: value3\", 'search_query: Run command, program or executable.\\n\\n    Context is a dictionary or dictionary-like.\\n\\n    Context must contain the following keys:\\n    cmd: <<cmd string>> (command + args to execute.)\\n\\n    OR, as a dict\\n    cmd:\\n        run: str. mandatory. <<cmd string>> command + args to execute.\\n        save: bool. defaults False. save output to cmdOut.\\n\\n    Will execute the command string in the shell as a sub-process.\\n    Escape curly braces: if you want a literal curly brace, double it like\\n    {{ or }}.\\n\\n    If save is True, will save the output to context as follows:\\n        cmdOut:\\n            returncode: 0\\n            stdout: \\'stdout str here. None if empty.\\'\\n            stderr: \\'stderr str here. None if empty.\\'\\n\\n    cmdOut.returncode is the exit status of the called process. Typically 0\\n    means OK. A negative value -N indicates that the child was terminated by\\n    signal N (POSIX only).\\n\\n    context[\\'cmd\\'] will interpolate anything in curly braces for values\\n    found in context. So if your context looks like this:\\n        key1: value1\\n        key2: value2\\n        cmd: mything --arg1 {key1}\\n\\n    The cmd passed to the shell will be \"mything --arg value1\"', \"search_query: Set hierarchy into context with substitutions if it doesn't exist yet.\\n\\n    context is a dictionary or dictionary-like.\\n    context['defaults'] must exist. It's a dictionary.\\n\\n    Will iterate context['defaults'] and add these as new values where\\n    their keys don't already exist. While it's doing so, it will leave\\n    all other values in the existing hierarchy untouched.\\n\\n    List merging is purely additive, with no checks for uniqueness or already\\n    existing list items. E.g context [0,1,2] with contextMerge=[2,3,4]\\n    will result in [0,1,2,2,3,4]\\n\\n    Keep this in mind especially where complex types like\\n    dicts nest inside a list - a merge will always add a new dict list item,\\n    not merge it into whatever dicts might exist on the list already.\\n\\n    For example, say input context is:\\n        key1: value1\\n        key2: value2\\n        key3:\\n            k31: value31\\n            k32: value32\\n        defaults:\\n            key2: 'aaa_{key1}_zzz'\\n            key3:\\n                k33: value33\\n            key4: 'bbb_{key2}_yyy'\\n\\n    This will result in return context:\\n        key1: value1\\n        key2: value2\\n        key3:\\n            k31: value31\\n            k32: value32\\n            k33: value33\\n        key4: bbb_value2_yyy\", 'search_query: Get the steps attribute of module pipeline.\\n\\n    If there is no steps sequence on the pipeline, return None. Guess you\\n    could theoretically want to run a pipeline with nothing in it.', 'search_query: Run the on_failure step group if it exists.\\n\\n    This function will swallow all errors, to prevent obfuscating the error\\n    condition that got it here to begin with.', 'search_query: Run the run_step(context) method of each step in steps.\\n\\n    Args:\\n        steps: list. Sequence of Steps to execute\\n        context: pypyr.context.Context. The pypyr context. Will mutate.', 'search_document:         \\n        if self.id: self.id += idsuffix\\n        if recursive:\\n            for e in self:\\n                try:\\n                    e.addidsuffix(idsuffix, recursive)\\n                except Exception:\\n                    pass', 'search_document:         \\n        for c in self:\\n            if isinstance(c, AbstractElement):\\n                c.parent = self\\n                c.setparents()', 'search_document:         \\n        self.doc = newdoc\\n        if self.doc and self.id:\\n            self.doc.index[self.id] = self\\n        for c in self:\\n            if isinstance(c, AbstractElement):\\n                c.setdoc(newdoc)', 'search_document:             cls (str): The class of the text content to obtain,             bool\\n        \"\"\"\\n        if not self.PRINTABLE: #only printable elements can hold text\\n            return False\\n        elif self.TEXTCONTAINER:\\n            return True\\n        else:\\n            try:\\n                if strict:\\n                    self.textcontent(cls, correctionhandling) #will raise NoSuchTextException when not found\\n                    return True\\n                else:\\n                    #Check children\\n                    for e in self:\\n                        if e.PRINTABLE and not isinstance(e, TextContent):\\n                            if e.hastext(cls, strict, correctionhandling):\\n                                return True\\n\\n                    self.textcontent(cls, correctionhandling)  #will raise NoSuchTextException when not found\\n                    return True\\n            except NoSuchText:\\n                return False', 'search_document:             cls (str): The class of the phonetic content to obtain,             bool\\n        \"\"\"\\n        if not self.SPEAKABLE: #only printable elements can hold text\\n            return False\\n        elif self.PHONCONTAINER:\\n            return True\\n        else:\\n            try:\\n                if strict:\\n                    self.phoncontent(cls, correctionhandling)\\n                    return True\\n                else:\\n                    #Check children\\n                    for e in self:\\n                        if e.SPEAKABLE and not isinstance(e, PhonContent):\\n                            if e.hasphon(cls, strict, correctionhandling):\\n                                return True\\n\\n                    self.phoncontent(cls)  #will raise NoSuchTextException when not found\\n                    return True\\n            except NoSuchPhon:\\n                return False', 'search_document:         \\n        self.replace(TextContent, value=text, cls=cls)']\n",
            "['search_document:         \\n        assert isinstance(doc, Document)\\n\\n        if not self.doc:\\n            self.doc = doc\\n            if self.id:\\n                if self.id in doc:\\n                    raise DuplicateIDError(self.id)\\n                else:\\n                    self.doc.index[id] = self\\n\\n        for e in self: #recursive for all children\\n            if isinstance(e,AbstractElement): e.setdocument(doc)', 'search_document:         \\n\\n\\n        if not parent.__class__.accepts(Class, raiseexceptions, parent):\\n            return False\\n\\n        if Class.OCCURRENCES > 0:\\n            #check if the parent doesn\\'t have too many already\\n            count = parent.count(Class,None,True,[True, AbstractStructureElement]) #never descend into embedded structure annotatioton\\n            if count >= Class.OCCURRENCES:\\n                if raiseexceptions:\\n                    if parent.id:\\n                        extra = \\' (id=\\' + parent.id + \\')\\'\\n                    else:\\n                        extra = \\'\\'\\n                    raise DuplicateAnnotationError(\"Unable to add another object of type \" + Class.__name__ + \" to \" + parent.__class__.__name__ + \" \" + extra + \". There are already \" + str(count) + \" instances of this class, which is the maximum.\")\\n                else:\\n                    return False\\n\\n        if Class.OCCURRENCES_PER_SET > 0 and set and Class.REQUIRED_ATTRIBS and Attrib.CLASS in Class.REQUIRED_ATTRIBS:\\n            count = parent.count(Class,set,True, [True, AbstractStructureElement])\\n            if count >= Class.OCCURRENCES_PER_SET:\\n                if raiseexceptions:\\n                    if parent.id:\\n                        extra = \\' (id=\\' + parent.id + \\')\\'\\n                    else:\\n                        extra = \\'\\'\\n                    raise DuplicateAnnotationError(\"Unable to add another object of set \" + set + \" and type \" + Class.__name__ + \" to \" + parent.__class__.__name__ + \" \" + extra + \". There are already \" + str(count) + \" instances of this class, which is the maximum for the set.\")\\n                else:\\n                    return False\\n\\n\\n\\n        return True', 'search_document:         \\n\\n        #If the element was not associated with a document yet, do so now (and for all unassociated children:\\n        if not self.doc and self.parent.doc:\\n            self.setdocument(self.parent.doc)\\n\\n        if self.doc and self.doc.deepvalidation:\\n            self.deepvalidation()', 'search_document:         \\n        if self.doc and self.doc.deepvalidation and self.set and self.set[0] != \\'_\\':\\n            try:\\n                self.doc.set                if self.cls and not self.doc.allowadhocsets:\\n                    raise DeepValidationError(\"Set                 errormsg =  str(e) + \" (in set \" + self.set+\" for \" + self.XMLTAG\\n                if self.id:\\n                    errormsg += \" with ID \" + self.id\\n                errormsg += \")\"\\n                raise DeepValidationError(errormsg)', 'search_document:         \\n        return list(parent.select(Class,set,False))', 'search_document:         \\n        if self.TEXTCONTAINER:\\n            s = \"\"\\n            for child in self:\\n                if isinstance(child, AbstractElement):\\n                    child.updatetext()\\n                    s += child.text()\\n                elif isstring(child):\\n                    s += child\\n            self.data = [s]', 'search_document:         \\n\\n        if \\'set\\' in kwargs:\\n            set = kwargs[\\'set\\']\\n            del kwargs[\\'set\\']\\n        else:\\n            try:\\n                set = child.set\\n            except AttributeError:\\n                set = None\\n\\n        if inspect.isclass(child):\\n            Class = child\\n            replace = Class.findreplaceables(self, set, **kwargs)\\n        elif (self.TEXTCONTAINER or self.PHONCONTAINER) and isstring(child):\\n            #replace will replace ALL text content, removing text markup along the way!\\n            self.data = []\\n            return self.append(child, *args,**kwargs)\\n        else:\\n            Class = child.__class__\\n            kwargs[\\'instance\\'] = child\\n            replace = Class.findreplaceables(self,set,**kwargs)\\n            del kwargs[\\'instance\\']\\n\\n        kwargs[\\'set\\'] = set #was deleted temporarily for findreplaceables\\n\\n        if len(replace) == 0:\\n            #nothing to replace, simply call append\\n            if \\'alternative\\' in kwargs:\\n                del kwargs[\\'alternative\\'] #has other meaning in append()\\n            return self.append(child, *args, **kwargs)\\n        elif len(replace) > 1:\\n            raise Exception(\"Unable to replace. Multiple candidates found, unable to choose.\")\\n        elif len(replace) == 1:\\n            if \\'alternative\\' in kwargs and kwargs[\\'alternative\\']:\\n                #old version becomes alternative\\n                if replace[0] in self.data:\\n                    self.data.remove(replace[0])\\n                alt = self.append(Alternative)\\n                alt.append(replace[0])\\n                del kwargs[\\'alternative\\'] #has other meaning in append()\\n            else:\\n                #remove old version competely\\n                self.remove(replace[0])\\n            e = self.append(child, *args, **kwargs)\\n            self.updatetext()\\n            return e', 'search_document:         \\n        e = self\\n        while e:\\n            if e.parent:\\n                e = e.parent\\n                if not Class or isinstance(e,Class):\\n                    yield e\\n                elif isinstance(Class, tuple):\\n                    for C in Class:\\n                        if isinstance(e,C):\\n                            yield e\\n            else:\\n                break', 'search_document:         \\n        for e in self.ancestors(tuple(Classes)):\\n            return e\\n        raise NoSuchAnnotation', 'search_document:         \\n        E = ElementMaker(namespace=NSFOLIA,nsmap={None: NSFOLIA, \\'xml\\' : \"http://www.w3.org/XML/1998/namespace\"})\\n\\n        if not attribs: attribs = {}\\n        if not elements: elements = []\\n\\n        if self.id:\\n            attribs[\\'{http://www.w3.org/XML/1998/namespace}id\\'] = self.id\\n\\n        #Some attributes only need to be added if they are not the same as what\\'s already set in the declaration\\n        if not isinstance(self, AbstractAnnotationLayer):\\n            if \\'{\\' + NSFOLIA + \\'}set\\' not in attribs: #do not override if overloaded function already set it\\n                try:\\n                    if self.set:\\n                        if not self.ANNOTATIONTYPE in self.doc.annotation                            if self.set != None:\\n                                if self.ANNOTATIONTYPE in self.doc.set_alias and self.set in self.doc.set_alias[self.ANNOTATIONTYPE]:\\n                                    attribs[\\'{\\' + NSFOLIA + \\'}set\\'] = self.doc.set_alias[self.ANNOTATIONTYPE][self.set] #use alias instead\\n                                else:\\n                                    attribs[\\'{\\' + NSFOLIA + \\'}set\\'] = self.set\\n                except AttributeError:\\n                    pass\\n\\n        if \\'{\\' + NSFOLIA + \\'}class\\' not in attribs: #do not override if caller already set it\\n            try:\\n                if self.cls:\\n                    attribs[\\'{\\' + NSFOLIA + \\'}class\\'] = self.cls\\n            except AttributeError:\\n                pass\\n\\n        if \\'{\\' + NSFOLIA + \\'}annotator\\' not in attribs: #do not override if caller already set it\\n            try:\\n                if self.annotator and ((not (self.ANNOTATIONTYPE in self.doc.annotation                    attribs[\\'{\\' + NSFOLIA + \\'}annotator\\'] = self.annotator\\n                if self.annotatortype and ((not (self.ANNOTATIONTYPE in self.doc.annotation                    if self.annotatortype == AnnotatorType.AUTO:\\n                        attribs[\\'{\\' + NSFOLIA + \\'}annotatortype\\'] = \\'auto\\'\\n                    elif self.annotatortype == AnnotatorType.MANUAL:\\n                        attribs[\\'{\\' + NSFOLIA + \\'}annotatortype\\'] = \\'manual\\'\\n            except AttributeError:\\n                pass\\n\\n        if \\'{\\' + NSFOLIA + \\'}confidence\\' not in attribs: #do not override if caller already set it\\n            if self.confidence:\\n                attribs[\\'{\\' + NSFOLIA + \\'}confidence\\'] = str(self.confidence)\\n\\n        if \\'{\\' + NSFOLIA + \\'}n\\' not in attribs: #do not override if caller already set it\\n            if self.n:\\n                attribs[\\'{\\' + NSFOLIA + \\'}n\\'] = str(self.n)\\n\\n        if \\'{\\' + NSFOLIA + \\'}auth\\' not in attribs: #do not override if caller already set it\\n            try:\\n                if not self.AUTH or not self.auth: #(former is static, latter isn\\'t)\\n                    attribs[\\'{\\' + NSFOLIA + \\'}auth\\'] = \\'no\\'\\n            except AttributeError:\\n                pass\\n\\n        if \\'{\\' + NSFOLIA + \\'}datetime\\' not in attribs: #do not override if caller already set it\\n            if self.datetime and ((not (self.ANNOTATIONTYPE in self.doc.annotation                attribs[\\'{\\' + NSFOLIA + \\'}datetime\\'] = self.datetime.strftime(\"%Y-%m-%dT%H:%M:%S\")\\n\\n        if \\'{\\' + NSFOLIA + \\'}src\\' not in attribs: #do not override if caller already set it\\n            if self.src:\\n                attribs[\\'{\\' + NSFOLIA + \\'}src\\'] = self.src\\n\\n        if \\'{\\' + NSFOLIA + \\'}speaker\\' not in attribs: #do not override if caller already set it\\n            if self.speaker:\\n                attribs[\\'{\\' + NSFOLIA + \\'}speaker\\'] = self.speaker\\n\\n        if \\'{\\' + NSFOLIA + \\'}begintime\\' not in attribs: #do not override if caller already set it\\n            if self.begintime:\\n                attribs[\\'{\\' + NSFOLIA + \\'}begintime\\'] = \"%02d:%02d:%02d.%03d\" % self.begintime\\n\\n        if \\'{\\' + NSFOLIA + \\'}endtime\\' not in attribs: #do not override if caller already set it\\n            if self.endtime:\\n                attribs[\\'{\\' + NSFOLIA + \\'}endtime\\'] = \"%02d:%02d:%02d.%03d\" % self.endtime\\n\\n        if \\'{\\' + NSFOLIA + \\'}textclass\\' not in attribs: #do not override if caller already set it\\n            if self.textclass and self.textclass != \"current\":\\n                attribs[\\'{\\' + NSFOLIA + \\'}textclass\\'] = self.textclass\\n\\n        if \\'{\\' + NSFOLIA + \\'}metadata\\' not in attribs: #do not override if caller already set it\\n            if self.metadata:\\n                attribs[\\'{\\' + NSFOLIA + \\'}metadata\\'] = self.metadata\\n\\n        if self.XLINK:\\n            if self.href:\\n                attribs[\\'{http://www.w3.org/1999/xlink}href\\'] = self.href\\n                if not self.xlinktype:\\n                    attribs[\\'{http://www.w3.org/1999/xlink}type\\'] = \"simple\"\\n            if self.xlinktype:\\n                attribs[\\'{http://www.w3.org/1999/xlink}type\\'] = self.xlinktype\\n            if self.xlinklabel:\\n                attribs[\\'{http://www.w3.org/1999/xlink}label\\'] = self.xlinklabel\\n            if self.xlinkrole:\\n                attribs[\\'{http://www.w3.org/1999/xlink}role\\'] = self.xlinkrole\\n            if self.xlinkshow:\\n                attribs[\\'{http://www.w3.org/1999/xlink}show\\'] = self.xlinkshow\\n            if self.xlinktitle:\\n                attribs[\\'{http://www.w3.org/1999/xlink}title\\'] = self.xlinktitle\\n\\n        omitchildren =  []\\n\\n        #Are there predetermined Features in ACCEPTED_DATA?\\n        for c in self.ACCEPTED_DATA:\\n            if issubclass(c, Feature) and c.SUBSET:\\n                #Do we have any of those?\\n                for c2 in self.data:\\n                    if c2.__class__ is c and c.SUBSET == c2.SUBSET and c2.cls:\\n                        #Yes, serialize them as attributes\\n                        attribs[c2.SUBSET] = c2.cls\\n                        omitchildren.append(c2) #and skip them as elements\\n                        break #only one\\n\\n        e  = makeelement(E, \\'{\\' + NSFOLIA + \\'}\\' + self.XMLTAG, **attribs)\\n\\n\\n\\n        if not skipchildren and self.data:\\n            #append children,\\n            # we want make sure that text elements are in the right order, \\'current\\' class first\\n            # so we first put them in  a list\\n            textelements = []\\n            otherelements = []\\n            for child in self:\\n                if isinstance(child, TextContent):\\n                    if child.cls == \\'current\\':\\n                        textelements.insert(0, child)\\n                    else:\\n                        textelements.append(child)\\n                elif not child in omitchildren:\\n                    otherelements.append(child)\\n            for child in textelements+otherelements:\\n                if (self.TEXTCONTAINER or self.PHONCONTAINER) and isstring(child):\\n                    if len(e) == 0:\\n                        if e.text:\\n                            e.text += child\\n                        else:\\n                            e.text = child\\n                    else:\\n                        #add to tail of last child\\n                        if e[-1].tail:\\n                            e[-1].tail += child\\n                        else:\\n                            e[-1].tail = child\\n\\n                else:\\n                    xml = child.xml() #may return None in rare occassions, meaning we wan to skip\\n                    if not xml is None:\\n                        e.append(xml)\\n\\n        if elements: #extra elements\\n            for e2 in elements:\\n                if isinstance(e2, str) or (sys.version < \\'3\\' and isinstance(e2, unicode)):\\n                    if e.text is None:\\n                        e.text = e2\\n                    else:\\n                        e.text += e2\\n                else:\\n                    e.append(e2)\\n        return e', 'search_document:         \\n        jsonnode = {}\\n\\n        jsonnode[\\'type\\'] = self.XMLTAG\\n        if self.id:\\n            jsonnode[\\'id\\'] = self.id\\n        if self.set:\\n            jsonnode[\\'set\\'] = self.set\\n        if self.cls:\\n            jsonnode[\\'class\\'] = self.cls\\n        if self.annotator:\\n            jsonnode[\\'annotator\\'] = self.annotator\\n        if self.annotatortype:\\n            if self.annotatortype == AnnotatorType.AUTO:\\n                jsonnode[\\'annotatortype\\'] = \"auto\"\\n            elif self.annotatortype == AnnotatorType.MANUAL:\\n                jsonnode[\\'annotatortype\\'] = \"manual\"\\n        if self.confidence is not None:\\n            jsonnode[\\'confidence\\'] = self.confidence\\n        if self.n:\\n            jsonnode[\\'n\\'] = self.n\\n        if self.auth:\\n            jsonnode[\\'auth\\'] = self.auth\\n        if self.datetime:\\n            jsonnode[\\'datetime\\'] = self.datetime.strftime(\"%Y-%m-%dT%H:%M:%S\")\\n\\n        if recurse: #pylint: disable=too-many-nested-blocks\\n            jsonnode[\\'children\\'] = []\\n            if self.TEXTCONTAINER:\\n                jsonnode[\\'text\\'] = self.text()\\n            if self.PHONCONTAINER:\\n                jsonnode[\\'phon\\'] = self.phon()\\n            for child in self:\\n                if self.TEXTCONTAINER and isstring(child):\\n                    jsonnode[\\'children\\'].append(child)\\n                elif not self.PHONCONTAINER:\\n                    #check ignore list\\n                    ignore = False\\n                    if ignorelist:\\n                        for e in ignorelist:\\n                            if isinstance(child,e):\\n                                ignore = True\\n                                break\\n                    if not ignore:\\n                        jsonnode[\\'children\\'].append(child.json(attribs,recurse,ignorelist))\\n\\n        if attribs:\\n            for attrib in attribs:\\n                jsonnode[attrib] = attribs\\n\\n        return jsonnode', \"search_document:         \\n        s = ElementTree.tostring(self.xml(), xml_declaration=False, pretty_print=pretty_print, encoding='utf-8')\\n        if sys.version < '3':\\n            if isinstance(s, str):\\n                s = unicode(s,'utf-8') #pylint: disable=un            if isinstance(s,bytes):\\n                s = str(s,'utf-8')\\n\\n        s = s.replace('ns0:','') #ugly patch to get rid of namespace prefix\\n        s = s.replace(':ns0','')\\n        return s\", 'search_document:             Class (class): The class to select; any python class (not instance) subclassed off :class:`AbstractElement`\\n            Set (str): The set to match against, only elements pertaining to this set will be returned. If set to None (            Elements (instances derived from :class:`AbstractElement`)\\n\\n        Example::\\n\\n            for sense in text.select(folia.Sense, \\'cornetto\\', True, [folia.Original, folia.Suggestion, folia.Alternative] ):\\n                ..\\n\\n        \"\"\"\\n\\n        #if ignorelist is True:\\n        #    ignorelist =             node = self\\n        for e in self.data: #pylint: disable=too-many-nested-blocks\\n            if (not self.TEXTCONTAINER and not self.PHONCONTAINER) or isinstance(e, AbstractElement):\\n                if ignore is True:\\n                    try:\\n                        if not e.auth:\\n                            continue\\n                    except AttributeError:\\n                        #not all elements have auth attribute..\\n                        pass\\n                elif ignore: #list\\n                    doignore = False\\n                    for c in ignore:\\n                        if c is True:\\n                            try:\\n                                if not e.auth:\\n                                    doignore =True\\n                                    break\\n                            except AttributeError:\\n                                #not all elements have auth attribute..\\n                                pass\\n                        elif c == e.__class__ or issubclass(e.__class__,c):\\n                            doignore = True\\n                            break\\n                    if doignore:\\n                        continue\\n\\n                if isinstance(e, Class):\\n                    if not set is None:\\n                        try:\\n                            if e.set != set:\\n                                continue\\n                        except AttributeError:\\n                            continue\\n                    yield e\\n                if recursive:\\n                    for e2 in e.select(Class, set, recursive, ignore, e):\\n                        if not set is None:\\n                            try:\\n                                if e2.set != set:\\n                                    continue\\n                            except AttributeError:\\n                                continue\\n                        yield e2', 'search_document:         \\n        return sum(1 for i in self.select(Class,set,recursive,ignore,node) )', 'search_document:             if  e not in founditems: #prevent going in recursive loops\\n                l.append(e)\\n                if isinstance(e, AbstractElement):\\n                    l += e.items(l)\\n        return l', 'search_document:         \\n        if self.metadata:\\n            d =  self.doc.submetadata[self.metadata]\\n        elif self.parent:\\n            d =  self.parent.getmetadata()\\n        elif self.doc:\\n            d =  self.doc.metadata\\n        else:\\n            return None\\n        if key:\\n            return d[key]\\n        else:\\n            return d', 'search_document:         \\n\\n        #breadth first search\\n        for i, c in enumerate(self.data):\\n            if c is child:\\n                return i\\n        if recursive:  #pylint: disable=too-many-nested-blocks\\n            for i, c in enumerate(self.data):\\n                if ignore is True:\\n                    try:\\n                        if not c.auth:\\n                            continue\\n                    except AttributeError:\\n                        #not all elements have auth attribute..\\n                        pass\\n                elif ignore: #list\\n                    doignore = False\\n                    for e in ignore:\\n                        if e is True:\\n                            try:\\n                                if not c.auth:\\n                                    doignore =True\\n                                    break\\n                            except AttributeError:\\n                                #not all elements have auth attribute..\\n                                pass\\n                        elif e == c.__class__ or issubclass(c.__class__,e):\\n                            doignore = True\\n                            break\\n                    if doignore:\\n                        continue\\n                if isinstance(c, AbstractElement):\\n                    j = c.getindex(child, recursive)\\n                    if j != -1:\\n                        return i #yes, i ... not j!\\n        return -1', 'search_document:         \\n        try:\\n            ancestor = next(commonancestors(AbstractElement, self, other))\\n        except StopIteration:\\n            raise Exception(\"Elements share no common ancestor\")\\n        #now we just do a depth first search and see who comes first\\n                    if e is self:\\n                return True\\n            elif e is other:\\n                return False\\n            return None\\n        result = ancestor.depthfirstsearch(callback)\\n        if result is None:\\n            raise Exception(\"Unable to find relation between elements! (shouldn\\'t happen)\")\\n        return result', 'search_document:         \\n        result = function(self)\\n        if result is not None:\\n            return result\\n        for e in self:\\n            result = e.depthfirstsearch(function)\\n            if result is not None:\\n                return result\\n        return None', \"search_document:         \\n        if Class is True: Class = self.__class__\\n        if scope is True: scope = STRUCTURESCOPE\\n\\n        structural = Class is not None and issubclass(Class,AbstractStructureElement)\\n\\n        if reverse:\\n            order = reversed\\n            descendindex = -1\\n        else:\\n            order = lambda x: x #pylint: disable=re                returnnext = False\\n                for e in order(parent):\\n                    if e is child:\\n                        #we found the current item, next item will be the one to return\\n                        returnnext = True\\n                    elif returnnext and e.auth and not isinstance(e,AbstractAnnotationLayer) and (not structural or (structural and (not isinstance(e,(AbstractTokenAnnotation,TextContent)) ) )):\\n                        if structural and isinstance(e,Correction):\\n                            if not list(e.select(AbstractStructureElement)): #skip-over non-structural correction\\n                                continue\\n\\n                        if Class is None or (isinstance(Class,tuple) and (any(isinstance(e,C) for C in Class))) or isinstance(e,Class):\\n                            return e\\n                        else:\\n                            #this is not yet the element of the type we are looking for, we are going to descend again in the very leftmost (rightmost if reversed) branch only\\n                            while e.data:\\n                                e = e.data[descendindex]\\n                                if not isinstance(e, AbstractElement):\\n                                    return None #we've gone too far\\n                                if e.auth and not isinstance(e,AbstractAnnotationLayer):\\n                                    if Class is None or (isinstance(Class,tuple) and (any(isinstance(e,C) for C in Class))) or isinstance(e,Class):\\n                                        return e\\n                                    else:\\n                                        #descend deeper\\n                                        continue\\n                        return None\\n\\n            #generational iteration\\n            child = parent\\n            if scope is not None and child.__class__ in scope:\\n                #you shall not pass!\\n                break\\n            parent = parent.parent\\n\\n        return None\", 'search_document:         \\n        return self.next(Class,scope, True)', 'search_document:         \\n\\n        if size == 0: return [] #for efficiency\\n\\n        context = []\\n        e = self\\n        while len(context) < size:\\n            e = e.previous(True,scope)\\n            if not e: break\\n            context.append(e)\\n\\n        if placeholder:\\n            while len(context) < size:\\n                context.append(placeholder)\\n\\n        context.reverse()\\n        return context', 'search_document:         \\n\\n        if size == 0: return [] #for efficiency\\n\\n        context = []\\n        e = self\\n        while len(context) < size:\\n            e = e.next(True,scope)\\n            if not e: break\\n            context.append(e)\\n\\n        if placeholder:\\n            while len(context) < size:\\n                context.append(placeholder)\\n\\n        return context', 'search_document:         \\n        return self.leftcontext(size, placeholder,scope) + [self] + self.rightcontext(size, placeholder,scope)', 'search_document:         \\n\\n        E = ElementMaker(namespace=\"http://relaxng.org/ns/structure/1.0\",nsmap={None:\\'http://relaxng.org/ns/structure/1.0\\' , \\'folia\\': \"http://ilk.uvt.nl/folia\", \\'xml\\' : \"http://www.w3.org/XML/1998/namespace\",\\'a\\':\"http://relaxng.org/ns/annotation/0.9\" })\\n\\n        if origclass: cls = origclass\\n\\n        preamble = []\\n        try:\\n            if cls.__doc__:\\n                E2 = ElementMaker(namespace=\"http://relaxng.org/ns/annotation/0.9\", nsmap={\\'a\\':\\'http://relaxng.org/ns/annotation/0.9\\'} )\\n                preamble.append(E2.documentation(cls.__doc__))\\n        except AttributeError:\\n            pass\\n\\n        if cls.REQUIRED_ATTRIBS is None: cls.REQUIRED_ATTRIBS = () #bit hacky\\n        if cls.OPTIONAL_ATTRIBS is None: cls.OPTIONAL_ATTRIBS = () #bit hacky\\n\\n\\n        attribs = [ ]\\n        if cls.REQUIRED_ATTRIBS and Attrib.ID in cls.REQUIRED_ATTRIBS:\\n            attribs.append( E.attribute(E.data(type=\\'ID\\',datatypeLibrary=\\'http://www.w3.org/2001/XMLSchema-datatypes\\'),name=\\'id\\', ns=\"http://www.w3.org/XML/1998/namespace\") )\\n        elif Attrib.ID in cls.OPTIONAL_ATTRIBS:\\n            attribs.append( E.optional( E.attribute(E.data(type=\\'ID\\',datatypeLibrary=\\'http://www.w3.org/2001/XMLSchema-datatypes\\'), name=\\'id\\', ns=\"http://www.w3.org/XML/1998/namespace\") ) )\\n        if Attrib.CLASS in cls.REQUIRED_ATTRIBS:\\n            #Set is a tough one, we can\\'t require it as it may be             attribs.append( E.optional( E.attribute(E.data(type=\\'string\\',datatypeLibrary=\\'http://www.w3.org/2001/XMLSchema-datatypes\\'),name=\\'class\\') ) )\\n            attribs.append( E.optional( E.attribute(E.data(type=\\'string\\',datatypeLibrary=\\'http://www.w3.org/2001/XMLSchema-datatypes\\'), name=\\'set\\' ) ) )\\n        if Attrib.ANNOTATOR in cls.REQUIRED_ATTRIBS or Attrib.ANNOTATOR in cls.OPTIONAL_ATTRIBS:\\n            #Similarly tough\\n            attribs.append( E.optional( E.attribute(E.data(type=\\'string\\',datatypeLibrary=\\'http://www.w3.org/2001/XMLSchema-datatypes\\'), name=\\'annotator\\') ) )\\n            attribs.append( E.optional( E.attribute(name=\\'annotatortype\\') ) )\\n        if Attrib.CONFIDENCE in cls.REQUIRED_ATTRIBS:\\n            attribs.append(  E.attribute(E.data(type=\\'double\\',datatypeLibrary=\\'http://www.w3.org/2001/XMLSchema-datatypes\\'), name=\\'confidence\\') )\\n        elif Attrib.CONFIDENCE in cls.OPTIONAL_ATTRIBS:\\n            attribs.append(  E.optional( E.attribute(E.data(type=\\'double\\',datatypeLibrary=\\'http://www.w3.org/2001/XMLSchema-datatypes\\'), name=\\'confidence\\') ) )\\n        if Attrib.N in cls.REQUIRED_ATTRIBS:\\n            attribs.append( E.attribute( E.data(type=\\'string\\',datatypeLibrary=\\'http://www.w3.org/2001/XMLSchema-datatypes\\'),name=\\'n\\') )\\n        elif Attrib.N in cls.OPTIONAL_ATTRIBS:\\n            attribs.append( E.optional( E.attribute( E.data(type=\\'string\\',datatypeLibrary=\\'http://www.w3.org/2001/XMLSchema-datatypes\\'),name=\\'n\\') ) )\\n        if Attrib.DATETIME in cls.REQUIRED_ATTRIBS:\\n            attribs.append( E.attribute(E.data(type=\\'dateTime\\',datatypeLibrary=\\'http://www.w3.org/2001/XMLSchema-datatypes\\'), name=\\'datetime\\') )\\n        elif Attrib.DATETIME in cls.OPTIONAL_ATTRIBS:\\n            attribs.append( E.optional( E.attribute( E.data(type=\\'dateTime\\',datatypeLibrary=\\'http://www.w3.org/2001/XMLSchema-datatypes\\'),  name=\\'datetime\\') ) )\\n        if Attrib.BEGINTIME in cls.REQUIRED_ATTRIBS:\\n            attribs.append(E.attribute(name=\\'begintime\\') )\\n        elif Attrib.BEGINTIME in cls.OPTIONAL_ATTRIBS:\\n            attribs.append( E.optional( E.attribute(name=\\'begintime\\') ) )\\n        if Attrib.ENDTIME in cls.REQUIRED_ATTRIBS:\\n            attribs.append(E.attribute(name=\\'endtime\\') )\\n        elif Attrib.ENDTIME in cls.OPTIONAL_ATTRIBS:\\n            attribs.append( E.optional( E.attribute(name=\\'endtime\\') ) )\\n        if Attrib.SRC in cls.REQUIRED_ATTRIBS:\\n            attribs.append(E.attribute(E.data(type=\\'anyURI\\',datatypeLibrary=\\'http://www.w3.org/2001/XMLSchema-datatypes\\'),name=\\'src\\') )\\n        elif Attrib.SRC in cls.OPTIONAL_ATTRIBS:\\n            attribs.append( E.optional( E.attribute(E.data(type=\\'anyURI\\',datatypeLibrary=\\'http://www.w3.org/2001/XMLSchema-datatypes\\'),name=\\'src\\') ) )\\n        if Attrib.SPEAKER in cls.REQUIRED_ATTRIBS:\\n            attribs.append(E.attribute(E.data(type=\\'string\\',datatypeLibrary=\\'http://www.w3.org/2001/XMLSchema-datatypes\\'), name=\\'speaker\\') )\\n        elif Attrib.SPEAKER in cls.OPTIONAL_ATTRIBS:\\n            attribs.append( E.optional( E.attribute(E.data(type=\\'string\\',datatypeLibrary=\\'http://www.w3.org/2001/XMLSchema-datatypes\\'),name=\\'speaker\\') ) )\\n        if Attrib.TEXTCLASS in cls.REQUIRED_ATTRIBS:\\n            attribs.append(E.attribute(name=\\'textclass\\') )\\n        elif Attrib.TEXTCLASS in cls.OPTIONAL_ATTRIBS:\\n            attribs.append( E.optional( E.attribute(name=\\'textclass\\') ) )\\n        if Attrib.METADATA in cls.REQUIRED_ATTRIBS:\\n            attribs.append(E.attribute(name=\\'metadata\\') )\\n        elif Attrib.METADATA in cls.OPTIONAL_ATTRIBS:\\n            attribs.append( E.optional( E.attribute(name=\\'metadata\\') ) )\\n        if cls.XLINK:\\n            attribs += [ #loose interpretation of specs, not checking whether xlink combinations are valid\\n                    E.optional(E.attribute(name=\\'href\\',ns=\"http://www.w3.org/1999/xlink\"),E.attribute(name=\\'type\\',ns=\"http://www.w3.org/1999/xlink\") ),\\n                    E.optional(E.attribute(name=\\'role\\',ns=\"http://www.w3.org/1999/xlink\")),\\n                    E.optional(E.attribute(name=\\'title\\',ns=\"http://www.w3.org/1999/xlink\")),\\n                    E.optional(E.attribute(name=\\'label\\',ns=\"http://www.w3.org/1999/xlink\")),\\n                    E.optional(E.attribute(name=\\'show\\',ns=\"http://www.w3.org/1999/xlink\")),\\n            ]\\n\\n        attribs.append( E.optional( E.attribute( name=\\'auth\\' ) ) )\\n\\n\\n\\n        if extraattribs:\\n            for e in extraattribs:\\n                attribs.append(e) #s\\n\\n        attribs.append( E.ref(name=\"allow_foreign_attributes\") )\\n\\n\\n        elements = [] #(including attributes)\\n        if cls.TEXTCONTAINER or cls.PHONCONTAINER:\\n            elements.append( E.text())\\n            #We actually want to require non-empty text (E.text() is not sufficient)\\n            #but this is not solved yet, see https://github.com/proycon/folia/issues/19\\n            #elements.append( E.data(E.param(r\".+\",name=\"pattern\"),type=\\'string\\'))\\n            #elements.append( E.data(E.param(r\"(.|\\\\n|\\\\r)*\\\\S+(.|\\\\n|\\\\r)*\",name=\"pattern\"),type=\\'string\\'))\\n        done = {}\\n        if includechildren and cls.ACCEPTED_DATA: #pylint: disable=too-many-nested-blocks\\n            for c in cls.ACCEPTED_DATA:\\n                if c.__name__[:8] == \\'Abstract\\' and inspect.isclass(c):\\n                    for c2 in globals().values():\\n                        try:\\n                            if inspect.isclass(c2) and issubclass(c2, c):\\n                                try:\\n                                    if c2.XMLTAG and c2.XMLTAG not in done:\\n                                        if c2.OCCURRENCES == 1:\\n                                            elements.append( E.optional( E.ref(name=c2.XMLTAG) ) )\\n                                        else:\\n                                            elements.append( E.zeroOrMore( E.ref(name=c2.XMLTAG) ) )\\n                                            if c2.XMLTAG == \\'item\\': #nasty hack for backward compatibility with deprecated listitem element\\n                                                elements.append( E.zeroOrMore( E.ref(name=\\'listitem\\') ) )\\n                                        done[c2.XMLTAG] = True\\n                                except AttributeError:\\n                                    continue\\n                        except TypeError:\\n                            pass\\n                elif issubclass(c, Feature) and c.SUBSET:\\n                    attribs.append( E.optional( E.attribute(name=c.SUBSET)))  #features as attributes\\n                else:\\n                    try:\\n                        if c.XMLTAG and c.XMLTAG not in done:\\n                            if cls.REQUIRED_DATA and c in cls.REQUIRED_DATA:\\n                                if c.OCCURRENCES == 1:\\n                                    elements.append( E.ref(name=c.XMLTAG) )\\n                                else:\\n                                    elements.append( E.oneOrMore( E.ref(name=c.XMLTAG) ) )\\n                            elif c.OCCURRENCES == 1:\\n                                elements.append( E.optional( E.ref(name=c.XMLTAG) ) )\\n                            else:\\n                                elements.append( E.zeroOrMore( E.ref(name=c.XMLTAG) ) )\\n                                if c.XMLTAG == \\'item\\':\\n                                    #nasty hack for backward compatibility with deprecated listitem element\\n                                    elements.append( E.zeroOrMore( E.ref(name=\\'listitem\\') )  )\\n                            done[c.XMLTAG] = True\\n                    except AttributeError:\\n                        continue\\n\\n        if extraelements:\\n            for e in extraelements:\\n                elements.append( e )\\n\\n        if elements:\\n            if len(elements) > 1:\\n                attribs.append( E.interleave(*elements) )\\n            else:\\n                attribs.append( *elements )\\n\\n        if not attribs:\\n            attribs.append( E.empty() )\\n\\n        if cls.XMLTAG in (\\'desc\\',\\'comment\\'):\\n            return E.            return E.define( E.element(*(preamble + attribs), **{\\'name\\': cls.XMLTAG}), name=cls.XMLTAG, ns=NSFOLIA)', 'search_document:             * ``node`` - XML Element\\n            * ``doc`` - Document\\n\\n        Returns:\\n            An instance of the current Class.\\n        \"\"\"\\n\\n        assert issubclass(Class, AbstractElement)\\n\\n        if doc.preparsexmlcallback:\\n            result = doc.preparsexmlcallback(node)\\n            if not result:\\n                return None\\n            if isinstance(result, AbstractElement):\\n                return result\\n\\n\\n\\n        dcoi = node.tag.startswith(\\'{\\' + NSDCOI + \\'}\\')\\n        args = []\\n        if not kwargs: kwargs = {}\\n        text = None #for dcoi support\\n        if (Class.TEXTCONTAINER or Class.PHONCONTAINER) and node.text:\\n            args.append(node.text)\\n\\n\\n        for subnode in node: #pylint: disable=too-many-nested-blocks\\n            #don\\'t trip over comments\\n            if isinstance(subnode, ElementTree._Comment): #pylint: disable=protected-access\\n                if (Class.TEXTCONTAINER or Class.PHONCONTAINER) and subnode.tail:\\n                    args.append(subnode.tail)\\n            else:\\n                if subnode.tag.startswith(\\'{\\' + NSFOLIA + \\'}\\'):\\n                    if doc.debug >= 1: print(\"[PyNLPl FoLiA DEBUG] Processing subnode \" + subnode.tag[nslen:],file=stderr)\\n                    try:\\n                        e = doc.parsexml(subnode, Class)\\n                    except ParseError as e:\\n                        raise #just re-raise deepest parseError\\n                    except Exception as e:\\n                        #Python 3 will preserve full original traceback, Python 2 does not, original cause is explicitly passed to ParseError anyway:\\n                        raise ParseError(\"FoLiA exception in handling of <\" + subnode.tag[len(NSFOLIA)+2:] + \"> @ line \" + str(subnode.sourceline) + \": [\" + e.__class__.__name__ + \"] \" + str(e), cause=e)\\n                    if e is not None:\\n                        args.append(e)\\n                    if (Class.TEXTCONTAINER or Class.PHONCONTAINER) and subnode.tail:\\n                        args.append(subnode.tail)\\n                elif subnode.tag.startswith(\\'{\\' + NSDCOI + \\'}\\'):\\n                    #Dcoi support\\n                    if Class is Text and subnode.tag[nslendcoi:] == \\'body\\':\\n                        for subsubnode in subnode:\\n                            if doc.debug >= 1: print(\"[PyNLPl FoLiA DEBUG] Processing DCOI subnode \" + subnode.tag[nslendcoi:],file=stderr)\\n                            e = doc.parsexml(subsubnode, Class)\\n                            if e is not None:\\n                                args.append(e)\\n                    else:\\n                        if doc.debug >= 1: print( \"[PyNLPl FoLiA DEBUG] Processing DCOI subnode \" + subnode.tag[nslendcoi:],file=stderr)\\n                        e = doc.parsexml(subnode, Class)\\n                        if e is not None:\\n                            args.append(e)\\n                elif doc.debug >= 1:\\n                    print(\"[PyNLPl FoLiA DEBUG] Ignoring subnode outside of FoLiA namespace: \" + subnode.tag,file=stderr)\\n\\n\\n\\n        if dcoi:\\n            dcoipos = dcoilemma = dcoicorrection = dcoicorrectionoriginal = None\\n        for key, value in node.attrib.items():\\n            if key[0] == \\'{\\' or key ==\\'XMLid\\':\\n                if key == \\'{http://www.w3.org/XML/1998/namespace}id\\' or key == \\'XMLid\\':\\n                    key = \\'id\\'\\n                elif key.startswith( \\'{\\' + NSFOLIA + \\'}\\'):\\n                    key = key[nslen:]\\n                    if key == \\'id\\':\\n                        #ID in FoLiA namespace is always a reference, passed in kwargs as follows:\\n                        key = \\'idref\\'\\n                elif Class.XLINK and key.startswith(\\'{http://www.w3.org/1999/xlink}\\'):\\n                    key = key[30:]\\n                    if key != \\'href\\':\\n                        key = \\'xlink\\' + key #xlinktype, xlinkrole, xlinklabel, xlinkshow, etc..\\n                elif key.startswith(\\'{\\' + NSDCOI + \\'}\\'):\\n                    key = key[nslendcoi:]\\n\\n            #D-Coi support:\\n            if dcoi:\\n                if Class is Word and key == \\'pos\\':\\n                    dcoipos = value\\n                    continue\\n                elif Class is Word and  key == \\'lemma\\':\\n                    dcoilemma = value\\n                    continue\\n                elif Class is Word and  key == \\'correction\\':\\n                    dcoicorrection = value #class\\n                    continue\\n                elif Class is Word and  key == \\'original\\':\\n                    dcoicorrectionoriginal = value\\n                    continue\\n                elif Class is Gap and  key == \\'reason\\':\\n                    key = \\'class\\'\\n                elif Class is Gap and  key == \\'hand\\':\\n                    key = \\'annotator\\'\\n                elif Class is Division and  key == \\'type\\':\\n                    key = \\'cls\\'\\n\\n            kwargs[key] = value\\n\\n        #D-Coi support:\\n        if dcoi and TextContent in Class.ACCEPTED_DATA and node.text:\\n            text = node.text.strip()\\n\\n            kwargs[\\'text\\'] = text\\n            if not AnnotationType.TOKEN in doc.annotation                doc.declare(AnnotationType.TOKEN, set=\\'http://ilk.uvt.nl/folia/sets/ilktok.foliaset\\')\\n\\n        if doc.debug >= 1: print(\"[PyNLPl FoLiA DEBUG] Found \" + node.tag[nslen:],file=stderr)\\n        instance = Class(doc, *args, **kwargs)\\n        #if id:\\n        #    if doc.debug >= 1: print >>stderr, \"[PyNLPl FoLiA DEBUG] Adding to index: \" + id\\n        #    doc.index[id] = instance\\n        if dcoi:\\n            if dcoipos:\\n                if not AnnotationType.POS in doc.annotation                    doc.declare(AnnotationType.POS, set=\\'http://ilk.uvt.nl/folia/sets/cgn-legacy.foliaset\\')\\n                instance.append( PosAnnotation(doc, cls=dcoipos) )\\n            if dcoilemma:\\n                if not AnnotationType.LEMMA in doc.annotation                    doc.declare(AnnotationType.LEMMA, set=\\'http://ilk.uvt.nl/folia/sets/mblem-nl.foliaset\\')\\n                instance.append( LemmaAnnotation(doc, cls=dcoilemma) )\\n            if dcoicorrection and dcoicorrectionoriginal and text:\\n                if not AnnotationType.CORRECTION in doc.annotation                    doc.declare(AnnotationType.CORRECTION, set=\\'http://ilk.uvt.nl/folia/sets/dcoi-corrections.foliaset\\')\\n                instance.correct(generate_id_in=instance, cls=dcoicorrection, original=dcoicorrectionoriginal, new=text)\\n\\n        if doc.parsexmlcallback:\\n            result = doc.parsexmlcallback(instance)\\n            if not result:\\n                return None\\n            if isinstance(result, AbstractElement):\\n                return result\\n\\n        return instance', 'search_document:         \\n        if not isinstance(child, AbstractElement):\\n            raise ValueError(\"Expected AbstractElement, got \" + str(type(child)))\\n        if child.parent == self:\\n            child.parent = None\\n        self.data.remove(child)\\n        #delete from index\\n        if child.id and self.doc and child.id in self.doc.index:\\n            del self.doc.index[child.id]', 'search_document:         \\n        e = self.parent\\n\\n        while e:\\n            if isinstance(e, Correction):\\n                return e\\n            if isinstance(e, AbstractStructureElement):\\n                break\\n            e = e.parent\\n        return None', 'search_document:         \\n\\n        if \\'insertindex_offset\\' in kwargs:\\n            del kwargs[\\'insertindex_offset\\'] #dealt with in an earlier stage\\n\\n        if \\'confidence\\' in kwargs and kwargs[\\'confidence\\'] is None:\\n            del kwargs[\\'confidence\\']\\n\\n        if \\'reuse\\' in kwargs:\\n            #reuse an existing correction instead of making a new one\\n            if isinstance(kwargs[\\'reuse\\'], Correction):\\n                c = kwargs[\\'reuse\\']\\n            else: #assume it\\'s an index\\n                try:\\n                    c = self.doc.index[kwargs[\\'reuse\\']]\\n                    assert isinstance(c, Correction)\\n                except:\\n                    raise ValueError(\"reuse= must point to an existing correction (id or instance)! Got \" + str(kwargs[\\'reuse\\']))\\n\\n            suggestionsonly = (not c.hasnew(True) and not c.hasoriginal(True) and c.hassuggestions(True))\\n\\n            if \\'new\\' in kwargs and c.hascurrent():\\n                #can\\'t add new if there\\'s current, so first set original to current, and then delete current\\n\\n                if \\'current\\' in kwargs:\\n                    raise Exception(\"Can\\'t set both new= and current= !\")\\n                if \\'original\\' not in kwargs:\\n                    kwargs[\\'original\\'] = c.current()\\n\\n                c.remove(c.current())\\n        else:\\n            if \\'id\\' not in kwargs and \\'generate_id_in\\' not in kwargs:\\n                kwargs[\\'generate_id_in\\'] = self\\n            kwargs2 = copy(kwargs)\\n            for x in [\\'new\\',\\'original\\',\\'suggestion\\', \\'suggestions\\',\\'current\\', \\'insertindex\\',\\'nooriginal\\']:\\n                if x in kwargs2:\\n                    del kwargs2[x]\\n            c = Correction(self.doc, **kwargs2)\\n\\n        addnew = False\\n        if \\'insertindex\\' in kwargs:\\n            insertindex = int(kwargs[\\'insertindex\\'])\\n            del kwargs[\\'insertindex\\']\\n        else:\\n            insertindex = -1 #append\\n\\n        if \\'nooriginal\\' in kwargs and kwargs[\\'nooriginal\\']:\\n            nooriginal = True\\n            del kwargs[\\'nooriginal\\']\\n        else:\\n            nooriginal = False\\n\\n        if \\'current\\' in kwargs:\\n            if \\'original\\' in kwargs or \\'new\\' in kwargs: raise Exception(\"When setting current=, original= and new= can not be set!\")\\n            if not isinstance(kwargs[\\'current\\'], list) and not isinstance(kwargs[\\'current\\'], tuple): kwargs[\\'current\\'] = [kwargs[\\'current\\']] #support both lists (for multiple elements at once), as well as single element\\n            c.replace(Current(self.doc, *kwargs[\\'current\\']))\\n            for o in kwargs[\\'current\\']: #delete current from current element\\n                if o in self and isinstance(o, AbstractElement): #pylint: disable=unsupported-membership-test\\n                    if insertindex == -1: insertindex = self.data.index(o)\\n                    self.remove(o)\\n            del kwargs[\\'current\\']\\n        if \\'new\\' in kwargs:\\n            if not isinstance(kwargs[\\'new\\'], list) and not isinstance(kwargs[\\'new\\'], tuple): kwargs[\\'new\\'] = [kwargs[\\'new\\']] #support both lists (for multiple elements at once), as well as single element\\n            addnew = New(self.doc, *kwargs[\\'new\\']) #pylint: disable=re            if not isinstance(kwargs[\\'original\\'], list) and not isinstance(kwargs[\\'original\\'], tuple): kwargs[\\'original\\'] = [kwargs[\\'original\\']] #support both lists (for multiple elements at once), as well as single element\\n            c.replace(Original(self.doc, *kwargs[\\'original\\']))\\n            for o in kwargs[\\'original\\']: #delete original from current element\\n                if o in self and isinstance(o, AbstractElement): #pylint: disable=unsupported-membership-test\\n                    if insertindex == -1: insertindex = self.data.index(o)\\n                    self.remove(o)\\n            for o in kwargs[\\'original\\']: #make sure IDs are still properly set after removal\\n                o.addtoindex()\\n            for current in c.select(Current):  #delete current if present\\n                c.remove(current)\\n            del kwargs[\\'original\\']\\n        elif addnew and not nooriginal:\\n            #original not specified, find automagically:\\n            original = []\\n            for new in addnew:\\n                kwargs2 = {}\\n                if isinstance(new, TextContent):\\n                    kwargs2[\\'cls\\'] = new.cls\\n                try:\\n                    set = new.set\\n                except AttributeError:\\n                    set = None\\n                #print(\"DEBUG: Finding replaceables within \" + str(repr(self)) + \" for \", str(repr(new)), \" set \" ,set , \" args \" ,repr(kwargs2),file=sys.stderr)\\n                replaceables = new.__class__.findreplaceables(self, set, **kwargs2)\\n                #print(\"DEBUG: \" , len(replaceables) , \" found\",file=sys.stderr)\\n                original += replaceables\\n            if not original:\\n                #print(\"DEBUG: \", self.xmlstring(),file=sys.stderr)\\n                raise Exception(\"No original= specified and unable to automatically infer on \" + str(repr(self)) + \" for \" + str(repr(new)) + \" with set \" + set)\\n            else:\\n                c.replace( Original(self.doc, *original))\\n                for current in c.select(Current):  #delete current if present\\n                    c.remove(current)\\n\\n        if addnew and not nooriginal:\\n            for original in c.original():\\n                if original in self: #pylint: disable=unsupported-membership-test\\n                    self.remove(original)\\n\\n        if \\'suggestion\\' in kwargs:\\n            kwargs[\\'suggestions\\'] = [kwargs[\\'suggestion\\']]\\n            del kwargs[\\'suggestion\\']\\n        if \\'suggestions\\' in kwargs:\\n            for suggestion in kwargs[\\'suggestions\\']:\\n                if isinstance(suggestion, Suggestion):\\n                    c.append(suggestion)\\n                elif isinstance(suggestion, list) or isinstance(suggestion, tuple):\\n                    c.append(Suggestion(self.doc, *suggestion))\\n                else:\\n                    c.append(Suggestion(self.doc, suggestion))\\n            del kwargs[\\'suggestions\\']\\n\\n\\n\\n\\n        if \\'reuse\\' in kwargs:\\n            if addnew and suggestionsonly:\\n                #What was previously only a suggestion, now becomes a real correction\\n                #If annotator, annotatortypes\\n                #are associated with the correction as a whole, move it to the suggestions\\n                #correction-wide annotator, annotatortypes might be overwritten\\n                for suggestion in c.suggestions():\\n                    if c.annotator and not suggestion.annotator:\\n                        suggestion.annotator = c.annotator\\n                    if c.annotatortype and not suggestion.annotatortype:\\n                        suggestion.annotatortype = c.annotatortype\\n\\n            if \\'annotator\\' in kwargs:\\n                c.annotator = kwargs[\\'annotator\\'] #pylint: disable=attribute-                c.annotatortype = kwargs[\\'annotatortype\\'] #pylint: disable=attribute-                c.confidence = float(kwargs[\\'confidence\\']) #pylint: disable=attribute-            c.addtoindex()\\n            if insertindex == -1:\\n                self.append(c)\\n            else:\\n                self.insert(insertindex, c)\\n        return c', 'search_document:         \\n        found = False\\n        for e in self.select(Class,set,True,            found = True\\n            yield e\\n        if not found:\\n            raise NoSuchAnnotation()', 'search_document:         \\n        return sum( 1 for _ in self.select(Class,set,True,default_ignore_annotations))', 'search_document:         \\n        \\n        for e in self.select(type,set,True,            return e\\n        raise NoSuchAnnotation()']\n",
            "['search_document:         \\n        e = super(AbstractStructureElement,self).append(child, *args, **kwargs)\\n        self._setmaxid(e)\\n        return e', 'search_document:         \\n        if index is None:\\n            return self.select(Word,None,True,            if index < 0:\\n                index = self.count(Word,None,True,                if i == index:\\n                    return e\\n            raise IndexError', 'search_document:         \\n        if index is None:\\n            return self.select(Paragraph,None,True,            if index < 0:\\n                index = self.count(Paragraph,None,True,                if i == index:\\n                    return e\\n            raise IndexError', 'search_document:         \\n        if index is None:\\n            return self.select(Sentence,None,True,            if index < 0:\\n                index = self.count(Sentence,None,True,                if i == index:\\n                    return e\\n            raise IndexError', 'search_document:         \\n        if inspect.isclass(annotationtype): annotationtype = annotationtype.ANNOTATIONTYPE\\n        return [ x for x in self.select(AbstractAnnotationLayer,set,False,True) if annotationtype is None or x.ANNOTATIONTYPE == annotationtype ]', 'search_document:         \\n        l = self.layers(annotationtype, set)\\n        return (len(l) > 0)', \"search_document:         \\n        if not attribs: attribs = {}\\n        if self.idref:\\n            attribs['id'] = self.idref\\n        return super(AbstractTextMarkup,self).xml(attribs,elements, skipchildren)\", \"search_document:         \\n        if not attribs: attribs = {}\\n        if self.idref:\\n            attribs['id'] = self.idref\\n        return super(AbstractTextMarkup,self).json(attribs,recurse, ignorelist)\", 'search_document:         \\n        return super(TextContent,self).text(normalize_spaces=normalize_spaces)', 'search_document:         \\n\\n        if self.offset is None: return None #nothing to test\\n        if self.ref:\\n            ref = self.doc[self.ref]\\n        else:\\n            ref = self.find            raise UnresolvableTextContent(\"Default reference for textcontent not found!\")\\n        elif not ref.hastext(self.cls):\\n            raise UnresolvableTextContent(\"Reference (ID \" + str(ref.id) + \") has no such text (class=\" + self.cls+\")\")\\n        elif validate and self.text() != ref.textcontent(self.cls).text()[self.offset:self.offset+len(self.data[0])]:\\n            raise UnresolvableTextContent(\"Reference (ID \" + str(ref.id) + \", class=\" + self.cls+\") found but no text match at specified offset (\"+str(self.offset)+\")! Expected \\'\" + self.text() + \"\\', got \\'\" + ref.textcontent(self.cls).text()[self.offset:self.offset+len(self.data[0])] +\"\\'\")\\n        else:\\n            #finally, we made it!\\n            return ref', 'search_document:         \\n        attribs = {}\\n        if not self.offset is None:\\n            attribs[\\'{\\' + NSFOLIA + \\'}offset\\'] = str(self.offset)\\n        if self.parent and self.ref:\\n            attribs[\\'{\\' + NSFOLIA + \\'}ref\\'] = self.ref\\n\\n        #if self.cls != \\'current\\' and not (self.cls == \\'original\\' and any( isinstance(x, Original) for x in self.ancestors() )  ):\\n        #    attribs[\\'{\\' + NSFOLIA + \\'}class\\'] = self.cls\\n        #else:\\n        #    if \\'{\\' + NSFOLIA + \\'}class\\' in attribs:\\n        #        del attribs[\\'{\\' + NSFOLIA + \\'}class\\']\\n        #return E.t(self.value, **attribs)\\n\\n        e = super(TextContent,self).xml(attribs,elements,skipchildren)\\n        if \\'{\\' + NSFOLIA + \\'}class\\' in e.attrib and e.attrib[\\'{\\' + NSFOLIA + \\'}class\\'] == \"current\":\\n            #delete \\'class=current\\'\\n            del e.attrib[\\'{\\' + NSFOLIA + \\'}class\\']\\n\\n        return e', 'search_document:         \\n\\n        if self.offset is None: return None #nothing to test\\n        if self.ref:\\n            ref = self.doc[self.ref]\\n        else:\\n            ref = self.find            raise UnresolvableTextContent(\"Default reference for phonetic content not found!\")\\n        elif not ref.hasphon(self.cls):\\n            raise UnresolvableTextContent(\"Reference has no such phonetic content (class=\" + self.cls+\")\")\\n        elif validate and self.phon() != ref.textcontent(self.cls).phon()[self.offset:self.offset+len(self.data[0])]:\\n            raise UnresolvableTextContent(\"Reference (class=\" + self.cls+\") found but no phonetic match at specified offset (\"+str(self.offset)+\")! Expected \\'\" + self.text() + \"\\', got \\'\" + ref.textcontent(self.cls).text()[self.offset:self.offset+len(self.data[0])] +\"\\'\")\\n        else:\\n            #finally, we made it!\\n            return ref', 'search_document:         \\n\\n        depth = 0\\n        e = self\\n        while True:\\n            if e.parent:\\n                e = e.parent #pylint: disable=re                #no parent, breaking\\n                return False\\n\\n            if isinstance(e,AbstractStructureElement) or isinstance(e,AbstractSubtokenAnnotation):\\n                depth += 1\\n                if depth == 2:\\n                    return e\\n\\n\\n        return False', \"search_document:         if 'cls' not in kwargs:\\n            kwargs['cls'] = 'current'\\n        replace = super(PhonContent, Class).findreplaceables(parent, set, **kwargs)\\n        replace = [ x for x in replace if x.cls == kwargs['cls']]\\n        del kwargs['cls'] #always delete what we processed\\n        return replace\", \"search_document:             kwargs['offset'] = int(node.attrib['offset'])\\n        if 'ref' in node.attrib:\\n            kwargs['ref'] = node.attrib['ref']\\n        return super(PhonContent,Class).parsexml(node,doc, **kwargs)\", 'search_document:         \\n        for layer in self.select(MorphologyLayer):\\n            for m in layer.select(Morpheme, set):\\n                yield m', 'search_document:         \\n        for layer in self.select(PhonologyLayer):\\n            for p in layer.select(Phoneme, set):\\n                yield p', 'search_document:         \\n        for layer in self.select(MorphologyLayer):\\n            for i, m in enumerate(layer.select(Morpheme, set)):\\n                if index == i:\\n                    return m\\n        raise NoSuchAnnotation', 'search_document:         \\n        for layer in self.select(PhonologyLayer):\\n            for i, p in enumerate(layer.select(Phoneme, set)):\\n                if index == i:\\n                    return p\\n        raise NoSuchAnnotation', 'search_document:         \\n\\n        if issubclass(type, AbstractAnnotationLayer):\\n            layerclass = type\\n        else:\\n            layerclass = ANNOTATIONTYPE2LAYERCLASS[type.ANNOTATIONTYPE]\\n        e = self\\n        while True:\\n            if not e.parent: break\\n            e = e.parent\\n            for layer in e.select(layerclass,set,False):\\n                if type is layerclass:\\n                    for e2 in layer.select(AbstractSpanAnnotation,set,True, (True, Word, Morpheme)):\\n                        if not isinstance(e2, AbstractSpanRole) and self in e2.wrefs():\\n                            yield e2\\n                else:\\n                    for e2 in layer.select(type,set,True, (True, Word, Morpheme)):\\n                        if not isinstance(e2, AbstractSpanRole) and self in e2.wrefs():\\n                            yield e2', 'search_document:         \\n        if self.doc and self.doc.deepvalidation and self.parent.set and self.parent.set[0] != \\'_\\':\\n            try:\\n                self.doc.set                if self.parent.cls and not self.doc.allowadhocsets:\\n                    raise DeepValidationError(\"Set                 errormsg =  str(e) + \" (in set \" + self.parent.set+\" for \" + self.parent.XMLTAG\\n                if self.parent.id:\\n                    errormsg += \" with ID \" + self.parent.id\\n                errormsg +=  \")\"\\n                raise DeepValidationError(errormsg)', 'search_document:         \\n        if not attribs: attribs = {}\\n        E = ElementMaker(namespace=\"http://ilk.uvt.nl/folia\",nsmap={None: \"http://ilk.uvt.nl/folia\", \\'xml\\' : \"http://www.w3.org/XML/1998/namespace\"})\\n        e = super(AbstractSpanAnnotation,self).xml(attribs, elements, True)\\n        for child in self:\\n            if isinstance(child, (Word, Morpheme, Phoneme)):\\n                #Include REFERENCES to word items instead of word items themselves\\n                attribs[\\'{\\' + NSFOLIA + \\'}id\\'] = child.id\\n                if child.PRINTABLE and child.hastext(self.textclass):\\n                    attribs[\\'{\\' + NSFOLIA + \\'}t\\'] = child.text(self.textclass)\\n                e.append( E.wref(**attribs) )\\n            elif not (isinstance(child, Feature) and child.SUBSET): #Don\\'t add pre-defined features, they are already added as attributes\\n                e.append( child.xml() )\\n        return e', \"search_document:         \\n        #Accept Word instances instead of WordReference, references will be automagically used upon serialisation\\n        if isinstance(child, (Word, Morpheme, Phoneme)) and WordReference in self.ACCEPTED_DATA:\\n            #We don't really append but do an insertion so all references are in proper order\\n            insertionpoint = len(self.data)\\n            for i, sibling in enumerate(self.data):\\n                if isinstance(sibling, (Word, Morpheme, Phoneme)):\\n                    try:\\n                        if not sibling.precedes(child):\\n                            insertionpoint = i\\n                    except: #happens if we can't determine common ancestors\\n                        pass\\n\\n            self.data.insert(insertionpoint, child)\\n            return child\\n        elif isinstance(child, AbstractSpanAnnotation): #(covers span roles just as well)\\n            insertionpoint = len(self.data)\\n            try:\\n                firstword = child.wrefs(0)\\n            except IndexError:\\n                #we have no basis to determine an insertionpoint for this child, just append it then\\n                return super(AbstractSpanAnnotation,self).append(child, *args, **kwargs)\\n\\n            insertionpoint = len(self.data)\\n            for i, sibling in enumerate(self.data):\\n                if isinstance(sibling, (Word, Morpheme, Phoneme)):\\n                    try:\\n                        if not sibling.precedes(firstword):\\n                            insertionpoint = i\\n                    except: #happens if we can't determine common ancestors\\n                        pass\\n            return super(AbstractSpanAnnotation,self).insert(insertionpoint, child, *args, **kwargs)\\n        else:\\n            return super(AbstractSpanAnnotation,self).append(child, *args, **kwargs)\", 'search_document:         \\n        self.data = []\\n        for child in args:\\n            self.append(child)', 'search_document:         \\n        return self.count(Class,set,True,default_ignore_annotations)', 'search_document:         \\n        l = list(self.select(type,set,True,            return l[0]\\n        else:\\n            raise NoSuchAnnotation()', 'search_document:         \\n        for c in self:\\n            if isinstance(c,Word) or isinstance(c,Morpheme) or isinstance(c, Phoneme):\\n                targets.append(c)\\n            elif isinstance(c,WordReference):\\n                try:\\n                    targets.append(self.doc[c.id]) #try to resolve\\n                except KeyError:\\n                    targets.append(c) #add unresolved\\n            elif isinstance(c, AbstractSpanAnnotation) and recurse:\\n                #recursion\\n                c._helper_wrefs(targets) #pylint: disable=protected-access\\n            elif isinstance(c, Correction) and c.auth: #recurse into corrections\\n                for e in c:\\n                    if isinstance(e, AbstractCorrectionChild) and e.auth:\\n                        for e2 in e:\\n                            if isinstance(e2, AbstractSpanAnnotation):\\n                                #recursion\\n                                e2._helper_wrefs(targets)', 'search_document:         \\n        targets =[]\\n        self._helper_wrefs(targets, recurse)\\n        if index is None:\\n            return targets\\n        else:\\n            return targets[index]', 'search_document:         \\n        if not norecurse: norecurse = (Word, Morpheme, Phoneme)\\n        if self.id:\\n            self.doc.index[self.id] = self\\n        for e in self.data:\\n            if all([not isinstance(e, C) for C in norecurse]):\\n                try:\\n                    e.addtoindex(norecurse)\\n                except AttributeError:\\n                    pass', 'search_document:         \\n        if idsuffix is True: idsuffix = \".copy.\" + \"%08x\" % random.getrandbits(32) #random 32-bit hash for each copy, same one will be reused for all children\\n        for c in self:\\n            if isinstance(c, Word):\\n                yield WordReference(newdoc, id=c.id)\\n            else:\\n                yield c.copy(newdoc,idsuffix)', 'search_document:         \\n        if self.set is False or self.set is None:\\n            if len(self.data) == 0: #just skip if there are no children\\n                return None\\n            else:\\n                raise ValueError(\"No set specified or derivable for annotation layer \" + self.__class__.__name__)\\n        return super(AbstractAnnotationLayer, self).xml(attribs, elements, skipchildren)', \"search_document:         \\n        #if no set is associated with the layer yet, we learn it from span annotation elements that are added\\n        if self.set is False or self.set is None:\\n            if inspect.isclass(child):\\n                if issubclass(child,AbstractSpanAnnotation):\\n                    if 'set' in kwargs:\\n                        self.set = kwargs['set']\\n            elif isinstance(child, AbstractSpanAnnotation):\\n                if child.set:\\n                    self.set = child.set\\n            elif isinstance(child, Correction):\\n                #descend into corrections to find the proper set for this layer (derived from span annotation elements)\\n                for e in itertools.chain( child.new(), child.original(), child.suggestions() ):\\n                    if isinstance(e, AbstractSpanAnnotation) and e.set:\\n                        self.set = e.set\\n                        break\\n\\n        return super(AbstractAnnotationLayer, self).append(child, *args, **kwargs)\"]\n",
            "[\"search_document:         \\n\\n        for e in self.select(AlternativeLayers,None, True, ['Original','Suggestion']): #pylint: disable=too-many-nested-blocks\\n            if Class is None:\\n                yield e\\n            elif len(e) >= 1: #child elements?\\n                for e2 in e:\\n                    try:\\n                        if isinstance(e2, Class):\\n                            try:\\n                                if set is None or e2.set == set:\\n                                    yield e #not e2\\n                                    break #yield an alternative only once (in case there are multiple matches)\\n                            except AttributeError:\\n                                continue\\n                    except AttributeError:\\n                        continue\", 'search_document:         \\n\\n        for span in self.select(AbstractSpanAnnotation,None,True):\\n            if tuple(span.wrefs()) == words:\\n                return span\\n        raise NoSuchAnnotation', 'search_document:         \\n        E = ElementMaker(namespace=\"http://relaxng.org/ns/structure/1.0\",nsmap={None:\\'http://relaxng.org/ns/structure/1.0\\' , \\'folia\\': \"http://ilk.uvt.nl/folia\", \\'xml\\' : \"http://www.w3.org/XML/1998/namespace\",\\'a\\':\"http://relaxng.org/ns/annotation/0.9\" })\\n        if not extraattribs:\\n            extraattribs = []\\n        extraattribs.append(E.optional(E.attribute(E.text(), name=\\'set\\')) )\\n        return AbstractElement.relaxng(includechildren, extraattribs, extraelements, cls)', 'search_document:         \\n        for e in  self.select(New,None,False, False):\\n            if not allowempty and len(e) == 0: continue\\n            return True\\n        return False', 'search_document:         \\n        for e in self.select(Original,None,False, False):\\n            if not allowempty and len(e) == 0: continue\\n            return True\\n        return False', 'search_document:         \\n        for e in self.select(Current,None,False, False):\\n            if not allowempty and len(e) == 0: continue\\n            return True\\n        return False', 'search_document:         \\n        for e in self.select(Suggestion,None,False, False):\\n            if not allowempty and len(e) == 0: continue\\n            return True\\n        return False', \"search_document:         \\n        if cls == 'original': correctionhandling = CorrectionHandling.ORIGINAL #backward compatibility\\n        if correctionhandling in (CorrectionHandling.CURRENT, CorrectionHandling.EITHER):\\n            for e in self:\\n                if isinstance(e, New) or isinstance(e, Current):\\n                    return e.textcontent(cls,correctionhandling)\\n        if correctionhandling in (CorrectionHandling.ORIGINAL, CorrectionHandling.EITHER):\\n            for e in self:\\n                if isinstance(e, Original):\\n                    return e.textcontent(cls,correctionhandling)\\n        raise NoSuchText\", \"search_document:         \\n        if cls == 'original': correctionhandling = CorrectionHandling.ORIGINAL #backward compatibility\\n        if correctionhandling in (CorrectionHandling.CURRENT, CorrectionHandling.EITHER):\\n            for e in self:\\n                if isinstance(e, New) or isinstance(e, Current):\\n                    return e.phoncontent(cls, correctionhandling)\\n        if correctionhandling in (CorrectionHandling.ORIGINAL, CorrectionHandling.EITHER):\\n            for e in self:\\n                if isinstance(e, Original):\\n                    return e.phoncontent(cls, correctionhandling)\\n        raise NoSuchPhon\", \"search_document:         \\n        if cls == 'original': correctionhandling = CorrectionHandling.ORIGINAL #backward compatibility\\n        if correctionhandling in (CorrectionHandling.CURRENT, CorrectionHandling.EITHER):\\n            for e in self:\\n                if isinstance(e, New) or isinstance(e, Current):\\n                    return e.hastext(cls,strict, correctionhandling)\\n        if correctionhandling in (CorrectionHandling.ORIGINAL, CorrectionHandling.EITHER):\\n            for e in self:\\n                if isinstance(e, Original):\\n                    return e.hastext(cls,strict, correctionhandling)\\n        return False\", 'search_document:         \\n        if cls == \\'original\\': correctionhandling = CorrectionHandling.ORIGINAL #backward compatibility\\n        if correctionhandling in (CorrectionHandling.CURRENT, CorrectionHandling.EITHER):\\n            for e in self:\\n                if isinstance(e, New) or isinstance(e, Current):\\n                    s = previousdelimiter + e.text(cls, retaintokenisation,\"\", strict, correctionhandling)\\n                    if normalize_spaces:\\n                        return norm_spaces(s)\\n                    else:\\n                        return s\\n        if correctionhandling in (CorrectionHandling.ORIGINAL, CorrectionHandling.EITHER):\\n            for e in self:\\n                if isinstance(e, Original):\\n                    s =  previousdelimiter + e.text(cls, retaintokenisation,\"\", strict, correctionhandling)\\n                    if normalize_spaces:\\n                        return norm_spaces(s)\\n                    else:\\n                        return s\\n        raise NoSuchText', 'search_document:         \\n        if cls == \\'original\\': correctionhandling = CorrectionHandling.ORIGINAL #backward compatibility\\n        if correctionhandling in (CorrectionHandling.CURRENT, CorrectionHandling.EITHER):\\n            for e in self:\\n                if isinstance(e, New) or isinstance(e, Current):\\n                    return previousdelimiter + e.phon(cls, \"\", strict, correctionhandling)\\n        if correctionhandling in (CorrectionHandling.ORIGINAL, CorrectionHandling.EITHER):\\n            for e in self:\\n                if isinstance(e, Original):\\n                    return previousdelimiter + e.phon(cls, \"\", correctionhandling)\\n        raise NoSuchPhon', 'search_document:         \\n        for e in self:\\n            if isinstance(e, New) or isinstance(e, Current):\\n                return e.gettextdelimiter(retaintokenisation)\\n        return \"\"', 'search_document:         \\n\\n        if index is None:\\n            try:\\n                return next(self.select(New,None,False))\\n            except StopIteration:\\n                raise NoSuchAnnotation\\n        else:\\n            for e in self.select(New,None,False):\\n                return e[index]\\n            raise NoSuchAnnotation', 'search_document:         \\n        if index is None:\\n            try:\\n                return next(self.select(Original,None,False, False))\\n            except StopIteration:\\n                raise NoSuchAnnotation\\n        else:\\n            for e in self.select(Original,None,False, False):\\n                return e[index]\\n            raise NoSuchAnnotation', 'search_document:         \\n        if index is None:\\n            try:\\n                return next(self.select(Current,None,False))\\n            except StopIteration:\\n                raise NoSuchAnnotation\\n        else:\\n            for e in self.select(Current,None,False):\\n                return e[index]\\n            raise NoSuchAnnotation', 'search_document:         \\n        if index is None:\\n            return self.select(Suggestion,None,False, False)\\n        else:\\n            for i, e in enumerate(self.select(Suggestion,None,False, False)):\\n                if index == i:\\n                    return e\\n            raise IndexError', 'search_document:         \\n        if self.include:\\n            return self.subdoc.data[0].select(Class,set,recursive, ignore, node) #pass it on to the text node of the subdoc\\n        else:\\n            return iter([])', 'search_document:         \\n        E = ElementMaker(namespace=NSFOLIA,nsmap={None: NSFOLIA, \\'xml\\' : \"http://www.w3.org/XML/1998/namespace\"})\\n\\n        if not attribs: attribs = {}\\n        if not elements: elements = []\\n\\n        if self.id:\\n            attribs[\\'id\\'] = self.id\\n            try:\\n                w = self.doc[self.id]\\n                attribs[\\'t\\'] = w.text()\\n            except KeyError:\\n                pass\\n\\n        e  = makeelement(E, \\'{\\' + NSFOLIA + \\'}\\' + self.XMLTAG, **attribs)\\n        return e', 'search_document:         \\n        l = self.count(type,set,True,            return l[0]\\n        else:\\n            raise NoSuchAnnotation()', 'search_document:         \\n        if issubclass(type, AbstractAnnotationLayer):\\n            layerclass = type\\n        else:\\n            layerclass = ANNOTATIONTYPE2LAYERCLASS[type.ANNOTATIONTYPE]\\n        e = self\\n        while True:\\n            if not e.parent: break\\n            e = e.parent\\n            for layer in e.select(layerclass,set,False):\\n                for e2 in layer:\\n                    if isinstance(e2, AbstractSpanAnnotation):\\n                        if self in e2.wrefs():\\n                            yield e2', 'search_document:         \\n        for w in originalwords:\\n            if not isinstance(w, Word):\\n                raise Exception(\"Original word is not a Word instance: \" + str(type(w)))\\n            elif w.sentence() != self:\\n                raise Exception(\"Original not found as member of sentence!\")\\n        for w in newwords:\\n            if not isinstance(w, Word):\\n                raise Exception(\"New word is not a Word instance: \" + str(type(w)))\\n        if \\'suggest\\' in kwargs and kwargs[\\'suggest\\']:\\n            del kwargs[\\'suggest\\']\\n            return self.correct(suggestion=newwords,current=originalwords, **kwargs)\\n        else:\\n            return self.correct(original=originalwords, new=newwords, **kwargs)', 'search_document:         \\n        if isstring(originalword):\\n            originalword = self.doc[u(originalword)]\\n        return self.correctwords([originalword], newwords, **kwargs)', 'search_document:         \\n        return self.correctwords(originalwords, [newword], **kwargs)', 'search_document:         \\n        if isstring(word):\\n            word = self.doc[u(word)]\\n        return self.correctwords([word], [], **kwargs)', 'search_document:         \\n        if nextword:\\n            if isstring(nextword):\\n                nextword = self.doc[u(nextword)]\\n            if not nextword in self or not isinstance(nextword, Word):\\n                raise Exception(\"Next word not found or not instance of Word!\")\\n            if isinstance(newword, list) or isinstance(newword, tuple):\\n                if not all([ isinstance(x, Word) for x in newword ]):\\n                    raise Exception(\"New word (iterable) constains non-Word instances!\")\\n            elif not isinstance(newword, Word):\\n                raise Exception(\"New word no instance of Word!\")\\n\\n            kwargs[\\'insertindex\\'] = self.getindex(nextword)\\n        else:\\n            kwargs[\\'insertindex\\'] = 0\\n        kwargs[\\'nooriginal\\'] = True\\n        if isinstance(newword, list) or isinstance(newword, tuple):\\n            return self.correctwords([], newword, **kwargs)\\n        else:\\n            return self.correctwords([], [newword], **kwargs)', 'search_document:         \\n        if not self.variablesize():\\n            raise Exception(\"Can only resize patterns with * wildcards\")\\n\\n        nrofwildcards = 0\\n        for x in self.sequence:\\n            if x == \\'*\\':\\n                nrofwildcards += 1\\n\\n        assert (len(distribution) == nrofwildcards)\\n\\n        wildcardnr = 0\\n        newsequence = []\\n        for x in self.sequence:\\n            if x == \\'*\\':\\n                newsequence += [True] * distribution[wildcardnr]\\n                wildcardnr += 1\\n            else:\\n                newsequence.append(x)\\n        d = { \\'matchannotation\\':self.matchannotation, \\'matchannotationset\\':self.matchannotationset, \\'casesensitive\\':self.casesensitive }\\n        yield Pattern(*newsequence, **d )', \"search_document:         \\n        #if LXE and self.mode != Mode.XPATH:\\n        #    #workaround for xml:id problem (disabled)\\n        #    #f = open(filename)\\n        #    #s = f.read().replace(' xml:id=', ' id=')\\n        #    #f.close()\\n        #    self.tree = ElementTree.parse(filename)\\n        #else:\\n        self.tree = xmltreefromfile(filename)\\n        self.parsexml(self.tree.getroot())\\n        if self.mode != Mode.XPATH:\\n            #XML Tree is now obsolete (only needed when partially loaded for xpath queries)\\n            self.tree = None\", 'search_document:         \\n        l = []\\n        for e in self.data:\\n            l += e.items()\\n        return l', \"search_document:         \\n        for result in self.tree.xpath(query,namespaces={'f': 'http://ilk.uvt.nl/folia','folia': 'http://ilk.uvt.nl/folia' }):\\n            yield self.parsexml(result)\", 'search_document:         \\n        if inspect.isclass(annotationtype): annotationtype = annotationtype.ANNOTATIONTYPE\\n        if annotationtype in self.set_alias and set in self.set_alias[annotationtype]:\\n            return self.set_alias[annotationtype][set]\\n        elif fallback:\\n            return set\\n        else:\\n            raise KeyError(\"No alias for set \" + set)', 'search_document:         \\n        if inspect.isclass(annotationtype): annotationtype = annotationtype.ANNOTATIONTYPE\\n        return self.alias_set[annotationtype][alias]']\n",
            "['search_document:         \\n        if not filename:\\n            filename = self.filename\\n        if not filename:\\n            raise Exception(\"No filename specified\")\\n        if filename[-4:].lower() == \\'.bz2\\':\\n            f = bz2.BZ2File(filename,\\'wb\\')\\n            f.write(self.xmlstring().encode(\\'utf-8\\'))\\n            f.close()\\n        elif filename[-3:].lower() == \\'.gz\\':\\n            f = gzip.GzipFile(filename,\\'wb\\') #pylint: disable=re            f = io.open(filename,\\'w\\',encoding=\\'utf-8\\')\\n            f.write(self.xmlstring())\\n            f.close()', \"search_document:         \\n        if text is Text:\\n            text = Text(self, id=self.id + '.text.' + str(len(self.data)+1) )\\n        elif text is Speech:\\n            text = Speech(self, id=self.id + '.speech.' + str(len(self.data)+1) ) #pylint: disable=re            assert isinstance(text, Text) or isinstance(text, Speech)\\n        self.data.append(text)\\n        return text\", 'search_document:         \\n        l = []\\n        E = ElementMaker(namespace=\"http://ilk.uvt.nl/folia\",nsmap={None: \"http://ilk.uvt.nl/folia\", \\'xml\\' : \"http://www.w3.org/XML/1998/namespace\"})\\n\\n        for annotationtype, set in self.annotations:\\n            label = None\\n            #Find the \\'label\\' for the declarations dynamically (aka: AnnotationType --> String)\\n            for key, value in vars(AnnotationType).items():\\n                if value == annotationtype:\\n                    label = key\\n                    break\\n            #gather attribs\\n\\n            if (annotationtype == AnnotationType.TEXT or annotationtype == AnnotationType.PHON) and set == \\'un                #this is the implicit TextContent declaration, no need to output it explicitly\\n                continue\\n\\n            attribs = {}\\n            if set and set != \\'un                attribs[\\'{\\' + NSFOLIA + \\'}set\\'] = set\\n\\n\\n            for key, value in self.annotation                if key == \\'annotatortype\\':\\n                    if value == AnnotatorType.MANUAL:\\n                        attribs[\\'{\\' + NSFOLIA + \\'}\\' + key] = \\'manual\\'\\n                    elif value == AnnotatorType.AUTO:\\n                        attribs[\\'{\\' + NSFOLIA + \\'}\\' + key] = \\'auto\\'\\n                elif key == \\'datetime\\':\\n                    attribs[\\'{\\' + NSFOLIA + \\'}\\' + key] = value.strftime(\"%Y-%m-%dT%H:%M:%S\") #proper iso-formatting\\n                elif value:\\n                    attribs[\\'{\\' + NSFOLIA + \\'}\\' + key] = value\\n            if label:\\n                l.append( makeelement(E,\\'{\\' + NSFOLIA + \\'}\\' + label.lower() + \\'-annotation\\', **attribs) )\\n            else:\\n                raise Exception(\"Invalid annotation type\")\\n        return l', 'search_document:         \\n        l = []\\n        for annotationtype, set in self.annotations:\\n            label = None\\n            #Find the \\'label\\' for the declarations dynamically (aka: AnnotationType --> String)\\n            for key, value in vars(AnnotationType).items():\\n                if value == annotationtype:\\n                    label = key\\n                    break\\n            #gather attribs\\n\\n            if (annotationtype == AnnotationType.TEXT or annotationtype == AnnotationType.PHON) and set == \\'un                #this is the implicit TextContent declaration, no need to output it explicitly\\n                continue\\n\\n            jsonnode = {\\'annotationtype\\': label.lower()}\\n            if set and set != \\'un                jsonnode[\\'set\\'] = set\\n\\n\\n            for key, value in self.annotation                if key == \\'annotatortype\\':\\n                    if value == AnnotatorType.MANUAL:\\n                        jsonnode[key] = \\'manual\\'\\n                    elif value == AnnotatorType.AUTO:\\n                        jsonnode[key] = \\'auto\\'\\n                elif key == \\'datetime\\':\\n                    jsonnode[key] = value.strftime(\"%Y-%m-%dT%H:%M:%S\") #proper iso-formatting\\n                elif value:\\n                    jsonnode[key] = value\\n            if label:\\n                l.append( jsonnode  )\\n            else:\\n                raise Exception(\"Invalid annotation type\")\\n        return l', 'search_document:         \\n\\n        self.pendingvalidation()\\n\\n        E = ElementMaker(namespace=\"http://ilk.uvt.nl/folia\",nsmap={\\'xml\\' : \"http://www.w3.org/XML/1998/namespace\", \\'xlink\\':\"http://www.w3.org/1999/xlink\"})\\n        attribs = {}\\n        attribs[\\'{http://www.w3.org/XML/1998/namespace}id\\'] = self.id\\n\\n        #if self.version:\\n        #    attribs[\\'version\\'] = self.version\\n        #else:\\n        attribs[\\'version\\'] = FOLIAVERSION\\n\\n        attribs[\\'generator\\'] = \\'pynlpl.formats.folia-v\\' + LIBVERSION\\n\\n        metadataattribs = {}\\n        metadataattribs[\\'{\\' + NSFOLIA + \\'}type\\'] = self.metadatatype\\n\\n        if isinstance(self.metadata, ExternalMetaData):\\n            metadataattribs[\\'{\\' + NSFOLIA + \\'}src\\'] = self.metadata.url\\n\\n        e = E.FoLiA(\\n            E.metadata(\\n                E.annotations(\\n                    *self.xmldeclarations()\\n                ),\\n                *self.xmlmetadata(),\\n                **metadataattribs\\n            )\\n            , **attribs)\\n        for text in self.data:\\n            e.append(text.xml())\\n        return e', \"search_document:         \\n        self.pendingvalidation()\\n\\n        jsondoc = {'id': self.id, 'children': [], 'declarations': self.jsondeclarations() }\\n        if self.version:\\n            jsondoc['version'] = self.version\\n        else:\\n            jsondoc['version'] = FOLIAVERSION\\n        jsondoc['generator'] = 'pynlpl.formats.folia-v' + LIBVERSION\\n\\n        for text in self.data:\\n            jsondoc['children'].append(text.json())\\n        return jsondoc\", 'search_document:         \\n        E = ElementMaker(namespace=\"http://ilk.uvt.nl/folia\",nsmap={None: \"http://ilk.uvt.nl/folia\", \\'xml\\' : \"http://www.w3.org/XML/1998/namespace\"})\\n        elements = []\\n        if self.metadatatype == \"native\":\\n            if isinstance(self.metadata, NativeMetaData):\\n                for key, value in self.metadata.items():\\n                    elements.append(E.meta(value,id=key) )\\n        else:\\n            if isinstance(self.metadata, ForeignData):\\n                #in-document\\n                m = self.metadata\\n                while m is not None:\\n                    elements.append(m.xml())\\n                    m = m.next\\n        for metadata_id, submetadata in self.submetadata.items():\\n            subelements = []\\n            attribs = {\\n                \"{http://www.w3.org/XML/1998/namespace}id\": metadata_id,\\n                \"type\": self.submetadatatype[metadata_id] }\\n            if isinstance(submetadata, NativeMetaData):\\n                for key, value in submetadata.items():\\n                    subelements.append(E.meta(value,id=key) )\\n            elif isinstance(submetadata, ExternalMetaData):\\n                attribs[\\'src\\'] = submetadata.url\\n            elif isinstance(submetadata, ForeignData):\\n                #in-document\\n                m = submetadata\\n                while m is not None:\\n                    subelements.append(m.xml())\\n                    m = m.next\\n            elements.append( E.submetadata(*subelements, **attribs))\\n        return elements', 'search_document:         \\n        if self.debug >= 1:\\n            print(\"[PyNLPl FoLiA DEBUG] Processing Annotation Declarations\",file=stderr)\\n        self.declareprocessed = True\\n        for subnode in node: #pylint: disable=too-many-nested-blocks\\n            if not isinstance(subnode.tag, str): continue\\n            if subnode.tag[:25] == \\'{\\' + NSFOLIA + \\'}\\' and subnode.tag[-11:] == \\'-annotation\\':\\n                prefix = subnode.tag[25:][:-11]\\n                type = None\\n                if prefix.upper() in vars(AnnotationType):\\n                    type = vars(AnnotationType)[prefix.upper()]\\n                else:\\n                    raise Exception(\"Unknown declaration: \" + subnode.tag)\\n\\n                if \\'set\\' in subnode.attrib and subnode.attrib[\\'set\\']:\\n                    set = subnode.attrib[\\'set\\']\\n                else:\\n                    set = \\'un                    if type == AnnotationType.TEXT:\\n                        #explicit Text declaration, remove the implicit declaration:\\n                        a = []\\n                        for t,s in self.annotations:\\n                            if not (t == AnnotationType.TEXT and s == \\'un                                a.append( (t,s) )\\n                        self.annotations = a\\n                    #raise ValueError(\"Double declaration of \" + subnode.tag + \", set \\'\" + set + \"\\' + is already declared\")    //doubles are okay says Ko\\n                else:\\n                    self.annotations.append( (type, set) )\\n\\n                #Load set                     if set[:7] == \"http://\" or set[:8] == \"https://\" or set[:6] == \"ftp://\":\\n                        try:\\n                            self.set                            print(\"WARNING: Set \" + set + \" could not be downloaded, ignoring!\",file=sys.stderr) #warning and ignore\\n\\n                #Set                     #handle duplicate. If ambiguous: remove                         if not (\\'annotator\\' in self.annotation                            self.annotation                            del self.annotation                        if not (\\'annotatortype\\' in self.annotation                            self.annotation                            del self.annotation                                                                    if subnode.attrib[\\'annotatortype\\'] == \\'auto\\':\\n                                                                                if isinstance(subnode.attrib[\\'datetime\\'], datetime):\\n                                                                                self.annotation                    if self.debug >= 1:\\n                        print(\"[PyNLPl FoLiA DEBUG] Loading external document: \" + subnode.attrib[\\'external\\'],file=stderr)\\n                    if not type in self.standoffdocs:\\n                        self.standoffdocs[type] = {}\\n                    self.standoffdocs[type][set] = {}\\n\\n                    #check if it is already loaded, if multiple references are made to the same doc we reuse the instance\\n                    standoffdoc = None\\n                    for t in self.standoffdocs:\\n                        for s in self.standoffdocs[t]:\\n                            for source in self.standoffdocs[t][s]:\\n                                if source == subnode.attrib[\\'external\\']:\\n                                    standoffdoc = self.standoffdocs[t][s]\\n                                    break\\n                            if standoffdoc: break\\n                        if standoffdoc: break\\n\\n                    if not standoffdoc:\\n                        if subnode.attrib[\\'external\\'][:7] == \\'http://\\' or subnode.attrib[\\'external\\'][:8] == \\'https://\\':\\n                            #document is remote, download (in memory)\\n                            try:\\n                                f = urlopen(subnode.attrib[\\'external\\'])\\n                            except:\\n                                raise DeepValidationError(\"Unable to download standoff document: \" + subnode.attrib[\\'external\\'])\\n                            try:\\n                                content = u(f.read())\\n                            except IOError:\\n                                raise DeepValidationError(\"Unable to download standoff document: \" + subnode.attrib[\\'external\\'])\\n                            f.close()\\n                            standoffdoc = Document(string=content, parentdoc=self, set                            #document is on disk:\\n                            standoffdoc = Document(file=subnode.attrib[\\'external\\'], parentdoc=self, set                            #document not found\\n                            raise DeepValidationError(\"Unable to find standoff document: \" + subnode.attrib[\\'external\\'])\\n\\n                    self.standoffdocs[type][set][subnode.attrib[\\'external\\']] = standoffdoc\\n                    standoffdoc.parentdoc = self\\n\\n                if self.debug >= 1:\\n                    print(\"[PyNLPl FoLiA DEBUG] Found declared annotation \" + subnode.tag + \". Defaults: \" + repr(defaults),file=stderr)', \"search_document:             self.metadata = ElementTree.tostring(node, xml_declaration=False, pretty_print=True, encoding='utf-8')\\n        else:\\n            self.metadata = ElementTree.tostring(node, encoding='utf-8')\\n        n = node.xpath('imdi:Session/imdi:Title', namespaces=ns)\\n        if n and n[0].text: self._title = n[0].text\\n        n = node.xpath('imdi:Session/imdi:Date', namespaces=ns)\\n        if n and n[0].text: self._date = n[0].text\\n        n = node.xpath('//imdi:Source/imdi:Access/imdi:Publisher', namespaces=ns)\\n        if n and n[0].text: self._publisher = n[0].text\\n        n = node.xpath('//imdi:Source/imdi:Access/imdi:Availability', namespaces=ns)\\n        if n and n[0].text: self._license = n[0].text\\n        n = node.xpath('//imdi:Languages/imdi:Language/imdi:ID', namespaces=ns)\\n        if n and n[0].text: self._language = n[0].text\", 'search_document:         \\n        if (sys.version > \\'3\\' and not isinstance(set,str)) or (sys.version < \\'3\\' and not isinstance(set,(str,unicode))):\\n            raise ValueError(\"Set parameter for declare() must be a string\")\\n\\n        if inspect.isclass(annotationtype):\\n            annotationtype = annotationtype.ANNOTATIONTYPE\\n        if annotationtype in self.alias_set and set in self.alias_set[annotationtype]:\\n            raise ValueError(\"Set \" + set + \" conflicts with alias, may not be equal!\")\\n        if not (annotationtype, set) in self.annotations:\\n            self.annotations.append( (annotationtype,set) )\\n            if set and self.loadset                if set[:7] == \"http://\" or set[:8] == \"https://\" or set[:6] == \"ftp://\":\\n                    self.set            self.annotation            if annotationtype in self.set_alias and set in self.set_alias[annotationtype] and self.set_alias[annotationtype][set] != kwargs[\\'alias\\']:\\n                raise ValueError(\"Redeclaring set \" + set + \" with another alias (\\'\"+kwargs[\\'alias\\']+\"\\') is not allowed!\")\\n            if annotationtype in self.alias_set and kwargs[\\'alias\\'] in self.alias_set[annotationtype] and self.alias_set[annotationtype][kwargs[\\'alias\\']] != set:\\n                raise ValueError(\"Redeclaring alias \" + kwargs[\\'alias\\'] + \" with another set (\\'\"+set+\"\\') is not allowed!\")\\n            if annotationtype in self.set_alias and kwargs[\\'alias\\'] in self.set_alias[annotationtype]:\\n                raise ValueError(\"Alias \" + kwargs[\\'alias\\'] + \" conflicts with set name, may not be equal!\")\\n            if annotationtype not in self.alias_set:\\n                self.alias_set[annotationtype] = {}\\n            if annotationtype not in self.set_alias:\\n                self.set_alias[annotationtype] = {}\\n            self.alias_set[annotationtype][kwargs[\\'alias\\']] = set\\n            self.set_alias[annotationtype][set] = kwargs[\\'alias\\']', 'search_document:         \\n        if inspect.isclass(annotationtype): annotationtype = annotationtype.ANNOTATIONTYPE\\n        return ( (annotationtype,set) in self.annotations) or (set in self.alias_set and self.alias_set[set] and (annotationtype, self.alias_set[set]) in self.annotations )', 'search_document:         \\n\\n        if inspect.isclass(annotationtype) or isinstance(annotationtype,AbstractElement): annotationtype = annotationtype.ANNOTATIONTYPE\\n        try:\\n            return list(self.annotation            raise NoDefaultError\\n        except IndexError:\\n            raise NoDefaultError', 'search_document:         \\n\\n        if inspect.isclass(annotationtype) or isinstance(annotationtype,AbstractElement): annotationtype = annotationtype.ANNOTATIONTYPE\\n        if not set: set = self.            return self.annotation            raise NoDefaultError', 'search_document:         \\n        if not (value is None):\\n            if (self.metadatatype == \"native\"):\\n                self.metadata[\\'title\\'] = value\\n            else:\\n                self._title = value\\n        if (self.metadatatype == \"native\"):\\n            if \\'title\\' in self.metadata:\\n                return self.metadata[\\'title\\']\\n            else:\\n                return None\\n        else:\\n            return self._title', 'search_document:         \\n        if not (value is None):\\n            if (self.metadatatype == \"native\"):\\n                self.metadata[\\'date\\'] = value\\n            else:\\n                self._date = value\\n        if (self.metadatatype == \"native\"):\\n            if \\'date\\' in self.metadata:\\n                return self.metadata[\\'date\\']\\n            else:\\n                return None\\n        else:\\n            return self._date', 'search_document:         \\n        if not (value is None):\\n            if (self.metadatatype == \"native\"):\\n                self.metadata[\\'publisher\\'] = value\\n            else:\\n                self._publisher = value\\n        if (self.metadatatype == \"native\"):\\n            if \\'publisher\\' in self.metadata:\\n                return self.metadata[\\'publisher\\']\\n            else:\\n                return None\\n        else:\\n            return self._publisher', 'search_document:         \\n        if not (value is None):\\n            if (self.metadatatype == \"native\"):\\n                self.metadata[\\'license\\'] = value\\n            else:\\n                self._license = value\\n        if (self.metadatatype == \"native\"):\\n            if \\'license\\' in self.metadata:\\n                return self.metadata[\\'license\\']\\n            else:\\n                return None\\n        else:\\n            return self._license', 'search_document:         \\n        if not (value is None):\\n            if (self.metadatatype == \"native\"):\\n                self.metadata[\\'language\\'] = value\\n            else:\\n                self._language = value\\n        if self.metadatatype == \"native\":\\n            if \\'language\\' in self.metadata:\\n                return self.metadata[\\'language\\']\\n            else:\\n                return None\\n        else:\\n            return self._language', 'search_document:         \\n\\n        if \\'type\\' in node.attrib:\\n            self.metadatatype = node.attrib[\\'type\\']\\n        else:\\n            #no type specified,             self.metadata = ExternalMetaData(node.attrib[\\'src\\'])\\n        elif self.metadatatype == \"native\":\\n            self.metadata = NativeMetaData()\\n        else:\\n            self.metadata = None #may be set below to ForeignData\\n\\n        for subnode in node:\\n            if subnode.tag == \\'{\\' + NSFOLIA + \\'}annotations\\':\\n                self.parsexmldeclarations(subnode)\\n            elif subnode.tag == \\'{\\' + NSFOLIA + \\'}meta\\':\\n                if self.metadatatype == \"native\":\\n                    if subnode.text:\\n                        self.metadata[subnode.attrib[\\'id\\']] = subnode.text\\n                else:\\n                    raise MetaDataError(\"Encountered a meta element but metadata type is not native!\")\\n            elif subnode.tag == \\'{\\' + NSFOLIA + \\'}provenance\\':\\n                #forward compatibility with FoLiA 2.0; ignore provenance\\n                print(\"WARNING: Ignoring provenance data. Use foliapy instead of pynlpl.formats.folia for FoLiA v2.0 compatibility!\",file=sys.stderr)\\n                pass\\n            elif subnode.tag == \\'{\\' + NSFOLIA + \\'}foreign-data\\':\\n                if self.metadatatype == \"native\":\\n                    raise MetaDataError(\"Encountered a foreign-data element but metadata type is native!\")\\n                elif self.metadata is not None:\\n                    #multiple foreign-data elements, chain:\\n                    e = self.metadata\\n                    while e.next is not None:\\n                        e = e.next\\n                    e.next = ForeignData(self, node=subnode)\\n                else:\\n                    self.metadata = ForeignData(self, node=subnode)\\n            elif subnode.tag == \\'{\\' + NSFOLIA + \\'}submetadata\\':\\n                self.parsesubmetadata(subnode)\\n            elif subnode.tag == \\'{http://www.mpi.nl/IMDI/Schema/IMDI}METATRANSCRIPT\\': #backward-compatibility for old IMDI without foreign-key\\n                E = ElementMaker(namespace=NSFOLIA,nsmap={None: NSFOLIA, \\'xml\\' : \"http://www.w3.org/XML/1998/namespace\"})\\n                self.metadatatype = \"imdi\"\\n                self.metadata = ForeignData(self, node=subnode)', 'search_document:         \\n        if (LXE and isinstance(node,ElementTree._ElementTree)) or (not LXE and isinstance(node, ElementTree.ElementTree)): #pylint: disable=protected-access\\n            node = node.getroot()\\n        elif isstring(node):\\n            node = xmltreefromstring(node).getroot()\\n\\n        if node.tag.startswith(\\'{\\' + NSFOLIA + \\'}\\'):\\n            foliatag = node.tag[nslen:]\\n            if foliatag == \"FoLiA\":\\n                if self.debug >= 1: print(\"[PyNLPl FoLiA DEBUG] Found FoLiA document\",file=stderr)\\n                try:\\n                    self.id = node.attrib[\\'{http://www.w3.org/XML/1998/namespace}id\\']\\n                except KeyError:\\n                    try:\\n                        self.id = node.attrib[\\'XMLid\\']\\n                    except KeyError:\\n                        try:\\n                            self.id = node.attrib[\\'id\\']\\n                        except KeyError:\\n                            raise Exception(\"FoLiA Document has no ID!\")\\n                if \\'version\\' in node.attrib:\\n                    self.version = node.attrib[\\'version\\']\\n                    if checkversion(self.version) > 0:\\n                        print(\"WARNING!!! Document uses a newer version of FoLiA than this library! (\" + self.version + \" vs \" + FOLIAVERSION + \"). Any possible subsequent failures in parsing or processing may probably be attributed to this. Upgrade to foliapy (https://github.com/proycon/foliapy) to remedy this.\",file=sys.stderr)\\n                else:\\n                    self.version = None\\n\\n                if \\'external\\' in node.attrib:\\n                    self.external = (node.attrib[\\'external\\'] == \\'yes\\')\\n\\n                    if self.external and not self.parentdoc:\\n                        raise DeepValidationError(\"Document is marked as external and should not be loaded independently. However, no parentdoc= has been specified!\")\\n\\n\\n                for subnode in node:\\n                    if subnode.tag == \\'{\\' + NSFOLIA + \\'}metadata\\':\\n                        self.parsemetadata(subnode)\\n                    elif (subnode.tag == \\'{\\' + NSFOLIA + \\'}text\\' or subnode.tag == \\'{\\' + NSFOLIA + \\'}speech\\') and self.mode == Mode.MEMORY:\\n                        if self.debug >= 1: print(\"[PyNLPl FoLiA DEBUG] Found Text\",file=stderr)\\n                        e = self.parsexml(subnode)\\n                        if e is not None:\\n                            self.data.append(e)\\n            else:\\n                #generic handling (FoLiA)\\n                if not foliatag in XML2CLASS:\\n                    raise Exception(\"Unknown FoLiA XML tag: \" + foliatag)\\n                Class = XML2CLASS[foliatag]\\n                return Class.parsexml(node,self)\\n        elif node.tag == \\'{\\' + NSDCOI + \\'}DCOI\\':\\n            if self.debug >= 1: print(\"[PyNLPl FoLiA DEBUG] Found DCOI document\",file=stderr)\\n            self.autodeclare = True\\n            try:\\n                self.id = node.attrib[\\'{http://www.w3.org/XML/1998/namespace}id\\']\\n            except KeyError:\\n                try:\\n                    self.id = node.attrib[\\'id\\']\\n                except KeyError:\\n                    try:\\n                        self.id = node.attrib[\\'XMLid\\']\\n                    except KeyError:\\n                        raise Exception(\"D-Coi Document has no ID!\")\\n            for subnode in node:\\n                if subnode.tag == \\'{http://www.mpi.nl/IMDI/Schema/IMDI}METATRANSCRIPT\\':\\n                    self.metadatatype = MetaDataType.IMDI\\n                    self.setimdi(subnode)\\n                elif subnode.tag == \\'{\\' + NSDCOI + \\'}text\\':\\n                    if self.debug >= 1: print(\"[PyNLPl FoLiA DEBUG] Found Text\",file=stderr)\\n                    e = self.parsexml(subnode)\\n                    if e is not None:\\n                        self.data.append( e )\\n        elif node.tag.startswith(\\'{\\' + NSDCOI + \\'}\\'):\\n            #generic handling (D-Coi)\\n            if node.tag[nslendcoi:] in XML2CLASS:\\n                Class = XML2CLASS[node.tag[nslendcoi:]]\\n                return Class.parsexml(node,self)\\n            elif node.tag[nslendcoi:][0:3] == \\'div\\': #support for div0, div1, etc:\\n                Class = Division\\n                return Class.parsexml(node,self)\\n            elif node.tag[nslendcoi:] == \\'item\\': #support for listitem\\n                Class = ListItem\\n                return Class.parsexml(node,self)\\n            elif node.tag[nslendcoi:] == \\'figDesc\\': #support for description in figures\\n                Class = Description\\n                return Class.parsexml(node,self)\\n            else:\\n                raise Exception(\"Unknown DCOI XML tag: \" + node.tag)\\n        else:\\n            raise Exception(\"Unknown FoLiA XML tag: \" + node.tag)\\n\\n        self.pendingvalidation()', 'search_document:         \\n        if self.debug: print(\"[PyNLPl FoLiA DEBUG] Processing pending validations (if any)\",file=stderr)\\n\\n        if warnonly is None and self and self.version:\\n            warnonly = (checkversion(self.version, \\'1.5.0\\') < 0) #warn only for documents older than FoLiA v1.5\\n        if self.textvalidation:\\n            while self.offsetvalidationbuffer:\\n                structureelement, textclass = self.offsetvalidationbuffer.pop()\\n\\n                if self.debug: print(\"[PyNLPl FoLiA DEBUG] Performing offset validation on \" + repr(structureelement) + \" textclass \" + textclass,file=stderr)\\n\\n                #validate offsets\\n                tc = structureelement.textcontent(textclass)\\n                if tc.offset is not None:\\n                    try:\\n                        tc.getreference(validate=True)\\n                    except UnresolvableTextContent:\\n                        msg = \"Text for \" + structureelement.__class__.__name__ + \", ID \" + str(structureelement.id) + \", textclass \" + textclass  + \", has incorrect offset \" + str(tc.offset) + \" or invalid reference\"\\n                        print(\"TEXT VALIDATION ERROR: \" + msg,file=sys.stderr)\\n                        if not warnonly:\\n                            raise', \"search_document:         \\n        if self.mode == Mode.MEMORY:\\n            for t in self.data:\\n                if Class.__name__ == 'Text':\\n                    yield t\\n                else:\\n                    for e in t.select(Class,set,recursive,ignore):\\n                        yield e\", 'search_document:         \\n        if self.mode == Mode.MEMORY:\\n            s = 0\\n            for t in self.data:\\n                s +=  sum( 1 for e in t.select(Class,recursive,True ) )\\n            return s', 'search_document:         \\n        if index is None:\\n            return self.select(Paragraph)\\n        else:\\n            if index < 0:\\n                index = sum(t.count(Paragraph) for t in self.data) + index\\n            for t in self.data:\\n                for i,e in enumerate(t.select(Paragraph)) :\\n                    if i == index:\\n                        return e\\n            raise IndexError', 'search_document:         \\n        if index is None:\\n            return self.select(Sentence,None,True,[Quote])\\n        else:\\n            if index < 0:\\n                index = sum(t.count(Sentence,None,True,[Quote]) for t in self.data) + index\\n            for t in self.data:\\n                for i,e in enumerate(t.select(Sentence,None,True,[Quote])) :\\n                    if i == index:\\n                        return e\\n            raise IndexError', 'search_document:         \\n\\n        #backward compatibility, old versions didn\\'t have cls as first argument, so if a boolean is passed first we interpret it as the 2nd:\\n        if cls is True or cls is False:\\n            retaintokenisation = cls\\n            cls = \\'current\\'\\n\\n        s = \"\"\\n        for c in self.data:\\n            if s: s += \"\\\\n\\\\n\\\\n\"\\n            try:\\n                s += c.text(cls, retaintokenisation)\\n            except NoSuchText:\\n                continue\\n        return s', 'search_document:             if not nextstate in processedstates:\\n                self._states(nextstate, processedstates)\\n\\n        for _, nextstate in state.transitions:\\n            if not nextstate in processedstates:\\n                self._states(nextstate, processedstates)\\n\\n        return processedstates', 'search_document:     \\n    if \\'debug\\' in kwargs:\\n        if \\'currentdebug\\' in kwargs:\\n            if kwargs[\\'currentdebug\\'] < kwargs[\\'debug\\']:\\n                return False\\n        else:\\n            return False #no currentdebug passed, assuming no debug mode and thus skipping message\\n\\n    s = \"[\" + datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\") + \"] \"\\n    if \\'system\\' in kwargs:\\n        s += \"[\" + system + \"] \"\\n\\n\\n    if \\'indent\\' in kwargs:\\n        s += (\"\\\\t\" * int(kwargs[\\'indent\\']))\\n\\n    s += u(msg)\\n\\n    if s[-1] != \\'\\\\n\\':\\n        s += \\'\\\\n\\'\\n\\n    if \\'streams\\' in kwargs:\\n        streams = kwargs[\\'streams\\']\\n    elif \\'stream\\' in kwargs:\\n        streams = [kwargs[\\'stream\\']]\\n    else:\\n        streams = [stderr]\\n\\n    for stream in streams:\\n        stream.write(s)\\n    return s', 'search_document:         \\n        finalsolution = None\\n        bestscore = None\\n        for solution in self:\\n            if bestscore == None:\\n                bestscore = solution.score()\\n                finalsolution = solution\\n            elif self.minimize:\\n                score = solution.score()\\n                if score < bestscore:\\n                    bestscore = score\\n                    finalsolution = solution\\n            elif not self.minimize:\\n                score = solution.score()\\n                if score > bestscore:\\n                    bestscore = score\\n                    finalsolution = solution                \\n        return finalsolution', 'search_document:                     \\n        solutions = PriorityQueue([], lambda x: x.score, self.minimize, length=n, blockworse=False, blockequal=False,duplicates=False)\\n        for solution in self:\\n            solutions.append(solution)\\n        return solutions', 'search_document:                     \\n        solutions = deque([], n)\\n        for solution in self:\\n            solutions.append(solution)\\n        return solutions', 'search_document:         \\n        if not isinstance(lemma,unicode):\\n            lemma = unicode(lemma,\\'utf-8\\')\\n\\n\\n        http, resp, content = self.connect()\\n\\n        params   = \"\"\\n        fragment = \"\"\\n\\n        path = \"cdb_syn\"\\n        if self.debug:\\n            printf( \"cornettodb/views/query_remote_syn_lemma: db_opt: %s\" % path )\\n\\n        query_opt = \"dict_search\"\\n        if self.debug:\\n            printf( \"cornettodb/views/query_remote_syn_lemma: query_opt: %s\" % query_opt )\\n    \\n        qdict = {}\\n        qdict[ \"action\" ] = \"queryList\"\\n        qdict[ \"word\" ]   = lemma.encode(\\'utf-8\\')\\n\\n\\n        query = urllib.urlencode( qdict )\\n\\n        db_url_tuple = ( self.scheme, self.host + \\':\\' + str(self.port), path, params, query, fragment )\\n        db_url = urlparse.urlunparse( db_url_tuple )\\n        if self.debug:\\n            printf( \"db_url: %s\" % db_url )\\n\\n        resp, content = http.request( db_url, \"GET\" )\\n        if self.debug:\\n            printf( \"resp:\\\\n%s\" % resp )\\n            printf( \"content:\\\\n%s\" % content )\\n        #    printf( \"content is of type: %s\" % type( content ) )\\n\\n        dict_list = []\\n        dict_list = eval( content )        # string to list\\n\\n        synsets = []\\n        items = len( dict_list )\\n        if self.debug:\\n            printf( \"items: %d\" % items )\\n\\n        # syn dict: like lu dict, but without pos: part-of-speech\\n        for dict in dict_list:\\n            if self.debug:\\n                printf( dict )\\n\\n            seq_nr = dict[ \"seq_nr\" ]   # sense number\\n            value  = dict[ \"value\" ]    # lexical unit identifier\\n            form   = dict[ \"form\" ]     # lemma\\n            label  = dict[ \"label\" ]    # label to be shown\\n\\n            if self.debug:\\n                printf( \"seq_nr: %s\" % seq_nr )\\n                printf( \"value:  %s\" % value )\\n                printf( \"form:   %s\" % form )\\n                printf( \"label:  %s\" % label )\\n\\n            if value != \"\":\\n                synsets.append( value )\\n\\n        return synsets']\n",
            "['search_document:         \\n\\n        http, resp, content = self.connect()\\n\\n        params   = \"\"\\n        fragment = \"\"\\n\\n        path = \"cdb_syn\"\\n        if self.debug:\\n            printf( \"cornettodb/views/query_remote_syn_id: db_opt: %s\" % path )\\n\\n        # output_opt: plain, html, xml\\n        # \\'xml\\' is actually xhtml (with markup), but it is not valid xml!\\n        # \\'plain\\' is actually valid xml (without markup)\\n        output_opt = \"plain\"\\n        if self.debug:\\n            printf( \"cornettodb/views/query_remote_syn_id: output_opt: %s\" % output_opt )\\n\\n        action = \"runQuery\"\\n        if self.debug:\\n            printf( \"cornettodb/views/query_remote_syn_id: action: %s\" % action )\\n            printf( \"cornettodb/views/query_remote_syn_id: query: %s\" % syn_id )\\n\\n        qdict = {}\\n        qdict[ \"action\" ]  = action\\n        qdict[ \"query\" ]   = syn_id\\n        qdict[ \"outtype\" ] = output_opt\\n\\n        query = urllib.urlencode( qdict )\\n\\n        db_url_tuple = ( self.scheme, self.host + \\':\\' + str(self.port), path, params, query, fragment )\\n        db_url = urlparse.urlunparse( db_url_tuple )\\n        if self.debug:\\n            printf( \"db_url: %s\" % db_url )\\n\\n        resp, content = http.request( db_url, \"GET\" )\\n        if self.debug:\\n            printf( \"resp:\\\\n%s\" % resp )\\n        #    printf( \"content:\\\\n%s\" % content )\\n        #    printf( \"content is of type: %s\" % type( content ) )        #<type \\'str\\'>\\n\\n        xml_data = eval( content )\\n        return etree.fromstring( xml_data )', 'search_document:         \\n\\n        root = self.get_synset_xml(syn_id)\\n        elem_synonyms = root.find( \".//synonyms\" )\\n\\n\\n        lus = []\\n        for elem_synonym in elem_synonyms:\\n            synonym_str = elem_synonym.get( \"c_lu_id-previewtext\" )        # get \"c_lu_id-previewtext\" attribute\\n            # synonym_str ends with \":<num>\"\\n            synonym = synonym_str.split( \\':\\' )[ 0 ].strip()\\n            lus.append( (synonym, elem_synonym.get( \"c_lu_id\") ) )\\n        return lus', 'search_document:         \\n        if not lemma:\\n            return self.get_lus_from_synset(syn_id) #alias\\n        if not isinstance(lemma,unicode):\\n            lemma = unicode(lemma,\\'utf-8\\')\\n\\n        root = self.get_synset_xml(syn_id)\\n        elem_synonyms = root.find( \".//synonyms\" )\\n\\n        lu_id = None\\n        synonyms = []\\n        for elem_synonym in elem_synonyms:\\n            synonym_str = elem_synonym.get( \"c_lu_id-previewtext\" )        # get \"c_lu_id-previewtext\" attribute\\n            # synonym_str ends with \":<num>\"\\n            synonym = synonym_str.split( \\':\\' )[ 0 ].strip()\\n\\n            if synonym != lemma:\\n                synonyms.append( (synonym, elem_synonym.get(\"c_lu_id\")) )\\n                if self.debug:\\n                    printf( \"synonym add: %s\" % synonym )\\n            else:\\n                lu_id = elem_synonym.get( \"c_lu_id\" )        # get \"c_lu_id\" attribute\\n                if self.debug:\\n                    printf( \"lu_id: %s\" % lu_id )\\n                    printf( \"synonym skip lemma: %s\" % synonym )\\n        return lu_id, synonyms', 'search_document:         \\n        l = []\\n        for word_id, senses,distance in self:\\n            for sense, confidence in senses:\\n                if not sense in l: l.append(sense)\\n                if bestonly:\\n                    break\\n        return l', 'search_document:         \\n        if isinstance(input_data, list) or isinstance(input_data, tuple):\\n            input_data = \" \".join(input_data)\\n\\n\\n\\n        input_data = u(input_data, source_encoding) #decode (or preferably do this in an earlier stage)\\n        input_data = input_data.strip(\\' \\\\t\\\\n\\')\\n\\n        s = input_data.encode(self.server_encoding) +b\\'\\\\r\\\\n\\'\\n        if not oldfrog: s += b\\'EOT\\\\r\\\\n\\'\\n        self.socket.sendall(s) #send to socket in desired encoding\\n        output = []\\n\\n        done = False\\n        while not done:\\n            data = b\"\"\\n            while not data.endswith(b\\'\\\\n\\'):\\n                moredata = self.socket.recv(self.BUFSIZE)\\n                if not moredata: break\\n                data += moredata\\n\\n\\n            data = u(data,self.server_encoding)\\n\\n\\n            for line in data.strip(\\' \\\\t\\\\r\\\\n\\').split(\\'\\\\n\\'):\\n                if line == \"READY\":\\n                    done = True\\n                    break\\n                elif line:\\n                    line = line.split(\\'\\\\t\\') #split on tab\\n                    if len(line) > 4 and line[0].isdigit(): #first column is token number\\n                        if line[0] == \\'1\\' and output:\\n                            if self.returnall:\\n                                output.append( (None,None,None,None, None,None,None, None) )\\n                            else:\\n                                output.append( (None,None,None,None) )\\n                        fields = line[1:]\\n                        parse1=parse2=ner=chunk=\"\"\\n                        word,lemma,morph,pos = fields[0:4]\\n                        if len(fields) > 5:\\n                            ner = fields[5]\\n                        if len(fields) > 6:\\n                            chunk = fields[6]\\n                        if len(fields) >= 8:\\n                            parse1 = fields[7]\\n                            parse2 = fields[8]\\n\\n                        if len(fields) < 5:\\n                            raise Exception(\"Can\\'t process response line from Frog: \", repr(line), \" got unexpected number of fields \", str(len(fields) + 1))\\n\\n                        if self.returnall:\\n                            output.append( (word,lemma,morph,pos,ner,chunk,parse1,parse2) )\\n                        else:\\n                            output.append( (word,lemma,morph,pos) )\\n\\n        return output', 'search_document:         \\n        alignment = []\\n        cursor = 0\\n        for inputword in inputwords:\\n            if len(outputwords) > cursor and outputwords[cursor] == inputword:\\n                alignment.append(cursor)\\n                cursor += 1\\n            elif len(outputwords) > cursor+1 and outputwords[cursor+1] == inputword:\\n                alignment.append(cursor+1)\\n                cursor += 2\\n            else:\\n                alignment.append(None)\\n                cursor += 1\\n        return alignment', 'search_document:     \\n    needle = tuple(needle)\\n    haystack = tuple(haystack)\\n    solutions = []\\n\\n    #equality check\\n    if needle == haystack:\\n        return [(needle, 2)]\\n\\n    if allowpartial:\\n        minl =1\\n    else:\\n        minl = len(needle)\\n\\n    for l in range(minl,min(len(needle), len(haystack))+1):\\n        #print \"LEFT-DEBUG\", l,\":\", needle[-l:], \" vs \", haystack[:l]\\n        #print \"RIGHT-DEBUG\", l,\":\", needle[:l], \" vs \", haystack[-l:]\\n        #Search for overlap left (including partial overlap!)\\n        if needle[-l:] == haystack[:l]:\\n            #print \"LEFT MATCH\"\\n            solutions.append( (needle[-l:], -1) )\\n        #Search for overlap right (including partial overlap!)\\n        if needle[:l] == haystack[-l:]:\\n            #print \"RIGHT MATCH\"\\n            solutions.append( (needle[:l], 1) )\\n\\n    if len(needle) <= len(haystack):\\n        options = list(iter(Windower(haystack,len(needle),beginmarker=None,endmarker=None)))\\n        for option in options[1:-1]:\\n            if option == needle:\\n                #print \"SUBSET MATCH\"\\n                solutions.append( (needle, 0) )\\n\\n    return solutions', 'search_document:     \\n\\n    for i,regexp in list(enumerate(regexps)):\\n        if isstring(regexp):\\n            regexps[i] = re.compile(regexp)\\n\\n    tokens = []\\n    begin = 0\\n    for i, c in enumerate(text):\\n        if begin > i:\\n            continue\\n        elif i == begin:\\n            m = False\\n            for regexp in regexps:\\n                m = regexp.findall(text[i:i+300])\\n                if m:\\n                    tokens.append(m[0])\\n                    begin = i + len(m[0])\\n                    break\\n            if m: continue\\n\\n        if c in string.punctuation or c in WHITESPACE:\\n            prev = text[i-1] if i > 0 else \"\"\\n            next = text[i+1] if i < len(text)-1 else \"\"\\n\\n            if (c == \\'.\\' or c == \\',\\') and prev.isdigit() and next.isdigit():\\n                #punctuation in between numbers, keep as one token\\n                pass\\n            elif (c == \"\\'\" or c == \"`\") and prev.isalpha() and next.isalpha():\\n                #quote in between chars, keep...\\n                pass\\n            elif c not in WHITESPACE and next == c: #group clusters of identical punctuation together\\n                continue\\n            elif c == \\'\\\\r\\' and prev == \\'\\\\n\\':\\n                #ignore\\n                begin = i+1\\n                continue\\n            else:\\n                token = text[begin:i]\\n                if token: tokens.append(token)\\n\\n                if c not in WHITESPACE:\\n                    tokens.append(c) #anything but spaces and newlines (i.e. punctuation) counts as a token too\\n                begin = i + 1 #set the begin cursor\\n\\n    if begin <= len(text) - 1:\\n        token = text[begin:]\\n        tokens.append(token)\\n\\n    return tokens', 'search_document:     \\n    begin = 0\\n    for i, token in enumerate(tokens):\\n        if is_end_of_sentence(tokens, i):\\n            yield tokens[begin:i+1]\\n            begin = i+1\\n    if begin <= len(tokens)-1:\\n        yield tokens[begin:]', \"search_document:     \\n    if sys.version < '3':\\n        if isinstance(s,unicode):\\n           return unicodedata.normalize('NFKD', s).encode('ASCII', 'ignore')\\n        else:\\n           return unicodedata.normalize('NFKD', unicode(s,encoding)).encode('ASCII', 'ignore')\\n    else:\\n        if isinstance(s,bytes): s = str(s,encoding)\\n        return str(unicodedata.normalize('NFKD', s).encode('ASCII', 'ignore'),'ascii')\", 'search_document:     \\n    assert maxdist >= 2\\n    tokens = list(tokens)\\n    if maxdist > len(tokens):\\n        maxdist = len(tokens)\\n    l = len(tokens)\\n    for i in range(0,l - 1):\\n        for permutation in permutations(tokens[i:i+maxdist]):\\n            if permutation != tuple(tokens[i:i+maxdist]):\\n                newtokens = tokens[:i]\\n                newtokens += permutation\\n                newtokens += tokens[i+maxdist:]\\n                yield newtokens\\n        if maxdist == len(tokens):\\n            break', 'search_document:     \\n    if isinstance(keyword,tuple) and isinstance(keyword,list):\\n        l = len(keyword)\\n    else:\\n        keyword = (keyword,)\\n        l = 1\\n    n = l + contextsize*2\\n    focuspos = contextsize + 1\\n    for ngram in Windower(tokens,n,None,None):\\n        if ngram[focuspos:focuspos+l] == keyword:\\n            yield ngram[:focuspos], ngram[focuspos:focuspos+l],ngram[focuspos+l+1:]', 'search_document:         \\n        e = self.data[self.start]\\n        self.start += 1\\n        if self.start > 5 and self.start > len(self.data)//2:\\n            self.data = self.data[self.start:]\\n            self.start = 0\\n        return e', \"search_document:         \\n        f = self.f(item)\\n        if callable(f):\\n            score = f()\\n        else:\\n            score = f\\n\\n        if not self.duplicates:\\n            for s, i in self.data:\\n                if s == score and item == i:\\n                    #item is a duplicate, don't add it\\n                    return False\\n\\n        if self.length and len(self.data) == self.length:\\n                #Fixed-length priority queue, abort when queue is full and new item scores worst than worst scoring item.\\n                if self.minimize:\\n                    worstscore = self.data[-1][0]\\n                    if score >= worstscore:\\n                        return False\\n                else:\\n                    worstscore = self.data[0][0]\\n                    if score <= worstscore:\\n                        return False\\n\\n        if self.blockworse and self.bestscore != None:\\n            if self.minimize:\\n                if score > self.bestscore:\\n                    return False\\n            else:\\n                if score < self.bestscore:\\n                    return False\\n        if self.blockequal and self.bestscore != None:\\n            if self.bestscore == score:\\n                return False\\n        if (self.bestscore == None) or (self.minimize and score < self.bestscore) or (not self.minimize and score > self.bestscore):\\n            self.bestscore = score\\n        bisect.insort(self.data, (score, item))\\n        if self.length:\\n            #fixed length queue: queue is now too long, delete worst items\\n            while len(self.data) > self.length:\\n                if self.minimize:\\n                    del self.data[-1]\\n                else:\\n                    del self.data[0]\\n        return True\", 'search_document:         \\n        if self.minimize:\\n            return self.data.pop(0)[1]\\n        else:\\n            return self.data.pop()[1]', 'search_document:         \\n        if self.minimize:\\n            return self.data[i][0]\\n        else:\\n            return self.data[(-1 * i) - 1][0]', 'search_document:         \\n        if self.minimize:\\n            self.data = self.data[:n]\\n        else:\\n            self.data = self.data[-1 * n:]', 'search_document:         \\n        self.data = random.sample(self.data, n)', 'search_document:         \\n        if retainequalscore:\\n            if self.minimize:\\n                f = lambda x: x[0] <= score\\n            else:\\n                f = lambda x: x[0] >= score\\n        else:\\n            if self.minimize:\\n                f = lambda x: x[0] < score\\n            else:\\n                f = lambda x: x[0] > score\\n        self.data = filter(f, self.data)', 'search_document:         \\n        if not isinstance(item, Tree):\\n            return ValueError(\"Can only append items of type Tree\")\\n        if not self.children: self.children = []\\n        item.parent = self\\n        self.children.append(item)', 'search_document:         \\n        if self.children:\\n            return sum( ( c.size() for c in self.children.values() ) ) + 1\\n        else:\\n            return 1', 'search_document:         \\n        if self.children:\\n            if not maxdepth or (maxdepth and _depth < maxdepth):\\n                for key, child in self.children.items():\\n                    if child.leaf():\\n                        yield child\\n                    else:\\n                        for results in child.walk(leavesonly, maxdepth, _depth + 1):\\n                            yield results', 'search_document:         \\n        prevp = 0\\n        prevs = 0\\n        sentence = [];\\n        sentence_id = \"\"\\n        for word, id, pos, lemma in iter(self):\\n            try:\\n                doc_id, ptype, p, s, w = re.findall(\\'([\\\\w\\\\d-]+)\\\\.(p|head)\\\\.(\\\\d+)\\\\.s\\\\.(\\\\d+)\\\\.w\\\\.(\\\\d+)\\',id)[0]\\n                if ((p != prevp) or (s != prevs)) and sentence:\\n                    yield sentence_id, sentence\\n                    sentence = []\\n                    sentence_id = doc_id + \\'.\\' + ptype + \\'.\\' + str(p) + \\'.s.\\' + str(s)\\n                prevp = p\\n            except IndexError:\\n                doc_id, s, w = re.findall(\\'([\\\\w\\\\d-]+)\\\\.s\\\\.(\\\\d+)\\\\.w\\\\.(\\\\d+)\\',id)[0]\\n                if s != prevs and sentence:\\n                    yield sentence_id, sentence\\n                    sentence = []\\n                    sentence_id = doc_id + \\'.s.\\' + str(s)\\n            sentence.append( (word,id,pos,lemma) )\\n            prevs = s\\n        if sentence:\\n            yield sentence_id, sentence', 'search_document:         \\n        prevp = 0\\n        partext = []\\n        for word, id, pos, lemma in iter(self):\\n            doc_id, ptype, p, s, w = re.findall(\\'([\\\\w\\\\d-]+)\\\\.(p|head)\\\\.(\\\\d+)\\\\.s\\\\.(\\\\d+)\\\\.w\\\\.(\\\\d+)\\',id)[0]\\n            if prevp != p and partext:\\n                    yield ( doc_id + \".\" + ptype + \".\" + prevp , \" \".join(partext) )\\n                    partext = []\\n            partext.append(word)\\n            prevp = p\\n        if partext:\\n            yield (doc_id + \".\" + ptype + \".\" + prevp, \" \".join(partext) )', 'search_document:         \\n        #TODO: download XSD from web\\n        if self.inline:\\n            xmlschema = ElementTree.XMLSchema(ElementTree.parse(StringIO(\"\\\\n\".join(open(formats_dir+\"dcoi-dsc.xsd\").readlines()))))\\n            xmlschema.assertValid(self.tree)\\n            #return xmlschema.validate(self)\\n        else:\\n            xmlschema = ElementTree.XMLSchema(ElementTree.parse(StringIO(\"\\\\n\".join(open(formats_dir+\"dutchsemcor-standalone.xsd\").readlines()))))\\n            xmlschema.assertValid(self.tree)', 'search_document:         \\n        global namespaces\\n        return self.tree.xpath(expression, namespaces=namespaces)', 'search_document:         \\n        targetwords = []\\n        for i, (word,lemma,postag) in enumerate(zip(datatuple[0],datatuple[1],datatuple[2])):\\n            if word:\\n                subwords = word.split(\"_\")\\n                for w in subwords: #split multiword expressions\\n                    targetwords.append( (w, lemma, postag, i, len(subwords) > 1 ) ) #word, lemma, pos, index, multiword? \\n\\n        referencewords = [ w.lower() for w in referencewords ]          \\n        alignment = []\\n        for i, referenceword in enumerate(referencewords):\\n            found = False\\n            best = 0  \\n            distance = 999999          \\n            for j, (targetword, lemma, pos, index, multiword) in enumerate(targetwords):\\n                if referenceword == targetword and abs(i-j) < distance:\\n                    found = True\\n                    best = j\\n                    distance = abs(i-j)\\n\\n            if found:\\n                alignment.append(targetwords[best])\\n            else:                \\n                alignment.append((None,None,None,None,False)) #no alignment found        \\n        \\n        return alignment', 'search_document:         \\n        if self.mainsetcache:\\n            return self.mainsetcache\\n        set_uri = self.get_set_uri()\\n        for row in self.graph.query(\"SELECT ?seturi ?setid ?setlabel ?setopen ?setempty WHERE { ?seturi rdf:type skos:Collection . OPTIONAL { ?seturi skos:notation ?setid } OPTIONAL { ?seturi skos:prefLabel ?setlabel } OPTIONAL { ?seturi fsd:open ?setopen } OPTIONAL { ?seturi fsd:empty ?setempty } FILTER NOT EXISTS { ?y skos:member ?seturi . ?y rdf:type skos:Collection } }\"):\\n            self.mainsetcache = {\\'uri\\': str(row.seturi), \\'id\\': str(row.setid), \\'label\\': str(row.setlabel) if row.setlabel else \"\", \\'open\\': bool(row.setopen), \\'empty\\': bool(row.setempty) }\\n            return self.mainsetcache\\n        raise DeepValidationError(\"Unable to find main set (set_uri=\" + str(set_uri)+\"), this should not happen\")', 'search_document:         \\n        if subset_id in self.subsetcache:\\n            return self.subsetcache[subset_id]\\n        set_uri = self.get_set_uri(subset_id)\\n        for row in self.graph.query(\"SELECT ?seturi ?setid ?setlabel ?setopen WHERE { ?seturi rdf:type skos:Collection . OPTIONAL { ?seturi skos:notation ?setid } OPTIONAL { ?seturi skos:prefLabel ?setlabel } OPTIONAL { ?seturi fsd:open ?setopen } FILTER (?seturi = <\" + str(set_uri)+\">) }\"):\\n            self.subsetcache[str(row.setid)] = {\\'uri\\': str(row.seturi), \\'id\\': str(row.setid), \\'label\\': str(row.setlabel) if row.setlabel else \"\", \\'open\\': bool(row.setopen) }\\n            return self.subsetcache[str(row.setid)]\\n        raise DeepValidationError(\"Unable to find subset (set_uri=\" + str(set_uri)+\")\")', 'search_document:         \\n        classes = self.classes(set_uri_or_id, nestedhierarchy)\\n        for classid in self.classorder(classes):\\n            yield classes[classid]', 'search_document:         \\n        if set_uri_or_id and set_uri_or_id.startswith((\\'http://\\',\\'https://\\')):\\n            set_uri = set_uri_or_id\\n        else:\\n            set_uri = self.get_set_uri(set_uri_or_id)\\n\\n        assert set_uri is not None\\n\\n        classes= {}\\n        uri2idmap = {}\\n        for row in self.graph.query(\"SELECT ?classuri ?classid ?classlabel ?parentclass ?seqnr  WHERE { ?classuri rdf:type skos:Concept ; skos:notation ?classid. <\" + str(set_uri) + \"> skos:member ?classuri . OPTIONAL { ?classuri skos:prefLabel ?classlabel } OPTIONAL { ?classuri skos:narrower ?parentclass } OPTIONAL { ?classuri fsd:sequenceNumber ?seqnr } }\"):\\n            classinfo = {\\'uri\\': str(row.classuri), \\'id\\': str(row.classid),\\'label\\': str(row.classlabel) if row.classlabel else \"\" }\\n            if nestedhierarchy:\\n                uri2idmap[str(row.classuri)] = str(row.classid)\\n            if row.parentclass:\\n                classinfo[\\'parentclass\\'] =  str(row.parentclass) #uri\\n            if row.seqnr:\\n                classinfo[\\'seqnr\\'] =  int(row.seqnr)\\n            classes[str(row.classid)] = classinfo\\n\\n        if nestedhierarchy:\\n            #build hierarchy\\n            removekeys = []\\n            for classid, classinfo in classes.items():\\n                if \\'parentclass\\' in classinfo:\\n                    removekeys.append(classid)\\n                    parentclassid = uri2idmap[classinfo[\\'parentclass\\']]\\n                    if \\'subclasses\\' not in classes[parentclassid]:\\n                        classes[parentclassid][\\'subclasses\\'] = {}\\n                    classes[parentclassid][\\'subclasses\\'][classid] = classinfo\\n            for key in removekeys:\\n                del classes[key]\\n        return classes', \"search_document:         \\n        return [ classid for classid, classitem in sorted( ((classid, classitem) for classid, classitem in classes.items() if 'seqnr' in classitem) , key=lambda pair: pair[1]['seqnr'] )] + \\\\\\n               [ classid for classid, classitem in sorted( ((classid, classitem) for classid, classitem in classes.items() if 'seqnr' not in classitem) , key=lambda pair: pair[1]['label'] if 'label' in pair[1] else pair[1]['id']) ]\"]\n",
            "['search_document:         \\n        self.lexer = ply.lex.lex(object=self, **kwargs)', 'search_document:     \\n    if tree.tag == \\'identifier\\':\\n        return tree.attrib[\\'name\\']\\n\\n    if tree.tag in (\\'string\\', \\'boolean\\'):\\n        return tree.text\\n\\n    if tree.tag == \\'number\\':\\n        return tree.attrib[\\'value\\']\\n\\n    if tree.tag in (\\'property\\', \\'object\\'):\\n        return make_varname(_xpath_one(tree, \\'*\\'))\\n\\n    if tree.tag.endswith(\\'accessor\\'):\\n        kind = tree.tag[:-len(\\'accessor\\')]\\n        obj = make_varname(_xpath_one(tree, \\'object\\'))\\n        prop = make_varname(_xpath_one(tree, \\'property\\'))\\n        if kind == \\'dot\\':\\n            fmt = \\'%s.%s\\'\\n        elif kind == \\'bracket\\':\\n            fmt = \\'%s[%s]\\'\\n        else:\\n            raise ValueError(\"Unknown accessor: %s\" % tree.tag)\\n        return fmt % (obj, prop)\\n\\n    raise ValueError(\"Unknown tag: %s\" % tree.tag)', \"search_document:     \\n    auth_string = os.environ.get(env_prefix + 'WSGI_AUTH_CREDENTIALS')\\n    if not auth_string:\\n        return {}\\n\\n    result = {}\\n    for credentials in auth_string.split('|'):\\n        username, password = credentials.split(':', 1)\\n        result[username] = password\\n    return result\", \"search_document:     \\n    paths = os.environ.get(env_prefix + 'WSGI_AUTH_EXCLUDE_PATHS')\\n    if not paths:\\n        return []\\n    return paths.split(';')\", \"search_document:     \\n    paths = os.environ.get(env_prefix + 'WSGI_AUTH_PATHS')\\n    if not paths:\\n        return []\\n    return paths.split(';')\", \"search_document:         \\n        if self._is_request_in_include_path(request):\\n            if self._is_request_in_exclude_path(request):\\n                return True\\n            else:\\n                auth = request.authorization\\n                if auth and auth[0] == 'Basic':\\n                    credentials = b64decode(auth[1]).decode('UTF-8')\\n                    username, password = credentials.split(':', 1)\\n                    return self._users.get(username) == password\\n                else:\\n                    return False\\n        else:\\n            return True\", \"search_document:         \\n        response = HTTPUnauthorized()\\n        response.www_authenticate = ('Basic', {'realm': self._realm})\\n        return response(environ, start_response)\", 'search_document:         \\n        if self._include_paths:\\n            for path in self._include_paths:\\n                if request.path.startswith(path):\\n                    return True\\n            return False\\n        else:\\n            return True', 'search_document:         \\n        if self._exclude_paths:\\n            for path in self._exclude_paths:\\n                if request.path.startswith(path):\\n                    return True\\n            return False\\n        else:\\n            return False', 'search_document:     \\n    prompt_kwargs = prompt_kwargs or {}\\n\\n                        prompt_kwargs[key] = default_value\\n\\n    return prompt_kwargs', 'search_document:     \\n    # parent should be available, but we\\'re not going to bother if not\\n    group_ctx = old_ctx.parent or old_ctx\\n    group = group_ctx.command\\n    isatty = sys.stdin.isatty()\\n\\n    # Delete the REPL command from those available, as we don\\'t want to allow\\n    # nesting REPLs (note: pass `None` to `pop` as we don\\'t want to error if\\n    # REPL command already not present for some reason).\\n    repl_command_name = old_ctx.command.name\\n    if isinstance(group_ctx.command, click.CommandCollection):\\n        available_commands = {\\n            cmd_name: cmd_obj\\n            for source in group_ctx.command.sources\\n            for cmd_name, cmd_obj in source.commands.items()\\n        }\\n    else:\\n        available_commands = group_ctx.command.commands\\n    available_commands.pop(repl_command_name, None)\\n\\n    prompt_kwargs = bootstrap_prompt(prompt_kwargs, group)\\n\\n    if isatty:\\n\\n                    return prompt(**prompt_kwargs)\\n\\n    else:\\n        get_command = sys.stdin.readline\\n\\n    while True:\\n        try:\\n            command = get_command()\\n        except KeyboardInterrupt:\\n            continue\\n        except EOFError:\\n            break\\n\\n        if not command:\\n            if isatty:\\n                continue\\n            else:\\n                break\\n\\n        if allow_system_commands and dispatch_repl_commands(command):\\n            continue\\n\\n        if allow_internal_commands:\\n            try:\\n                result = handle_internal_commands(command)\\n                if isinstance(result, six.string_types):\\n                    click.echo(result)\\n                    continue\\n            except ExitReplException:\\n                break\\n\\n        try:\\n            args = shlex.split(command)\\n        except ValueError as e:\\n            click.echo(\"{}: {}\".format(type(e).__name__, e))\\n            continue\\n\\n        try:\\n            with group.make_context(None, args, parent=group_ctx) as ctx:\\n                group.invoke(ctx)\\n                ctx.exit()\\n        except click.ClickException as e:\\n            e.show()\\n        except ClickExit:\\n            pass\\n        except SystemExit:\\n            pass\\n        except ExitReplException:\\n            break', 'search_document:     \\n    group.command(name=name)(click.pass_context(repl))', 'search_document:     \\n    if command.startswith(\":\"):\\n        target = _get_registered_target(command[1:],             return target()', 'search_document:         \\n        D = self._initialize(X)\\n        for i in range(self.max_iter):\\n            gamma = self._transform(D, X)\\n            e = np.linalg.norm(X - gamma.dot(D))\\n            if e < self.tol:\\n                break\\n            D, gamma = self._update_dict(X, D, gamma)\\n\\n        self.components_ = D\\n        return self', \"search_document:     '''\\n    Given a function to map from an ID to an underlying object, and a function\\n    to map from an underlying object to the concrete GraphQLObjectType it\\n    corresponds to, constructs a `Node` interface that objects can implement,\\n    and a field config for a `node` root field.\\n\\n    If the type_resolver is omitted, object resolution on the interface will be\\n    handled with the `isTypeOf` method on object types, as with any GraphQL\\n    interface without a provided `resolveType` method.\\n    '''\\n    node_interface = GraphQLInterfaceType(\\n        'Node',\\n        description='An object with an ID',\\n        fields=lambda: OrderedDict((\\n            ('id', GraphQLField(\\n                GraphQLNonNull(GraphQLID),\\n                description='The id of the object.',\\n                resolver=id_resolver,\\n            )),\\n        )),\\n        resolve_type=type_resolver\\n    )\\n    node_field = GraphQLField(\\n        node_interface,\\n        description='Fetches an object given its ID',\\n        args=OrderedDict((\\n            ('id', GraphQLArgument(\\n                GraphQLNonNull(GraphQLID),\\n                description='The ID of an object'\\n            )),\\n        )),\\n        resolver=lambda obj, args, *_: id_fetcher(args.get('id'), *_)\\n    )\\n    return node_interface, node_field\", 'search_document:     \\'\\'\\'\\n    Takes the \"global ID\" created by toGlobalID, and retuns the type name and ID\\n    used to create it.\\n    \\'\\'\\'\\n    unbased_global_id = unbase64(global_id)\\n    _type, _id = unbased_global_id.split(\\':\\', 1)\\n    return _type, _id', \"search_document:     '''\\n    Creates the configuration for an id field on a node, using `to_global_id` to\\n    construct the ID from the provided typename. The type-specific ID is fetcher\\n    by calling id_fetcher on the object, or if not provided, by accessing the `id`\\n    property on the object.\\n    '''\\n    return GraphQLField(\\n        GraphQLNonNull(GraphQLID),\\n        description='The ID of an object',\\n        resolver=lambda obj, args, context, info: to_global_id(\\n            type_name or info.parent_type.name,\\n            id_fetcher(obj, context, info) if id_fetcher else obj.id\\n        )\\n    )\", \"search_document:     '''\\n    A simple function that accepts an array and connection arguments, and returns\\n    a connection object for use in GraphQL. It uses array offsets as pagination,\\n    so pagination will only work if the array is static.\\n    '''\\n    _len = len(data)\\n    return connection_from_list_slice(\\n        data,\\n        args,\\n        slice_start=0,\\n        list_length=_len,\\n        list_slice_length=_len,\\n        **kwargs\\n    )\", \"search_document:     '''\\n    A version of `connectionFromArray` that takes a promised array, and returns a\\n    promised connection.\\n    '''\\n    return data_promise.then(lambda data: connection_from_list(data, args, **kwargs))\", \"search_document:     '''\\n    Given a slice (subset) of an array, returns a connection object for use in\\n    GraphQL.\\n    This function is similar to `connectionFromArray`, but is intended for use\\n    cases where you know the cardinality of the connection, consider it too large\\n    to materialize the entire array, and instead wish pass in a slice of the\\n    total result large enough to cover the range specified in `args`.\\n    '''\\n    connection_type = connection_type or Connection\\n    edge_type = edge_type or Edge\\n    pageinfo_type = pageinfo_type or PageInfo\\n\\n    args = args or {}\\n\\n    before = args.get('before')\\n    after = args.get('after')\\n    first = args.get('first')\\n    last = args.get('last')\\n    if list_slice_length is None:\\n        list_slice_length = len(list_slice)\\n    slice_end = slice_start + list_slice_length\\n    before_offset = get_offset_with_        end_offset = min(\\n            end_offset,\\n            start_offset + first\\n        )\\n    if isinstance(last, int):\\n        start_offset = max(\\n            start_offset,\\n            end_offset - last\\n        )\\n\\n    # If supplied slice is too large, trim it down before mapping over it.\\n    _slice = list_slice[\\n        max(start_offset - slice_start, 0):\\n        list_slice_length - (slice_end - end_offset)\\n    ]\\n    edges = [\\n        edge_type(\\n            node=node,\\n            cursor=offset_to_cursor(start_offset + i)\\n        )\\n        for i, node in enumerate(_slice)\\n    ]\\n\\n\\n    first_edge_cursor = edges[0].cursor if edges else None\\n    last_edge_cursor = edges[-1].cursor if edges else None\\n    lower_bound = after_offset + 1 if after else 0\\n    upper_bound = before_offset if before else list_length\\n\\n    return connection_type(\\n        edges=edges,\\n        page_info=pageinfo_type(\\n            start_cursor=first_edge_cursor,\\n            end_cursor=last_edge_cursor,\\n            has_previous_page=isinstance(last, int) and start_offset > lower_bound,\\n            has_next_page=isinstance(first, int) and end_offset < upper_bound\\n        )\\n    )\", \"search_document:     '''\\n    Return the cursor associated with an object in an array.\\n    '''\\n    if _object not in data:\\n        return None\\n\\n    offset = data.index(_object)\\n    return offset_to_cursor(offset)\", \"search_document:     '''\\n    Given an optional cursor and a         return         return int(offset)\\n    except:\\n        return default_offset\", 'search_document:     \\n    # Catch errors on string-based input before getting js involved\\n    shader_options = [\\'toon\\', \\'basic\\', \\'phong\\', \\'lambert\\']\\n    if shader not in shader_options:\\n        raise Exception(\\'Invalid shader! Please use one of: \\' +\\n                        \\', \\'.join(shader_options))\\n\\n    if isinstance(                        graph = json_formatter.dumps(generate(data, iterations=1))\\n    elif isinstance(data, dict):\\n        # Convert color hex to string for json handling\\n        for node_key in data[\\'nodes\\']:\\n            node = data[\\'nodes\\'][node_key]\\n            if \\'color\\' in node and isinstance(node[\\'color\\'], int):\\n                node[\\'color\\'] = hex(node[\\'color\\'])\\n        for edge in data[\\'edges\\']:\\n            if \\'color\\' in edge and isinstance(edge[\\'color\\'], int):\\n                edge[\\'color\\'] = hex(edge[\\'color\\'])\\n        graph = json_formatter.dumps(data)\\n    else:\\n        # Support both files and strings\\n        try:\\n            with open(data) as in_file:\\n                graph = in_file.read()\\n        except:\\n            graph = data\\n\\n    div_id = uuid.uuid4()\\n    html = \\'\\'\\'<div id=\"graph-%(id)s\"></div>\\n           <script type=\"text/javascript\">\\n           require.config({baseUrl: \\'/\\',\\n                             paths: {jgraph: [\\'%(local)s\\', \\'%(remote)s\\']}});\\n           require([\\'jgraph\\'], function () {\\n               var $d = $(\\'#graph-%(id)s\\');\\n               $d.width(%(w)d); $d.height(%(h)d);\\n               $d.jgraph = jQuery.extend({}, jgraph);\\n               $d.jgraph.create($d, {nodeSize: %(node_size)f,\\n                                     edgeSize: %(edge_size)f,\\n                                             display(HTML(html))\\n    else:\\n        return html', \"search_document:     \\n\\n    edges = [{'source': s, 'target': t} for s, t in data]\\n    nodes = force_directed_layout.run(edges, iterations, force_strength,\\n                                      dampening, max_velocity, max_distance,\\n                                      is_3d)\\n    return {'edges': edges, 'nodes': nodes}\", \"search_document:     \\n    return json.dumps(obj, sort_keys=True, separators=(',', ':'),\\n                      cls=CustomEncoder)\", 'search_document:     \\n    return json.dumps(obj, indent=4, sort_keys=True, cls=CustomEncoder)', 'search_document:         \\n        s = super(CustomEncoder, self).encode(obj)\\n        # If uncompressed, postprocess for formatting\\n        if len(s.splitlines()) > 1:\\n            s = self.postprocess(s)\\n        return s', 'search_document:         \\n        is_compressing, is_hash, compressed, spaces = False, False, [], 0\\n        for row in json_string.split(\\'\\\\n\\'):\\n            if is_compressing:\\n                if (row[:spaces + 5] == \\' \\' * (spaces + 4) +\\n                        (\\'\"\\' if is_hash else \\'{\\')):\\n                    compressed.append(row.rstrip())\\n                elif (len(row) > spaces and row[:spaces] == \\' \\' * spaces and\\n                        re.match(\\'[\\\\]\\\\}],?\\', row[spaces:].rstrip())):\\n                    compressed.append(row.rstrip())\\n                    is_compressing = False\\n                else:\\n                    compressed[-1] += \\' \\' + row.strip()\\n            else:\\n                compressed.append(row.rstrip())\\n                if any(a in row for a in [\\'edges\\', \\'nodes\\']):\\n                    # Fix to handle issues that arise with empty lists\\n                    if \\'[]\\' in row:\\n                        continue\\n                    spaces = sum(1 for _ in takewhile(str.isspace, row))\\n                    is_compressing, is_hash = True, \\'{\\' in row\\n        return \\'\\\\n\\'.join(compressed)', \"search_document:     \\n\\n    # Get a list of node ids from the edge data\\n    nodes = set(e['source'] for e in edges) | set(e['target'] for e in edges)\\n\\n    # Convert to a data-storing object and initialize some values\\n    d = 3 if is_3d else 2\\n    nodes = {n: {'velocity': [0.0] * d, 'force': [0.0] * d} for n in nodes}\\n\\n    # Repeat n times (is there a more Pythonic way to do this?)\\n    for _ in repeat(None, iterations):\\n\\n        # Add in Coulomb-esque node-node repulsive forces\\n        for node1, node2 in combinations(nodes.values(), 2):\\n            _coulomb(node1, node2, force_strength, max_distance)\\n\\n        # And Hooke-esque edge spring forces\\n        for edge in edges:\\n            _hooke(nodes[edge['source']], nodes[edge['target']],\\n                   force_strength * edge.get('size', 1), max_distance)\\n\\n        # Move by resultant force\\n        for node in nodes.values():\\n            # Constrain the force to the bounds specified by input parameter\\n            force = [_constrain(dampening * f, -max_velocity, max_velocity)\\n                     for f in node['force']]\\n            # Update velocities and reset force\\n            node['velocity'] = [v + dv\\n                                for v, dv in zip(node['velocity'], force)]\\n            node['force'] = [0] * d\\n\\n    # Clean and return\\n    for node in nodes.values():\\n        del node['force']\\n        node['location'] = node['velocity']\\n        del node['velocity']\\n        # Even if it's 2D, let's specify three dimensions\\n        if not is_3d:\\n            node['location'] += [0.0]\\n    return nodes\", \"search_document:     \\n    # Get relevant positional data\\n    delta = [x2 - x1 for x1, x2 in zip(n1['velocity'], n2['velocity'])]\\n    distance = sqrt(sum(d ** 2 for d in delta))\\n\\n    # If the deltas are too small, use random values to keep things moving\\n    if distance < 0.1:\\n        delta = [uniform(0.1, 0.2) for _ in repeat(None, 3)]\\n        distance = sqrt(sum(d ** 2 for d in delta))\\n\\n    # If the distance isn't huge (ie. Coulomb is negligible), calculate\\n    if distance < r:\\n        force = (k / distance) ** 2\\n        n1['force'] = [f - force * d for f, d in zip(n1['force'], delta)]\\n        n2['force'] = [f + force * d for f, d in zip(n2['force'], delta)]\", 'search_document:     \\n    logger.debug(\"started\")\\n\\n    context.clear()\\n    logger.info(f\"Context wiped. New context size: {len(context)}\")\\n\\n    logger.debug(\"done\")', 'search_document:     \\n    assert context_arg, (\"pipeline must be invoked with context arg set. For \"\\n                         \"this json parser you\\'re looking for something \"\\n                         \"like: \"\\n                         \"pypyr pipelinename \\'./myjsonfile.json\\'\")\\n    logger.debug(\"starting\")\\n    # open the json file on disk so that you can initialize the dictionary\\n    logger.debug(f\"attempting to open file: {context_arg}\")\\n    with open(context_arg) as json_file:\\n        payload = json.load(json_file)\\n\\n    logger.debug(f\"json file loaded into context. Count: {len(payload)}\")\\n    logger.debug(\"done\")\\n    return payload']\n",
            "['search_document:     \\n    logger.debug(\"started\")\\n    context.assert_key_has_value(key=\\'pathCheck\\', caller=__name__)\\n\\n    paths_to_check = context[\\'pathCheck\\']\\n\\n    if not paths_to_check:\\n        raise KeyInContextHasNoValueError(\"context[\\'pathCheck\\'] must have a \"\\n                                          f\"value for {__name__}.\")\\n\\n    # pathsToCheck can be a string or a list in case there are multiple paths\\n    if isinstance(paths_to_check, list):\\n        check_me = paths_to_check\\n    else:\\n        # assuming it\\'s a str/path at this point\\n        check_me = [paths_to_check]\\n\\n    out = {}\\n    total_found = 0\\n\\n    for path in check_me:\\n        logger.debug(f\"checking path: {path}\")\\n        formatted_path = context.get_formatted_string(path)\\n        found_paths = pypyr.utils.filesystem.get_glob(formatted_path)\\n        no_of_paths = len(found_paths)\\n        out[path] = {\\n            \\'exists\\': no_of_paths > 0,\\n            \\'count\\': no_of_paths,\\n            \\'found\\': found_paths\\n        }\\n        total_found = total_found + no_of_paths\\n\\n    context[\\'pathCheckOut\\'] = out\\n\\n    logger.info(f\\'checked {len(out)} path(s) and found {total_found}\\')\\n    logger.debug(\"done\")', 'search_document:     \\n    logger.debug(\"started\")\\n    context.assert_child_key_has_value(\\'fileWriteJson\\', \\'path\\', __name__)\\n\\n    out_path = context.get_formatted_string(context[\\'fileWriteJson\\'][\\'path\\'])\\n    # doing it like this to safeguard against accidentally dumping all context\\n    # with potentially sensitive values in it to disk if payload exists but is\\n    # None.\\n    is_payload_specified = \\'payload\\' in context[\\'fileWriteJson\\']\\n\\n    logger.debug(f\"opening destination file for writing: {out_path}\")\\n    os.makedirs(os.path.abspath(os.path.dirname(out_path)), exist_ok=True)\\n    with open(out_path, \\'w\\') as outfile:\\n        if is_payload_specified:\\n            payload = context[\\'fileWriteJson\\'][\\'payload\\']\\n            formatted_iterable = context.get_formatted_iterable(payload)\\n        else:\\n            formatted_iterable = context.get_formatted_iterable(context)\\n\\n        json.dump(formatted_iterable, outfile, indent=2, ensure_ascii=False)\\n\\n    logger.info(f\"formatted context content and wrote to {out_path}\")\\n    logger.debug(\"done\")', 'search_document:     \\n    logger.debug(\"started\")\\n\\n    (pipeline_name,\\n     use_parent_context,\\n     pipe_arg,\\n     skip_parse,\\n     raise_error,\\n     loader,\\n     ) = get_arguments(context)\\n\\n    try:\\n        if use_parent_context:\\n            logger.info(f\"pyping {pipeline_name}, using parent context.\")\\n            pipelinerunner.load_and_run_pipeline(\\n                pipeline_name=pipeline_name,\\n                pipeline_context_input=pipe_arg,\\n                context=context,\\n                parse_input=not skip_parse,\\n                loader=loader\\n            )\\n        else:\\n            logger.info(f\"pyping {pipeline_name}, without parent context.\")\\n            pipelinerunner.load_and_run_pipeline(\\n                pipeline_name=pipeline_name,\\n                pipeline_context_input=pipe_arg,\\n                working_dir=context.working_dir,\\n                parse_input=not skip_parse,\\n                loader=loader\\n            )\\n\\n        logger.info(f\"pyped {pipeline_name}.\")\\n    except Exception as ex_info:\\n        # yes, yes, don\\'t catch Exception. Have to, though, in order to swallow\\n        # errs if !raise_error\\n        logger.error(f\"Something went wrong pyping {pipeline_name}. \"\\n                     f\"{type(ex_info).__name__}: {ex_info}\")\\n\\n        if raise_error:\\n            logger.debug(\"Raising original exception to caller.\")\\n            raise\\n        else:\\n            logger.debug(\\n                f\"raiseError is False. Swallowing error in {pipeline_name}.\")\\n\\n    logger.debug(\"done\")', 'search_document:     \\n    context.assert_key_has_value(key=\\'pype\\', caller=__name__)\\n    pype = context.get_formatted(\\'pype\\')\\n\\n    try:\\n        pipeline_name = pype[\\'name\\']\\n\\n        if pipeline_name is None:\\n            raise KeyInContextHasNoValueError(\\n                \"pypyr.steps.pype [\\'pype\\'][\\'name\\'] exists but is empty.\")\\n    except KeyError as err:\\n        raise KeyNotInContextError(\\n            \"pypyr.steps.pype missing \\'name\\' in the \\'pype\\' context item. \"\\n            \"You need to specify the pipeline name to run another \"\\n            \"pipeline.\") from err\\n\\n    use_parent_context = pype.get(\\'useParentContext\\', True)\\n    pipe_arg = pype.get(\\'pipeArg\\', None)\\n    skip_parse = pype.get(\\'skipParse\\', True)\\n    raise_error = pype.get(\\'raiseError\\', True)\\n    loader = pype.get(\\'loader\\', None)\\n\\n    return (\\n        pipeline_name,\\n        use_parent_context,\\n        pipe_arg,\\n        skip_parse,\\n        raise_error,\\n        loader,\\n    )', 'search_document:     \\n    logger.debug(\"starting\")\\n\\n    # look for name.yaml in the pipelines/ sub-directory\\n    logger.debug(f\"current directory is {working_directory}\")\\n\\n    # looking for {cwd}/pipelines/[pipeline_name].yaml\\n    pipeline_path = os.path.abspath(os.path.join(\\n        working_directory,\\n        \\'pipelines\\',\\n        pipeline_name + \\'.yaml\\'))\\n\\n    if os.path.isfile(pipeline_path):\\n        logger.debug(f\"Found {pipeline_path}\")\\n    else:\\n        logger.debug(f\"{pipeline_name} not found in current \"\\n                     \"directory/pipelines folder. Looking in pypyr install \"\\n                     \"directory instead.\")\\n        pypyr_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\\n        logger.debug(f\"pypyr installation directory is: {pypyr_dir}\")\\n        pipeline_path = os.path.abspath(os.path.join(\\n            pypyr_dir,\\n            \\'pipelines\\',\\n            pipeline_name + \\'.yaml\\'))\\n\\n        if os.path.isfile(pipeline_path):\\n            logger.debug(f\"Found {pipeline_path}\")\\n        else:\\n            raise PipelineNotFoundError(f\"{pipeline_name}.yaml not found in \"\\n                                        f\"either \"\\n                                        f\"{working_directory}/pipelines \"\\n                                        f\"or {pypyr_dir}/pipelines\")\\n\\n    logger.debug(\"done\")\\n    return pipeline_path', 'search_document:     \\n    logger.debug(\"starting\")\\n\\n    pipeline_path = get_pipeline_path(\\n        pipeline_name=pipeline_name,\\n        working_directory=working_dir)\\n\\n    logger.debug(f\"Trying to open pipeline at path {pipeline_path}\")\\n    try:\\n        with open(pipeline_path) as yaml_file:\\n            pipeline_        logger.error(\\n            \"The pipeline doesn\\'t exist. Looking for a file here: \"\\n            f\"{pipeline_name}.yaml in the /pipelines sub directory.\")\\n        raise\\n\\n    logger.debug(\"pipeline definition loaded\")\\n\\n    logger.debug(\"done\")\\n    return pipeline_definition', 'search_document:         \\n        return representer.represent_scalar(cls.yaml_tag, node.value)', \"search_document:         \\n        if self.value:\\n            return expressions.eval_string(self.value, context)\\n        else:\\n            # Empty input raises cryptic EOF syntax err, this more human\\n            # friendly\\n            raise ValueError('!py string expression is empty. It must be a '\\n                             'valid python expression instead.')\", 'search_document:         \\n        logger.debug(\"starting\")\\n\\n        # Loop decorators only evaluated once, not for every step repeat\\n        # execution.\\n        foreach = context.get_formatted_iterable(self.foreach_items)\\n\\n        foreach_length = len(foreach)\\n\\n        logger.info(f\"foreach decorator will loop {foreach_length} times.\")\\n\\n        for i in foreach:\\n            logger.info(f\"foreach: running step {i}\")\\n            # the iterator must be available to the step when it executes\\n            context[\\'i\\'] = i\\n            # conditional operators apply to each iteration, so might be an\\n            # iteration run, skips or swallows.\\n            self.run_conditional_decorators(context)\\n            logger.debug(f\"foreach: done step {i}\")\\n\\n        logger.debug(f\"foreach decorator looped {foreach_length} times.\")\\n        logger.debug(\"done\")', 'search_document:         \\n        logger.debug(\"starting\")\\n\\n        logger.debug(f\"running step {self.module}\")\\n\\n        self.run_step_function(context)\\n\\n        logger.debug(f\"step {self.module} done\")', 'search_document:         \\n        logger.debug(\"starting\")\\n\\n        # The decorator attributes might contain formatting expressions that\\n        # change whether they evaluate True or False, thus apply formatting at\\n        # last possible instant.\\n        run_me = context.get_formatted_as_type(self.run_me, out_type=bool)\\n        skip_me = context.get_formatted_as_type(self.skip_me, out_type=bool)\\n        swallow_me = context.get_formatted_as_type(self.swallow_me,\\n                                                   out_type=bool)\\n\\n        if run_me:\\n            if not skip_me:\\n                try:\\n                    if self.retry_decorator:\\n                        self.retry_decorator.retry_loop(context,\\n                                                        self.invoke_step)\\n                    else:\\n                        self.invoke_step(context=context)\\n                except Exception as ex_info:\\n                    if swallow_me:\\n                        logger.error(\\n                            f\"{self.name} Ignoring error because swallow \"\\n                            \"is True for this step.\\\\n\"\\n                            f\"{type(ex_info).__name__}: {ex_info}\")\\n                    else:\\n                        raise\\n            else:\\n                logger.info(\\n                    f\"{self.name} not running because skip is True.\")\\n        else:\\n            logger.info(f\"{self.name} not running because run is False.\")\\n\\n        logger.debug(\"done\")', 'search_document:         \\n        logger.debug(\"starting\")\\n        # friendly reminder [] list obj (i.e empty) evals False\\n        if self.foreach_items:\\n            self.foreach_loop(context)\\n        else:\\n            # since no looping required, don\\'t pollute output with looping info\\n            self.run_conditional_decorators(context)\\n\\n        logger.debug(\"done\")', 'search_document:         \\n        logger.debug(\"starting\")\\n        # the in params should be added to context before step execution.\\n        self.set_step_input_context(context)\\n\\n        if self.while_decorator:\\n            self.while_decorator.while_loop(context,\\n                                            self.run_foreach_or_conditional)\\n        else:\\n            self.run_foreach_or_conditional(context)\\n\\n        logger.debug(\"done\")', 'search_document:         \\n        logger.debug(\"starting\")\\n        if self.in_parameters is not None:\\n            parameter_count = len(self.in_parameters)\\n            if parameter_count > 0:\\n                logger.debug(\\n                    f\"Updating context with {parameter_count} \\'in\\' \"\\n                    \"parameters.\")\\n                context.update(self.in_parameters)\\n\\n        logger.debug(\"done\")', 'search_document:         \\n        logger.debug(\"starting\")\\n        context[\\'retryCounter\\'] = counter\\n\\n        logger.info(f\"retry: running step with counter {counter}\")\\n        try:\\n            step_method(context)\\n            result = True\\n        except Exception as ex_info:\\n            if self.max:\\n                if counter == self.max:\\n                    logger.debug(f\"retry: max {counter} retries exhausted. \"\\n                                 \"raising error.\")\\n                    # arguably shouldn\\'t be using errs for control of flow.\\n                    # but would lose the err info if not, so lesser of 2 evils.\\n                    raise\\n\\n            if self.stop_on or self.retry_on:\\n                error_name = get_error_name(ex_info)\\n                if self.stop_on:\\n                    formatted_stop_list = context.get_formatted_iterable(\\n                        self.stop_on)\\n                    if error_name in formatted_stop_list:\\n                        logger.error(f\"{error_name} in stopOn. Raising error \"\\n                                     \"and exiting retry.\")\\n                        raise\\n                    else:\\n                        logger.debug(f\"{error_name} not in stopOn. Continue.\")\\n\\n                if self.retry_on:\\n                    formatted_retry_list = context.get_formatted_iterable(\\n                        self.retry_on)\\n                    if error_name not in formatted_retry_list:\\n                        logger.error(f\"{error_name} not in retryOn. Raising \"\\n                                     \"error and exiting retry.\")\\n                        raise\\n                    else:\\n                        logger.debug(f\"{error_name} in retryOn. Retry again.\")\\n\\n            result = False\\n            logger.error(f\"retry: ignoring error because retryCounter < max.\\\\n\"\\n                         f\"{type(ex_info).__name__}: {ex_info}\")\\n\\n        logger.debug(f\"retry: done step with counter {counter}\")\\n\\n        logger.debug(\"done\")\\n        return result', 'search_document:         \\n        logger.debug(\"starting\")\\n\\n        context[\\'retryCounter\\'] = 0\\n\\n        sleep = context.get_formatted_as_type(self.sleep, out_type=float)\\n        if self.max:\\n            max = context.get_formatted_as_type(self.max, out_type=int)\\n\\n            logger.info(f\"retry decorator will try {max} times at {sleep}s \"\\n                        \"intervals.\")\\n        else:\\n            max = None\\n            logger.info(f\"retry decorator will try indefinitely at {sleep}s \"\\n                        \"intervals.\")\\n\\n        # this will never be false. because on counter == max,\\n        # exec_iteration raises an exception, breaking out of the loop.\\n        # pragma because cov doesn\\'t know the implied else is impossible.\\n        # unit test cov is 100%, though.\\n        if poll.while_until_true(interval=sleep,\\n                                 max_attempts=max)(\\n                self.exec_iteration)(context=context,\\n                                     step_method=step_method\\n                                     ):  # pragma: no cover\\n            logger.debug(\"retry loop complete, reporting success.\")\\n\\n        logger.debug(\"retry loop done\")\\n\\n        logger.debug(\"done\")', 'search_document:         \\n        logger.debug(\"starting\")\\n        context[\\'whileCounter\\'] = counter\\n\\n        logger.info(f\"while: running step with counter {counter}\")\\n        step_method(context)\\n        logger.debug(f\"while: done step {counter}\")\\n\\n        result = False\\n        # if no stop, just iterating to max)\\n        if self.stop:\\n            # dynamically evaluate stop after step execution, since the step\\n            # might have changed True/False status for stop.\\n            result = context.get_formatted_as_type(self.stop, out_type=bool)\\n\\n        logger.debug(\"done\")\\n        return result', 'search_document:         \\n        logger.debug(\"starting\")\\n\\n        context[\\'whileCounter\\'] = 0\\n\\n        if self.stop is None and self.max is None:\\n            # the ctor already does this check, but guess theoretically\\n            # consumer could have messed with the props since ctor\\n            logger.error(f\"while decorator missing both max and stop.\")\\n            raise PipelineDefinitionError(\"the while decorator must have \"\\n                                          \"either max or stop, or both. \"\\n                                          \"But not neither.\")\\n\\n        error_on_max = context.get_formatted_as_type(\\n            self.error_on_max, out_type=bool)\\n        sleep = context.get_formatted_as_type(self.sleep, out_type=float)\\n        if self.max is None:\\n            max = None\\n            logger.info(f\"while decorator will loop until {self.stop} \"\\n                        f\"evaluates to True at {sleep}s intervals.\")\\n        else:\\n            max = context.get_formatted_as_type(self.max, out_type=int)\\n\\n            if max < 1:\\n                logger.info(\\n                    f\"max {self.max} is {max}. while only runs when max > 0.\")\\n                logger.debug(\"done\")\\n                return\\n\\n            if self.stop is None:\\n                logger.info(f\"while decorator will loop {max} times at \"\\n                            f\"{sleep}s intervals.\")\\n            else:\\n                logger.info(f\"while decorator will loop {max} times, or \"\\n                            f\"until {self.stop} evaluates to True at \"\\n                            f\"{sleep}s intervals.\")\\n\\n        if not poll.while_until_true(interval=sleep,\\n                                     max_attempts=max)(\\n                self.exec_iteration)(context=context,\\n                                     step_method=step_method):\\n            # False means loop exhausted and stop never eval-ed True.\\n            if error_on_max:\\n                logger.error(f\"exhausted {max} iterations of while loop, \"\\n                             \"and errorOnMax is True.\")\\n                if self.stop and max:\\n                    raise LoopMaxExhaustedError(\"while loop reached \"\\n                                                f\"{max} and {self.stop} \"\\n                                                \"never evaluated to True.\")\\n                else:\\n                    raise LoopMaxExhaustedError(f\"while loop reached {max}.\")\\n            else:\\n                if self.stop and max:\\n                    logger.info(\\n                        f\"while decorator looped {max} times, \"\\n                        f\"and {self.stop} never evaluated to True.\")\\n\\n            logger.debug(\"while loop done\")\\n        else:\\n            logger.info(f\"while loop done, stop condition {self.stop} \"\\n                        \"evaluated True.\")\\n\\n        logger.debug(\"done\")', 'search_document:     \\n    logger.debug(\"started\")\\n\\n    deprecated(context)\\n\\n    context.assert_key_has_value(key=\\'fetchYaml\\', caller=__name__)\\n\\n    fetch_yaml_input = context.get_formatted(\\'fetchYaml\\')\\n\\n    if isinstance(fetch_yaml_input, str):\\n        file_path = fetch_yaml_input\\n        destination_key_expression = None\\n    else:\\n        context.assert_child_key_has_value(parent=\\'fetchYaml\\',\\n                                           child=\\'path\\',\\n                                           caller=__name__)\\n        file_path = fetch_yaml_input[\\'path\\']\\n        destination_key_expression = fetch_yaml_input.get(\\'key\\', None)\\n\\n    logger.debug(f\"attempting to open file: {file_path}\")\\n    with open(file_path) as yaml_file:\\n        yaml_loader = yaml.YAML(typ=\\'safe\\', pure=True)\\n        payload = yaml_loader.load(yaml_file)\\n\\n    if destination_key_expression:\\n        destination_key = context.get_formatted_iterable(\\n            destination_key_expression)\\n        logger.debug(f\"yaml file loaded. Writing to context {destination_key}\")\\n        context[destination_key] = payload\\n    else:\\n        if not isinstance(payload, MutableMapping):\\n            raise TypeError(\\n                \"yaml input should describe a dictionary at the top \"\\n                \"level when fetchYamlKey isn\\'t specified. You should have \"\\n                \"something like \\\\n\\'key1: value1\\'\\\\n key2: value2\\'\\\\n\"\\n                \"in the yaml top-level, not \\\\n\\'- value1\\\\n - value2\\'\")\\n\\n        logger.debug(\"yaml file loaded. Merging into pypyr context. . .\")\\n        context.update(payload)\\n\\n    logger.info(f\"yaml file written into pypyr context. Count: {len(payload)}\")\\n    logger.debug(\"done\")', 'search_document:     \\n    logger.debug(\"started\")\\n\\n    deprecated(context)\\n\\n    ObjectRewriterStep(__name__, \\'fileFormatYaml\\', context).run_step(\\n        YamlRepresenter())\\n\\n    logger.debug(\"done\")', 'search_document:     \\n            logger.debug(\"started\")\\n\\n                    logger.debug(f\"Looping every {interval} seconds for \"\\n                         f\"{max_attempts} attempts\")\\n            for i in range(1, max_attempts + 1):\\n                result = f(*args, **kwargs)\\n                if result:\\n                    logger.debug(f\"iteration {i}. Desired state reached.\")\\n                    return True\\n                if i < max_attempts:\\n                    logger.debug(f\"iteration {i}. Still waiting. . .\")\\n                    time.sleep(interval)\\n            logger.debug(\"done\")\\n            return False\\n        return sleep_looper\\n\\n    return decorator', 'search_document:     \\n            logger.debug(\"started\")\\n\\n                    if max_attempts:\\n                logger.debug(f\"Looping every {interval} seconds for \"\\n                             f\"{max_attempts} attempts\")\\n            else:\\n                logger.debug(f\"Looping every {interval} seconds.\")\\n\\n            i = 0\\n            result = False\\n\\n            # pragma for coverage: cov can\\'t figure out the branch construct\\n            # with the dynamic function invocation, it seems, so marks the\\n            # branch partial. unit test cov is 100%, though.\\n            while not result:  # pragma: no branch\\n                i += 1\\n                result = f(i, *args, **kwargs)\\n                if result:\\n                    logger.debug(f\"iteration {i}. Desired state reached.\")\\n                    break\\n                elif max_attempts:\\n                    if i < max_attempts:\\n                        logger.debug(f\"iteration {i}. Still waiting. . .\")\\n                        time.sleep(interval)\\n                    else:\\n                        logger.debug(f\"iteration {i}. Max attempts exhausted.\")\\n                        break\\n                else:\\n                    # result False AND max_attempts is None means keep looping\\n                    # because None = infinite\\n                    logger.debug(f\"iteration {i}. Still waiting. . .\")\\n                    time.sleep(interval)\\n            logger.debug(\"done\")\\n            return result\\n\\n        return sleep_looper\\n\\n    return decorator', 'search_document:     \\n    logger.debug(\"started\")\\n    deprecated(context)\\n\\n    StreamRewriterStep(__name__, \\'fileFormat\\', context).run_step()\\n\\n    logger.debug(\"done\")', 'search_document:     \\n    if \\'fileFormatIn\\' in context:\\n        context.assert_keys_have_values(__name__,\\n                                        \\'fileFormatIn\\',\\n                                        \\'fileFormatOut\\')\\n\\n        context[\\'fileFormat\\'] = {\\'in\\': context[\\'fileFormatIn\\'],\\n                                 \\'out\\': context[\\'fileFormatOut\\']}\\n\\n        logger.warning(\"fileFormatIn and fileFormatOut \"\\n                       \"are deprecated. They will stop working upon the next \"\\n                       \"major release. Use the new context key fileFormat \"\\n                       \"instead. It\\'s a lot better, promise! For the moment \"\\n                       \"pypyr is creating the new fileFormat key for you \"\\n                       \"under the hood.\")', 'search_document:     \\n    logger.debug(\"started\")\\n\\n    format_expression = context.get(\\'nowUtcIn\\', None)\\n\\n    if format_expression:\\n        formatted_expression = context.get_formatted_string(format_expression)\\n        context[\\'nowUtc\\'] = datetime.now(\\n            timezone.utc).strftime(formatted_expression)\\n    else:\\n        context[\\'nowUtc\\'] = datetime.now(timezone.utc).isoformat()\\n\\n    logger.info(f\"timestamp {context[\\'nowUtc\\']} saved to context nowUtc\")\\n    logger.debug(\"done\")', 'search_document:     \\n    logger.debug(\"started\")\\n    assert context, f\"context must have value for {__name__}\"\\n    deprecated(context)\\n\\n    context.assert_key_has_value(\\'env\\', __name__)\\n\\n    found_get = env_get(context)\\n    found_set = env_set(context)\\n    found_unset = env_unset(context)\\n\\n    # at least 1 of envGet, envSet or envUnset must exist in context\\n    if not (found_get or found_set or found_unset):\\n        raise KeyNotInContextError(\\n            \"context must contain any combination of \"\\n            \"env[\\'get\\'], env[\\'set\\'] or env[\\'unset\\'] for \"\\n            f\"{__name__}\")\\n\\n    logger.debug(\"done\")', 'search_document:     \\n    get = context[\\'env\\'].get(\\'get\\', None)\\n\\n    exists = False\\n    if get:\\n        logger.debug(\"start\")\\n\\n        for k, v in get.items():\\n            logger.debug(f\"setting context {k} to $ENV {v}\")\\n            context[k] = os.environ[v]\\n\\n        logger.info(f\"saved {len(get)} $ENVs to context.\")\\n        exists = True\\n\\n        logger.debug(\"done\")\\n    return exists', 'search_document:     \\n    env_set = context[\\'env\\'].get(\\'set\\', None)\\n\\n    exists = False\\n    if env_set:\\n        logger.debug(\"started\")\\n\\n        for k, v in env_set.items():\\n            logger.debug(f\"setting ${k} to context[{v}]\")\\n            os.environ[k] = context.get_formatted_string(v)\\n\\n        logger.info(f\"set {len(env_set)} $ENVs from context.\")\\n        exists = True\\n\\n        logger.debug(\"done\")\\n\\n    return exists', 'search_document:     \\n    unset = context[\\'env\\'].get(\\'unset\\', None)\\n\\n    exists = False\\n    if unset:\\n        logger.debug(\"started\")\\n\\n        for env_var_name in unset:\\n            logger.debug(f\"unsetting ${env_var_name}\")\\n            try:\\n                del os.environ[env_var_name]\\n            except KeyError:\\n                # If user is trying to get rid of the $ENV, if it doesn\\'t\\n                # exist, no real point in throwing up an error that the thing\\n                # you\\'re trying to be rid off isn\\'t there anyway.\\n                logger.debug(\\n                    f\"${env_var_name} doesn\\'t exist anyway. As you were.\")\\n\\n        logger.info(f\"unset {len(unset)} $ENVs.\")\\n        exists = True\\n\\n        logger.debug(\"done\")\\n    return exists', 'search_document:     \\n    env = context.get(\\'env\\', None)\\n\\n    get_info, set_info, unset_info = context.keys_of_type_exist(\\n        (\\'envGet\\', dict),\\n        (\\'envSet\\', dict),\\n        (\\'envUnset\\', list)\\n    )\\n\\n    found_at_least_one = (get_info.key_in_context or set_info.key_in_context\\n                          or unset_info.key_in_context)\\n\\n    if found_at_least_one:\\n        env = context[\\'env\\'] = {}\\n    else:\\n        return\\n\\n    if get_info.key_in_context and get_info.is_expected_type:\\n        env[\\'get\\'] = context[get_info.key]\\n\\n    if set_info.key_in_context and set_info.is_expected_type:\\n        env[\\'set\\'] = context[set_info.key]\\n\\n    if unset_info.key_in_context and unset_info.is_expected_type:\\n        env[\\'unset\\'] = context[unset_info.key]\\n\\n    logger.warning(\"envGet, envSet and envUnset are deprecated. They will \"\\n                   \"stop working upon the next major release. \"\\n                   \"Use the new context key env instead. It\\'s a lot \"\\n                   \"better, promise! For the moment pypyr is creating the \"\\n                   \"new env key for you under the hood.\")', 'search_document:     \\n    logger.debug(\"started\")\\n    assert context, f\"context must have value for {__name__}\"\\n\\n    deprecated(context)\\n    context.assert_key_has_value(\\'assert\\', __name__)\\n\\n    assert_this = context[\\'assert\\'][\\'this\\']\\n    is_equals_there = \\'equals\\' in context[\\'assert\\']\\n    if is_equals_there:\\n        assert_equals = context[\\'assert\\'][\\'equals\\']\\n        # compare assertThis to assertEquals\\n        logger.debug(\"comparing assert[\\'this\\'] to assert[\\'equals\\'].\")\\n        assert_result = (context.get_formatted_iterable(assert_this)\\n                         == context.get_formatted_iterable(assert_equals))\\n    else:\\n        # nothing to compare means treat assertThis as a bool.\\n        logger.debug(\"evaluating assert[\\'this\\'] as a boolean.\")\\n        assert_result = context.get_formatted_as_type(assert_this,\\n                                                      out_type=bool)\\n\\n    logger.info(f\"assert evaluated to {assert_result}\")\\n\\n    if not assert_result:\\n        if is_equals_there:\\n            # emit type to help user, but not the actual field contents.\\n            type_this = (\\n                type(context.get_formatted_iterable(assert_this)).__name__)\\n            type_equals = (\\n                type(context.get_formatted_iterable(assert_equals)).__name__)\\n            error_text = (\\n                f\"assert assert[\\'this\\'] is of type {type_this} \"\\n                f\"and does not equal assert[\\'equals\\'] of type {type_equals}.\")\\n        else:\\n            # if it\\'s a bool it\\'s presumably not a sensitive value.\\n            error_text = (\\n                f\"assert {assert_this} evaluated to False.\")\\n        raise ContextError(error_text)\\n\\n    logger.debug(\"done\")', 'search_document:     \\n    assert_context = context.get(\\'assert\\', None)\\n    # specifically do \"key in dict\" to avoid python bool eval thinking\\n    # None/Empty values mean the key isn\\'t there.\\n    if \\'assertThis\\' in context:\\n        assert_this = context[\\'assertThis\\']\\n        assert_context = context[\\'assert\\'] = {\\'this\\': assert_this}\\n\\n        if \\'assertEquals\\' in context:\\n            assert_equals = context[\\'assertEquals\\']\\n            assert_context[\\'equals\\'] = assert_equals\\n\\n        logger.warning(\"assertThis and assertEquals are deprecated. They will \"\\n                       \"stop working upon the next major release. \"\\n                       \"Use the new context key assert instead. It\\'s a lot \"\\n                       \"better, promise! For the moment pypyr is creating the \"\\n                       \"new assert key for you under the hood.\")']\n",
            "['search_document:     \\n    logger.debug(\"started\")\\n\\n    assert context, f\"context must have value for {__name__}\"\\n\\n    deprecated(context)\\n    found_at_least_one = False\\n\\n    context.assert_key_has_value(\\'tar\\', __name__)\\n\\n    tar = context[\\'tar\\']\\n    if \\'extract\\' in tar:\\n        found_at_least_one = True\\n        tar_extract(context)\\n\\n    if \\'archive\\' in tar:\\n        found_at_least_one = True\\n        tar_archive(context)\\n\\n    if not found_at_least_one:\\n        # This will raise exception on first item with a problem.\\n        raise KeyNotInContextError(\\'pypyr.steps.tar must have either extract \\'\\n                                   \\'or archive specified under the tar key. \\'\\n                                   \\'Or both of these. It has neither.\\')\\n\\n    logger.debug(\"done\")', 'search_document:     \\n    format = context[\\'tar\\'].get(\\'format\\', None)\\n\\n    if format or format == \\'\\':\\n        mode = f\"r:{context.get_formatted_string(format)}\"\\n    else:\\n        mode = \\'r:*\\'\\n\\n    return mode', 'search_document:     \\n    format = context[\\'tar\\'].get(\\'format\\', None)\\n    # slightly weird double-check because falsy format could mean either format\\n    # doesn\\'t exist in input, OR that it exists and is empty. Exists-but-empty\\n    # has special meaning -         mode = f\"w:{context.get_formatted_string(format)}\"\\n    else:\\n        mode = \\'w:xz\\'\\n\\n    return mode', 'search_document:     \\n    logger.debug(\"start\")\\n\\n    mode = get_file_mode_for_writing(context)\\n\\n    for item in context[\\'tar\\'][\\'archive\\']:\\n        # value is the destination tar. Allow string interpolation.\\n        destination = context.get_formatted_string(item[\\'out\\'])\\n        # key is the source to archive\\n        source = context.get_formatted_string(item[\\'in\\'])\\n        with tarfile.open(destination, mode) as archive_me:\\n            logger.debug(f\"Archiving \\'{source}\\' to \\'{destination}\\'\")\\n\\n            archive_me.add(source, arcname=\\'.\\')\\n            logger.info(f\"Archived \\'{source}\\' to \\'{destination}\\'\")\\n\\n    logger.debug(\"end\")', 'search_document:     \\n    logger.debug(\"start\")\\n\\n    mode = get_file_mode_for_reading(context)\\n\\n    for item in context[\\'tar\\'][\\'extract\\']:\\n        # in is the path to the tar to extract. Allows string interpolation.\\n        source = context.get_formatted_string(item[\\'in\\'])\\n        # out is the outdir, dhur. Allows string interpolation.\\n        destination = context.get_formatted_string(item[\\'out\\'])\\n        with tarfile.open(source, mode) as extract_me:\\n            logger.debug(f\"Extracting \\'{source}\\' to \\'{destination}\\'\")\\n\\n            extract_me.extractall(destination)\\n            logger.info(f\"Extracted \\'{source}\\' to \\'{destination}\\'\")\\n\\n    logger.debug(\"end\")', 'search_document:     \\n    tar = context.get(\\'tar\\', None)\\n\\n    # at least 1 of tarExtract or tarArchive must exist in context\\n    tar_extract, tar_archive = context.keys_of_type_exist(\\n        (\\'tarExtract\\', list),\\n        (\\'tarArchive\\', list))\\n\\n    found_at_least_one = (tar_extract.key_in_context\\n                          or tar_archive.key_in_context)\\n\\n    if tar and not found_at_least_one:\\n        return\\n    elif found_at_least_one:\\n        tar = context[\\'tar\\'] = {}\\n\\n    if tar_extract.key_in_context and tar_extract.is_expected_type:\\n        tar[\\'extract\\'] = context[tar_extract.key]\\n\\n    if tar_archive.key_in_context and tar_archive.is_expected_type:\\n        tar[\\'archive\\'] = context[tar_archive.key]\\n\\n    if \\'tarFormat\\' in context:\\n        tar[\\'format\\'] = context[\\'tarFormat\\']\\n\\n    logger.warning(\"tarExtract and tarArchive are deprecated. They will \"\\n                   \"stop working upon the next major release. \"\\n                   \"Use the new context key env instead. It\\'s a lot \"\\n                   \"better, promise! For the moment pypyr is creating the \"\\n                   \"new env key for you under the hood.\")', 'search_document:     \\n    logger.debug(\"started\")\\n\\n    CmdStep(name=__name__, context=context).run_step(is_shell=True)\\n\\n    logger.debug(\"done\")', 'search_document:     \\n    logger.debug(\"started\")\\n    assert context, f\"context must have value for {__name__}\"\\n\\n    context.assert_key_has_value(\\'envGet\\', __name__)\\n\\n    # allow a list OR a single getenv dict\\n    if isinstance(context[\\'envGet\\'], list):\\n        get_items = context[\\'envGet\\']\\n    else:\\n        get_items = [context[\\'envGet\\']]\\n\\n    get_count = 0\\n\\n    for get_me in get_items:\\n        (env, key, has_            context[formatted_key] = os.environ[formatted_env]\\n            get_count += 1\\n        else:\\n            logger.debug(f\"$ENV {env} not found.\")\\n            if has_                logger.debug(f\"Using                 logger.debug(\\n                    f\"No default value for {env} found. Doin nuthin\\'.\")\\n\\n    logger.info(f\"saved {get_count} $ENVs to context.\")', \"search_document:     \\n    if not isinstance(get_item, dict):\\n        raise ContextError('envGet must contain a list of dicts.')\\n\\n    env = get_item.get('env', None)\\n\\n    if not env:\\n        raise KeyNotInContextError(\\n            'context envGet[env] must exist in context for envGet.')\\n\\n    key = get_item.get('key', None)\\n\\n    if not key:\\n        raise KeyNotInContextError(\\n            'context envGet[key] must exist in context for envGet.')\\n\\n    if '        has_        has_default = False\\n        default = None\\n\\n    return (env, key, has_default, default)\", 'search_document:     \\n    logger.debug(\"started\")\\n    context.assert_key_has_value(key=\\'pycode\\', caller=__name__)\\n\\n    logger.debug(f\"Executing python string: {context[\\'pycode\\']}\")\\n    locals_dictionary = locals()\\n    exec(context[\\'pycode\\'], globals(), locals_dictionary)\\n\\n    # It looks like this dance might be unnecessary in python 3.6\\n    logger.debug(\"looking for context update in exec\")\\n    exec_context = locals_dictionary[\\'context\\']\\n    context.update(exec_context)\\n    logger.debug(\"exec output context merged with pipeline context\")\\n\\n    logger.debug(\"done\")', 'search_document:     \\n    assert context_arg, (\"pipeline must be invoked with context arg set. For \"\\n                         \"this yaml parser you\\'re looking for something \"\\n                         \"like: \"\\n                         \"pypyr pipelinename \\'./myyamlfile.yaml\\'\")\\n    logger.debug(\"starting\")\\n    logger.debug(f\"attempting to open file: {context_arg}\")\\n    with open(context_arg) as yaml_file:\\n        yaml_loader = yaml.YAML(typ=\\'safe\\', pure=True)\\n        payload = yaml_loader.load(yaml_file)\\n\\n    logger.debug(f\"yaml file parsed. Count: {len(payload)}\")\\n\\n    if not isinstance(payload, MutableMapping):\\n        raise TypeError(\"yaml input should describe a dictionary at the top \"\\n                        \"level. You should have something like \"\\n                        \"\\\\n\\'key1: value1\\'\\\\n key2: value2\\'\\\\n\"\\n                        \"in the yaml top-level, not \\\\n\\'- value1\\\\n - value2\\'\")\\n\\n    logger.debug(\"done\")\\n    return payload', \"search_document:     \\n    parser = argparse.ArgumentParser(\\n        allow_abbrev=True,\\n        description='pypyr pipeline runner')\\n    parser.add_argument('pipeline_name',\\n                        help='Name of pipeline to run. It should exist in the '\\n                        './pipelines directory.')\\n    parser.add_argument(dest='pipeline_context',\\n                        nargs='?',\\n                        help='String for context values. Parsed by the '\\n                        'pipeline\\\\'s context_parser function.')\\n    parser.add_argument('--dir', dest='working_dir', default=os.getcwd(),\\n                        help='Working directory. Use if your pipelines '\\n                        'directory is elsewhere. Defaults to cwd.')\\n    parser.add_argument('--log', '--loglevel', dest='log_level', type=int,\\n                        default=20,\\n                        help='Integer log level. Defaults to 20 (INFO). '\\n                        '10=DEBUG\\\\n20=INFO\\\\n30=WARNING\\\\n40=ERROR\\\\n50=CRITICAL'\\n                        '.\\\\n Log Level < 10 gives full traceback on errors.')\\n    parser.add_argument('--logpath', dest='log_path',\\n                        help='Log-file path. Append log output to this path')\\n    parser.add_argument('--version', action='version',\\n                        help='Echo version number.',\\n                        version=f'{pypyr.version.get_version()}')\\n    return parser\", 'search_document:     \\n    if args is None:\\n        args = sys.argv[1:]\\n\\n    parsed_args = get_args(args)\\n\\n    try:\\n        return pypyr.pipelinerunner.main(\\n            pipeline_name=parsed_args.pipeline_name,\\n            pipeline_context_input=parsed_args.pipeline_context,\\n            working_dir=parsed_args.working_dir,\\n            log_level=parsed_args.log_level,\\n            log_path=parsed_args.log_path)\\n    except KeyboardInterrupt:\\n        # Shell standard is 128 + signum = 130 (SIGINT = 2)\\n        sys.stdout.write(\"\\\\n\")\\n        return 128 + signal.SIGINT\\n    except Exception as e:\\n        # stderr and exit code 255\\n        sys.stderr.write(\"\\\\n\")\\n        sys.stderr.write(f\"\\\\033[91m{type(e).__name__}: {str(e)}\\\\033[0;0m\")\\n        sys.stderr.write(\"\\\\n\")\\n        # at this point, you\\'re guaranteed to have args and thus log_level\\n        if parsed_args.log_level < 10:\\n            # traceback prints to stderr by default\\n            traceback.print_exc()\\n\\n        return 255', 'search_document:         \\n        assert is_shell is not None, (\"is_shell param must exist for CmdStep.\")\\n\\n        # why? If shell is True, it is recommended to pass args as a string\\n        # rather than as a sequence.\\n        if is_shell:\\n            args = self.cmd_text\\n        else:\\n            args = shlex.split(self.cmd_text)\\n\\n        if self.is_save:\\n            completed_process = subprocess.run(args,\\n                                               cwd=self.cwd,\\n                                               shell=is_shell,\\n                                               # capture_output=True,only>py3.7\\n                                               stdout=subprocess.PIPE,\\n                                               stderr=subprocess.PIPE,\\n                                               # text=True, only>=py3.7,\\n                                               universal_newlines=True)\\n            self.context[\\'cmdOut\\'] = {\\n                \\'returncode\\': completed_process.returncode,\\n                \\'stdout\\': completed_process.stdout,\\n                \\'stderr\\': completed_process.stderr\\n            }\\n\\n            # when capture is true, output doesn\\'t write to stdout\\n            self.logger.info(f\"stdout: {completed_process.stdout}\")\\n            if completed_process.stderr:\\n                self.logger.error(f\"stderr: {completed_process.stderr}\")\\n\\n            # don\\'t swallow the error, because it\\'s the Step swallow decorator\\n            # responsibility to decide to ignore or not.\\n            completed_process.check_returncode()\\n        else:\\n            # check=True throws CalledProcessError if exit code != 0\\n            subprocess.run(args, shell=is_shell, check=True, cwd=self.cwd)', 'search_document:     \\n    logger.debug(\"started\")\\n    context.assert_key_has_value(key=\\'contextClear\\', caller=__name__)\\n\\n    for k in context[\\'contextClear\\']:\\n        logger.debug(f\"removing {k} from context\")\\n        # slightly unorthodox pop returning None means you don\\'t get a KeyError\\n        # if key doesn\\'t exist\\n        context.pop(k, None)\\n        logger.info(f\"removed {k} from context\")\\n\\n    logger.debug(\"done\")', 'search_document:     \\n    logger.debug(\"started\")\\n\\n    pypyr.steps.cmd.run_step(context)\\n\\n    logger.debug(\"done\")', 'search_document:     \\n    logger.debug(\"started\")\\n    context.assert_key_has_value(key=\\'defaults\\', caller=__name__)\\n\\n    context.set_defaults(context[\\'defaults\\'])\\n\\n    logger.info(f\"set {len(context[\\'defaults\\'])} context item defaults.\")\\n\\n    logger.debug(\"done\")', 'search_document:     \\n    logger.debug(\"starting\")\\n    assert pipeline\\n    assert steps_group\\n\\n    logger.debug(f\"retrieving {steps_group} steps from pipeline\")\\n    if steps_group in pipeline:\\n        steps = pipeline[steps_group]\\n\\n        if steps is None:\\n            logger.warn(\\n                f\"{steps_group}: sequence has no elements. So it won\\'t do \"\\n                \"anything.\")\\n            logger.debug(\"done\")\\n            return None\\n\\n        steps_count = len(steps)\\n\\n        logger.debug(f\"{steps_count} steps found under {steps_group} in \"\\n                     \"pipeline         logger.debug(\\n            f\"pipeline doesn\\'t have a {steps_group} collection. Add a \"\\n            f\"{steps_group}: sequence to the yaml if you want {steps_group} \"\\n            \"actually to do something.\")\\n        logger.debug(\"done\")\\n        return None', 'search_document:     \\n    logger.debug(\"starting\")\\n    try:\\n        assert pipeline\\n        # if no on_failure exists, it\\'ll do nothing.\\n        run_step_group(pipeline_        logger.error(\"Failure handler also failed. Swallowing.\")\\n        logger.error(exception)\\n\\n    logger.debug(\"done\")', 'search_document:     \\n    logger.debug(\"starting\")\\n    assert isinstance(\\n        context, dict), \"context must be a dictionary, even if empty {}.\"\\n\\n    if steps is None:\\n        logger.debug(\"No steps found to execute.\")\\n    else:\\n        step_count = 0\\n\\n        for step in steps:\\n            step_instance = Step(step)\\n            step_instance.run_step(context)\\n            step_count += 1\\n\\n        logger.debug(f\"executed {step_count} steps\")\\n\\n    logger.debug(\"done\")']\n",
            "Nomic: 0.444\n",
            "Voyage: 0.672\n",
            "OpenAI: 0.52\n"
          ]
        }
      ],
      "source": [
        "print(\"Nomic:\",embedding_accuracy(with_embeddings(get_embeddings_nomic,all_objs)))\n",
        "print(\"Voyage:\",embedding_accuracy(with_embeddings(get_embeddings_voyage,all_objs)))\n",
        "print(\"OpenAI:\",embedding_accuracy(with_embeddings(get_embeddings_openai,all_objs)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## LoRAing Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "gonna train\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "get_embeddings_lora() got an unexpected keyword argument 'is_query'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[72], line 130\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Train loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Val loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(val_loader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Val accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_accuracy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgonna train\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 130\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_objs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_objs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2e-4\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[72], line 125\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_objs, val_objs, num_epochs, batch_size, lr)\u001b[0m\n\u001b[1;32m    122\u001b[0m         loss \u001b[38;5;241m=\u001b[39m info_nce_loss(question_embedding, soln_embedding, negative_embeddings)\n\u001b[1;32m    123\u001b[0m         val_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m--> 125\u001b[0m     val_accuracy \u001b[38;5;241m=\u001b[39m embedding_accuracy(\u001b[43mwith_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_embeddings_lora\u001b[49m\u001b[43m,\u001b[49m\u001b[43mval_objs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Train loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Val loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(val_loader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Val accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_accuracy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "Cell \u001b[0;32mIn[69], line 6\u001b[0m, in \u001b[0;36mwith_embeddings\u001b[0;34m(get_embeddings, objs)\u001b[0m\n\u001b[1;32m      4\u001b[0m all_solns \u001b[38;5;241m=\u001b[39m [obj[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcanonical_solution\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m objs]\n\u001b[1;32m      5\u001b[0m is_query \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mTrue\u001b[39;00m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlen\u001b[39m(all_questions) \u001b[38;5;241m+\u001b[39m [\u001b[38;5;28;01mFalse\u001b[39;00m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlen\u001b[39m(all_solns)\n\u001b[0;32m----> 6\u001b[0m all_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mget_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_questions\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mall_solns\u001b[49m\u001b[43m,\u001b[49m\u001b[43mis_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_query\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m ret \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, obj \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(objs):\n",
            "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mTypeError\u001b[0m: get_embeddings_lora() got an unexpected keyword argument 'is_query'"
          ]
        }
      ],
      "source": [
        "# Testing out LoRA for tuning models to code\n",
        "get_embeddings_train = get_embeddings_openai\n",
        "old_objs_with_embeddings = with_embeddings(get_embeddings_train,all_objs)\n",
        "\n",
        "train_val_split = 0.8\n",
        "train_objs,val_objs = old_objs_with_embeddings[:int(len(old_objs_with_embeddings)*train_val_split)],old_objs_with_embeddings[int(len(old_objs_with_embeddings)*train_val_split):]\n",
        "\n",
        "# make tiny little model to put questions close to their answers\n",
        "\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class EmbeddingTuner(nn.Module):\n",
        "    def __init__(self,dim:int,rank:int):\n",
        "        super().__init__()\n",
        "        self.rank = rank\n",
        "        self.dim = dim\n",
        "        self.lora_left = nn.Linear(dim,rank)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.lora_right = nn.Linear(rank,dim)\n",
        "\n",
        "    def forward(self,inp):\n",
        "        x = self.lora_left(inp)\n",
        "        # x = self.relu(x)\n",
        "        x = self.lora_right(x)\n",
        "        return inp + x\n",
        "\n",
        "\n",
        "dim = train_objs[0][\"question_embedding\"].shape[0]\n",
        "rank = 0\n",
        "\n",
        "model = EmbeddingTuner(dim,rank)\n",
        "\n",
        "@torch.no_grad()\n",
        "def get_embeddings_lora(sentences):\n",
        "    embeddings = get_embeddings_train(sentences)\n",
        "    return model(embeddings)\n",
        "\n",
        "# train model to put questions close to their answers\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "import torch\n",
        "\n",
        "def info_nce_loss(anchors, positives, negatives, temperature=0.1):\n",
        "    # print(anchors.shape, positives.shape, negatives.shape)\n",
        "    # normalize\n",
        "    anchors = F.normalize(anchors, p=2, dim=1)\n",
        "    positives = F.normalize(positives, p=2, dim=1)\n",
        "    negatives = F.normalize(negatives, p=2, dim=2)\n",
        "\n",
        "    # compute logits\n",
        "    logits = torch.einsum(\"bd,bd->b\", anchors, positives) / temperature\n",
        "    logits = torch.cat([logits.unsqueeze(1), torch.einsum(\"bd,bmd->bm\", anchors, negatives) / temperature], dim=1)\n",
        "\n",
        "    # compute log prob\n",
        "    log_prob = logits - torch.logsumexp(logits, dim=1, keepdim=True)\n",
        "\n",
        "    # loss\n",
        "    loss = -log_prob[:, 0]\n",
        "    return loss.mean()\n",
        "\n",
        "# in every batch, add some random negative examples\n",
        "class LoraDataset(Dataset):\n",
        "    def __init__(self, objs, num_negatives=10):\n",
        "        self.objs = objs\n",
        "        self.num_negatives = num_negatives\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.objs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        obj = self.objs[idx]\n",
        "        question_embedding = obj[\"question_embedding\"]\n",
        "        soln_embedding = obj[\"soln_embedding\"]\n",
        "\n",
        "        # get some random negative examples\n",
        "        negative_idxs = random.sample(range(len(self.objs)), self.num_negatives//2)\n",
        "\n",
        "        # get some hard negative examples (closest to me but not me)\n",
        "        closest_idxs = torch.argsort(F.cosine_similarity(soln_embedding.unsqueeze(0), torch.stack([o[\"soln_embedding\"] for o in self.objs]), dim=1))\n",
        "\n",
        "        negative_idxs += list(closest_idxs[1:self.num_negatives//2+1])\n",
        "        negative_embeddings = torch.stack([self.objs[i][\"soln_embedding\"] for i in negative_idxs])\n",
        "\n",
        "        return question_embedding, soln_embedding, negative_embeddings\n",
        "\n",
        "def train(model, train_objs, val_objs, num_epochs=10, batch_size=32, lr=1e-3):\n",
        "    train_dataset = LoraDataset(train_objs)\n",
        "    val_dataset = LoraDataset(val_objs)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        for question_embedding, soln_embedding, negative_embeddings in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            question_embedding = model(question_embedding)\n",
        "            soln_embedding = model(soln_embedding)\n",
        "            negative_embeddings = torch.stack([model(e) for e in negative_embeddings])\n",
        "\n",
        "            loss = info_nce_loss(question_embedding, soln_embedding, negative_embeddings)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # evaluate on validation set\n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "            val_loss = 0\n",
        "            for question_embedding, soln_embedding, negative_embeddings in val_loader:\n",
        "                question_embedding = model(question_embedding)\n",
        "                soln_embedding = model(soln_embedding)\n",
        "                negative_embeddings = torch.stack([model(e) for e in negative_embeddings])\n",
        "\n",
        "                # Positive pairs have a target of 1, negative pairs have a target of -1\n",
        "                loss = info_nce_loss(question_embedding, soln_embedding, negative_embeddings)\n",
        "                val_loss += loss.item()\n",
        "            \n",
        "            val_accuracy = embedding_accuracy(with_embeddings(get_embeddings_lora,val_objs))\n",
        "\n",
        "        print(f\"Epoch {epoch} | Train loss: {loss.item()} | Val loss: {val_loss/len(val_loader)} | Val accuracy: {val_accuracy}\")\n",
        "\n",
        "print(\"gonna train\")\n",
        "train(model, train_objs, val_objs, num_epochs=50, batch_size=32, lr=2e-4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Translating embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OpenAI: 0.52\n",
            "Voyage: 0.672\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/5r/1qv0cm1n4hb7yldk6h1gr53h0000gn/T/ipykernel_67854/420062248.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  openai_question_embeddings_tensor = torch.tensor(openai_question_embeddings, dtype=torch.float32)\n",
            "/var/folders/5r/1qv0cm1n4hb7yldk6h1gr53h0000gn/T/ipykernel_67854/420062248.py:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  openai_soln_embeddings_tensor = torch.tensor(openai_soln_embeddings, dtype=torch.float32)\n",
            "/var/folders/5r/1qv0cm1n4hb7yldk6h1gr53h0000gn/T/ipykernel_67854/420062248.py:53: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  voyage_question_embeddings_tensor = torch.tensor(voyage_question_embeddings, dtype=torch.float32)\n",
            "/var/folders/5r/1qv0cm1n4hb7yldk6h1gr53h0000gn/T/ipykernel_67854/420062248.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  voyage_soln_embeddings_tensor = torch.tensor(voyage_soln_embeddings, dtype=torch.float32)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 | Train Loss: 0.00038161076372489333 | Test Loss: 0.00039940080023370683\n",
            "Epoch 1 | Train Loss: 0.0002971742651425302 | Test Loss: 0.00030036966199986637\n",
            "Epoch 2 | Train Loss: 0.0002585800539236516 | Test Loss: 0.000263123627519235\n",
            "Epoch 3 | Train Loss: 0.00021256701438687742 | Test Loss: 0.0002286068192915991\n",
            "Epoch 4 | Train Loss: 0.00020017720817122608 | Test Loss: 0.00021509416546905413\n",
            "Epoch 5 | Train Loss: 0.00018887130136135966 | Test Loss: 0.00020397731714183465\n",
            "Epoch 6 | Train Loss: 0.00018209604604635388 | Test Loss: 0.0001971053279703483\n",
            "Epoch 7 | Train Loss: 0.00017558789113536477 | Test Loss: 0.00019197247456759214\n",
            "Epoch 8 | Train Loss: 0.00017019271035678685 | Test Loss: 0.00018717039347393438\n",
            "Epoch 9 | Train Loss: 0.00016852401313371956 | Test Loss: 0.00018531323439674452\n",
            "Epoch 10 | Train Loss: 0.00016872480046004057 | Test Loss: 0.00018469010683475062\n",
            "Epoch 11 | Train Loss: 0.00016645390132907778 | Test Loss: 0.00018291883316123858\n",
            "Epoch 12 | Train Loss: 0.00016529143613297492 | Test Loss: 0.00018206900131190196\n",
            "Epoch 13 | Train Loss: 0.00016539616626687348 | Test Loss: 0.00018188384274253622\n",
            "Epoch 14 | Train Loss: 0.0001647252356633544 | Test Loss: 0.0001812183763831854\n",
            "Epoch 15 | Train Loss: 0.00016381830209866166 | Test Loss: 0.00018062672461383045\n",
            "Epoch 16 | Train Loss: 0.0001632819912629202 | Test Loss: 0.0001802443730412051\n",
            "Epoch 17 | Train Loss: 0.0001632272033020854 | Test Loss: 0.00018054080283036456\n",
            "Epoch 18 | Train Loss: 0.00016282760770991445 | Test Loss: 0.00018012841610470787\n",
            "Epoch 19 | Train Loss: 0.00016299526032526046 | Test Loss: 0.00018039104179479182\n",
            "Epoch 20 | Train Loss: 0.0001632608618820086 | Test Loss: 0.00018056366388918832\n",
            "Epoch 21 | Train Loss: 0.0001626761513762176 | Test Loss: 0.0001804034982342273\n",
            "Epoch 22 | Train Loss: 0.00016338877321686596 | Test Loss: 0.00018084907060256228\n",
            "Epoch 23 | Train Loss: 0.00016264290024992079 | Test Loss: 0.0001804954808903858\n",
            "Epoch 24 | Train Loss: 0.0001626829762244597 | Test Loss: 0.000180727249244228\n",
            "Epoch 25 | Train Loss: 0.00016322967712767422 | Test Loss: 0.00018086015188600868\n",
            "Epoch 26 | Train Loss: 0.00016323562886100262 | Test Loss: 0.0001810103713069111\n",
            "Epoch 27 | Train Loss: 0.0001631166524020955 | Test Loss: 0.00018103867478203028\n",
            "Epoch 28 | Train Loss: 0.00016313490050379187 | Test Loss: 0.00018095572158927098\n",
            "Epoch 29 | Train Loss: 0.00016330268408637494 | Test Loss: 0.00018133359117200598\n",
            "Epoch 30 | Train Loss: 0.00016319648420903832 | Test Loss: 0.00018118439038516954\n",
            "Epoch 31 | Train Loss: 0.00016347478958778083 | Test Loss: 0.00018160093895858154\n",
            "Epoch 32 | Train Loss: 0.00016344411415047944 | Test Loss: 0.00018161412299377844\n",
            "Epoch 33 | Train Loss: 0.00016358985158149153 | Test Loss: 0.00018170599651057273\n",
            "Epoch 34 | Train Loss: 0.00016339027206413448 | Test Loss: 0.00018169830582337454\n",
            "Epoch 35 | Train Loss: 0.0001637152163311839 | Test Loss: 0.00018195510347140953\n",
            "Epoch 36 | Train Loss: 0.00016384247282985598 | Test Loss: 0.00018210363487014547\n",
            "Epoch 37 | Train Loss: 0.00016368654905818403 | Test Loss: 0.00018205090600531548\n",
            "Epoch 38 | Train Loss: 0.0001638192916288972 | Test Loss: 0.00018209787231171504\n",
            "Epoch 39 | Train Loss: 0.00016428664093837142 | Test Loss: 0.00018257722695125267\n",
            "Epoch 40 | Train Loss: 0.0001639565743971616 | Test Loss: 0.00018259783246321604\n",
            "Epoch 41 | Train Loss: 0.0001642538554733619 | Test Loss: 0.00018259282660437748\n",
            "Epoch 42 | Train Loss: 0.00016424535715486854 | Test Loss: 0.0001826617371989414\n",
            "Epoch 43 | Train Loss: 0.00016422674525529146 | Test Loss: 0.00018273933528689668\n",
            "Epoch 44 | Train Loss: 0.0001643591676838696 | Test Loss: 0.0001828633321565576\n",
            "Epoch 45 | Train Loss: 0.00016439985483884811 | Test Loss: 0.00018287741113454103\n",
            "Epoch 46 | Train Loss: 0.00016449754184577614 | Test Loss: 0.00018303446267964318\n",
            "Epoch 47 | Train Loss: 0.00016452556883450598 | Test Loss: 0.00018310737505089492\n",
            "Epoch 48 | Train Loss: 0.00016447225061710924 | Test Loss: 0.0001831247573136352\n",
            "Epoch 49 | Train Loss: 0.00016454119759146124 | Test Loss: 0.0001831661065807566\n",
            "Epoch 0 | Train Loss: 0.00038976495852693915 | Test Loss: 0.0003769378818105906\n",
            "Epoch 1 | Train Loss: 0.00023066902940627187 | Test Loss: 0.0002479260219843127\n",
            "Epoch 2 | Train Loss: 0.00018958200234919786 | Test Loss: 0.00020484813285293058\n",
            "Epoch 3 | Train Loss: 0.0001690148055786267 | Test Loss: 0.00018103531328961253\n",
            "Epoch 4 | Train Loss: 0.00014947718591429293 | Test Loss: 0.0001656299500609748\n",
            "Epoch 5 | Train Loss: 0.00014005678531248122 | Test Loss: 0.00015571530821034685\n",
            "Epoch 6 | Train Loss: 0.00013413315173238516 | Test Loss: 0.00015097614232217893\n",
            "Epoch 7 | Train Loss: 0.00013119960203766823 | Test Loss: 0.00014715391444042325\n",
            "Epoch 8 | Train Loss: 0.0001277109404327348 | Test Loss: 0.00014393568562809378\n",
            "Epoch 9 | Train Loss: 0.0001264210877707228 | Test Loss: 0.00014184532483341172\n",
            "Epoch 10 | Train Loss: 0.0001236400566995144 | Test Loss: 0.00013979880168335512\n",
            "Epoch 11 | Train Loss: 0.00012312237231526524 | Test Loss: 0.00013934677554061636\n",
            "Epoch 12 | Train Loss: 0.0001221921993419528 | Test Loss: 0.0001382187328999862\n",
            "Epoch 13 | Train Loss: 0.0001210453046951443 | Test Loss: 0.00013751481310464442\n",
            "Epoch 14 | Train Loss: 0.00012120477185817435 | Test Loss: 0.00013753930033999495\n",
            "Epoch 15 | Train Loss: 0.00011898419325007126 | Test Loss: 0.0001361858048767317\n",
            "Epoch 16 | Train Loss: 0.00011942423589061946 | Test Loss: 0.00013656832743436098\n",
            "Epoch 17 | Train Loss: 0.00011890624591615051 | Test Loss: 0.00013623092672787607\n",
            "Epoch 18 | Train Loss: 0.00011863097461173311 | Test Loss: 0.00013607341315946542\n",
            "Epoch 19 | Train Loss: 0.00011947165330639109 | Test Loss: 0.0001366660108033102\n",
            "Epoch 20 | Train Loss: 0.0001185665387311019 | Test Loss: 0.00013619925812236033\n",
            "Epoch 21 | Train Loss: 0.00011901641119038686 | Test Loss: 0.00013629433306050487\n",
            "Epoch 22 | Train Loss: 0.00011884683044627309 | Test Loss: 0.00013625810242956504\n",
            "Epoch 23 | Train Loss: 0.0001186223526019603 | Test Loss: 0.0001361855902359821\n",
            "Epoch 24 | Train Loss: 0.00011866709246532992 | Test Loss: 0.00013630278044729494\n",
            "Epoch 25 | Train Loss: 0.00011854790500365198 | Test Loss: 0.0001362029361189343\n",
            "Epoch 26 | Train Loss: 0.00011911238107131794 | Test Loss: 0.00013675925220013596\n",
            "Epoch 27 | Train Loss: 0.0001187157686217688 | Test Loss: 0.0001365210446238052\n",
            "Epoch 28 | Train Loss: 0.000119108721264638 | Test Loss: 0.00013706441677641124\n",
            "Epoch 29 | Train Loss: 0.00011953466309932992 | Test Loss: 0.00013734697495237924\n",
            "Epoch 30 | Train Loss: 0.00011918588279513642 | Test Loss: 0.0001370539866911713\n",
            "Epoch 31 | Train Loss: 0.0001193281204905361 | Test Loss: 0.00013722776202484965\n",
            "Epoch 32 | Train Loss: 0.00011893908231286332 | Test Loss: 0.00013726666293223388\n",
            "Epoch 33 | Train Loss: 0.00011924438877031207 | Test Loss: 0.00013732106162933633\n",
            "Epoch 34 | Train Loss: 0.0001193893258459866 | Test Loss: 0.0001374934581690468\n",
            "Epoch 35 | Train Loss: 0.00011970791820203885 | Test Loss: 0.00013768554708804004\n",
            "Epoch 36 | Train Loss: 0.00011956739763263613 | Test Loss: 0.0001376118598273024\n",
            "Epoch 37 | Train Loss: 0.00011916857329197228 | Test Loss: 0.00013746933836955577\n",
            "Epoch 38 | Train Loss: 0.00011948399333050475 | Test Loss: 0.00013779806977254339\n",
            "Epoch 39 | Train Loss: 0.00011969408660661429 | Test Loss: 0.00013797912106383592\n",
            "Epoch 40 | Train Loss: 0.00011968681064900011 | Test Loss: 0.0001380384637741372\n",
            "Epoch 41 | Train Loss: 0.0001197784804389812 | Test Loss: 0.00013806637798552401\n",
            "Epoch 42 | Train Loss: 0.00011985302262473851 | Test Loss: 0.0001381709662382491\n",
            "Epoch 43 | Train Loss: 0.00011975131928920746 | Test Loss: 0.00013834422134095803\n",
            "Epoch 44 | Train Loss: 0.0001199165781144984 | Test Loss: 0.00013836173093295656\n",
            "Epoch 45 | Train Loss: 0.00011988350161118433 | Test Loss: 0.00013833000775775872\n",
            "Epoch 46 | Train Loss: 0.00012006913311779499 | Test Loss: 0.0001385764917358756\n",
            "Epoch 47 | Train Loss: 0.00011994699889328331 | Test Loss: 0.0001384944625897333\n",
            "Epoch 48 | Train Loss: 0.00012016369873890653 | Test Loss: 0.00013867659072275274\n",
            "Epoch 49 | Train Loss: 0.00012023983435938135 | Test Loss: 0.00013867851157556288\n",
            "OpenAI Projected: 0.58\n"
          ]
        }
      ],
      "source": [
        "with_openai_embeddings = with_embeddings(get_embeddings_openai,all_objs)\n",
        "with_voyage_embeddings = with_embeddings(get_embeddings_voyage,all_objs)\n",
        "\n",
        "openai_question_embeddings = torch.stack([obj[\"question_embedding\"] for obj in with_openai_embeddings])\n",
        "openai_soln_embeddings = torch.stack([obj[\"soln_embedding\"] for obj in with_openai_embeddings])\n",
        "voyage_question_embeddings = torch.stack([obj[\"question_embedding\"] for obj in with_voyage_embeddings])\n",
        "voyage_soln_embeddings = torch.stack([obj[\"soln_embedding\"] for obj in with_voyage_embeddings])\n",
        "\n",
        "\n",
        "print(\"OpenAI:\",embedding_accuracy(with_openai_embeddings))\n",
        "print(\"Voyage:\",embedding_accuracy(with_voyage_embeddings))\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "class ProjectionLayer(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(ProjectionLayer, self).__init__()\n",
        "        self.linear = nn.Linear(input_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear(x)\n",
        "\n",
        "def train_projection_layer(projection_layer, train_loader, test_loader, num_epochs=10, lr=1e-4):\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(projection_layer.parameters(), lr=lr)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        projection_layer.train()\n",
        "        for openai_embeddings, voyage_embeddings in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            projected_embeddings = projection_layer(openai_embeddings)\n",
        "            loss = criterion(projected_embeddings, voyage_embeddings)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Evaluate on test set\n",
        "        projection_layer.eval()\n",
        "        with torch.no_grad():\n",
        "            test_loss = 0\n",
        "            for openai_embeddings, voyage_embeddings in test_loader:\n",
        "                projected_embeddings = projection_layer(openai_embeddings)\n",
        "                loss = criterion(projected_embeddings, voyage_embeddings)\n",
        "                test_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch} | Train Loss: {loss.item()} | Test Loss: {test_loss / len(test_loader)}\")\n",
        "\n",
        "# Assuming openai_embeddings and voyage_embeddings are available and have the same shape\n",
        "openai_question_embeddings_tensor = torch.tensor(openai_question_embeddings, dtype=torch.float32)\n",
        "openai_soln_embeddings_tensor = torch.tensor(openai_soln_embeddings, dtype=torch.float32)\n",
        "voyage_question_embeddings_tensor = torch.tensor(voyage_question_embeddings, dtype=torch.float32)\n",
        "voyage_soln_embeddings_tensor = torch.tensor(voyage_soln_embeddings, dtype=torch.float32)\n",
        "\n",
        "# For questions\n",
        "question_dataset = TensorDataset(openai_question_embeddings_tensor, voyage_question_embeddings_tensor)\n",
        "question_train_size = int(0.8 * len(question_dataset))\n",
        "question_test_size = len(question_dataset) - question_train_size\n",
        "question_train_dataset, question_test_dataset = torch.utils.data.random_split(question_dataset, [question_train_size, question_test_size])\n",
        "\n",
        "question_train_loader = DataLoader(question_train_dataset, batch_size=32, shuffle=True)\n",
        "question_test_loader = DataLoader(question_test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "question_input_dim = openai_question_embeddings_tensor.shape[1]\n",
        "question_output_dim = voyage_question_embeddings_tensor.shape[1]\n",
        "question_projection_layer = ProjectionLayer(question_input_dim, question_output_dim)\n",
        "\n",
        "train_projection_layer(question_projection_layer, question_train_loader, question_test_loader, num_epochs=50, lr=1e-3)\n",
        "\n",
        "# For solutions\n",
        "soln_dataset = TensorDataset(openai_soln_embeddings_tensor, voyage_soln_embeddings_tensor)\n",
        "soln_train_size = int(0.8 * len(soln_dataset))\n",
        "soln_test_size = len(soln_dataset) - soln_train_size\n",
        "soln_train_dataset, soln_test_dataset = torch.utils.data.random_split(soln_dataset, [soln_train_size, soln_test_size])\n",
        "\n",
        "soln_train_loader = DataLoader(soln_train_dataset, batch_size=32, shuffle=True)\n",
        "soln_test_loader = DataLoader(soln_test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "soln_input_dim = openai_soln_embeddings_tensor.shape[1]\n",
        "soln_output_dim = voyage_soln_embeddings_tensor.shape[1]\n",
        "soln_projection_layer = ProjectionLayer(soln_input_dim, soln_output_dim)\n",
        "\n",
        "train_projection_layer(soln_projection_layer, soln_train_loader, soln_test_loader, num_epochs=50, lr=1e-3)\n",
        "\n",
        "def get_embeddings_openai_projected(sentences,is_query):\n",
        "    assert len(is_query)==len(sentences)\n",
        "    question_idxes = [i for i in range(len(sentences)) if is_query[i]]\n",
        "    soln_idxes = [i for i in range(len(sentences)) if not is_query[i]]\n",
        "    question_embeddings = get_embeddings_openai([sentences[i] for i in question_idxes])\n",
        "    soln_embeddings = get_embeddings_openai([sentences[i] for i in soln_idxes])\n",
        "    question_embeddings = question_projection_layer(question_embeddings)\n",
        "    soln_embeddings = soln_projection_layer(soln_embeddings)\n",
        "    ret = []\n",
        "    for i in range(len(sentences)):\n",
        "        if is_query[i]:\n",
        "            ret.append(question_embeddings[question_idxes.index(i)])\n",
        "        else:\n",
        "            ret.append(soln_embeddings[soln_idxes.index(i)])\n",
        "    return ret\n",
        "\n",
        "with_projected_openai_embeddings = with_embeddings(get_embeddings_openai_projected,all_objs)\n",
        "print(\"OpenAI Projected:\",embedding_accuracy(with_projected_openai_embeddings))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.615"
            ]
          },
          "execution_count": 98,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embedding_accuracy(with_embeddings(get_embeddings_lora,all_objs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def plot_qa_embeddings(get_embeddings, objs):\n",
        "    # Combine question and answer embeddings for TSNE\n",
        "    question_embeddings = get_embeddings([obj[\"docstring\"] for obj in objs])\n",
        "    answer_embeddings = get_embeddings([obj[\"canonical_solution\"] for obj in objs])\n",
        "    combined_embeddings = np.vstack((question_embeddings, answer_embeddings))\n",
        "    \n",
        "    tsne = TSNE(n_components=2, random_state=0)\n",
        "    combined_embeddings_2d = tsne.fit_transform(combined_embeddings)\n",
        "    \n",
        "    # Split the transformed embeddings back into questions and answers\n",
        "    num_questions = question_embeddings.shape[0]\n",
        "    question_embeddings_2d = combined_embeddings_2d[:num_questions, :]\n",
        "    answer_embeddings_2d = combined_embeddings_2d[num_questions:, :]\n",
        "    \n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.scatter(question_embeddings_2d[:, 0], question_embeddings_2d[:, 1], c='blue', label='Questions')\n",
        "    plt.scatter(answer_embeddings_2d[:, 0], answer_embeddings_2d[:, 1], c='red', label='Answers')\n",
        "    plt.title('t-SNE plot of Question and Answer Embeddings')\n",
        "    plt.xlabel('Dimension 1')\n",
        "    plt.ylabel('Dimension 2')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "plot_qa_embeddings(get_embeddings_lora,all_objs)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0cbffaa3365546c083ff2bc555a62951": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "12647b6611554c2b9a3422c1840fb0ee": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "279b2498a5904ec287625ea4c2531cf2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5ed9d8d5490e4670b3ef732af2feb5ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_12647b6611554c2b9a3422c1840fb0ee",
            "placeholder": "",
            "style": "IPY_MODEL_69ac2a0ce4eb43b182ceced64838148d",
            "value": " 547M/547M [00:06&lt;00:00, 134MB/s]"
          }
        },
        "69ac2a0ce4eb43b182ceced64838148d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "846a9be319a148f78466917538b2a186": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b23ff8d3071b4f48aeaf73c38973314b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c8f3ac2f1e2a464dbdfa781b40da5a8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0cbffaa3365546c083ff2bc555a62951",
            "max": 546961866,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_279b2498a5904ec287625ea4c2531cf2",
            "value": 546961866
          }
        },
        "ddaa0780a71a4231944819950117ff97": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e0447abe66e0411cba639563bb5d43ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fadeff35a86d48c29ed5b559b2de75c8",
              "IPY_MODEL_c8f3ac2f1e2a464dbdfa781b40da5a8c",
              "IPY_MODEL_5ed9d8d5490e4670b3ef732af2feb5ac"
            ],
            "layout": "IPY_MODEL_ddaa0780a71a4231944819950117ff97"
          }
        },
        "fadeff35a86d48c29ed5b559b2de75c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b23ff8d3071b4f48aeaf73c38973314b",
            "placeholder": "",
            "style": "IPY_MODEL_846a9be319a148f78466917538b2a186",
            "value": "pytorch_model.bin: 100%"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
